{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro\n",
    "\n",
    "\n",
    "https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data\n",
    "https://github.com/VolodymyrGavrysh/My_RoadMap_Data_Science/blob/master/kds/books/Spark-The%20Definitive%20Guide.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local','test')\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('test').getOrCreate()\n",
    "\n",
    "# while creating sparksession or context, master and appname are mandatory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Number=0),\n",
       " Row(Number=1),\n",
       " Row(Number=2),\n",
       " Row(Number=3),\n",
       " Row(Number=4),\n",
       " Row(Number=5),\n",
       " Row(Number=6),\n",
       " Row(Number=7),\n",
       " Row(Number=8),\n",
       " Row(Number=9),\n",
       " Row(Number=10),\n",
       " Row(Number=11),\n",
       " Row(Number=12),\n",
       " Row(Number=13),\n",
       " Row(Number=14),\n",
       " Row(Number=15),\n",
       " Row(Number=16),\n",
       " Row(Number=17),\n",
       " Row(Number=18),\n",
       " Row(Number=19),\n",
       " Row(Number=20),\n",
       " Row(Number=21),\n",
       " Row(Number=22),\n",
       " Row(Number=23),\n",
       " Row(Number=24),\n",
       " Row(Number=25),\n",
       " Row(Number=26),\n",
       " Row(Number=27),\n",
       " Row(Number=28),\n",
       " Row(Number=29),\n",
       " Row(Number=30),\n",
       " Row(Number=31),\n",
       " Row(Number=32),\n",
       " Row(Number=33),\n",
       " Row(Number=34),\n",
       " Row(Number=35),\n",
       " Row(Number=36),\n",
       " Row(Number=37),\n",
       " Row(Number=38),\n",
       " Row(Number=39),\n",
       " Row(Number=40),\n",
       " Row(Number=41),\n",
       " Row(Number=42),\n",
       " Row(Number=43),\n",
       " Row(Number=44),\n",
       " Row(Number=45),\n",
       " Row(Number=46),\n",
       " Row(Number=47),\n",
       " Row(Number=48),\n",
       " Row(Number=49),\n",
       " Row(Number=50),\n",
       " Row(Number=51),\n",
       " Row(Number=52),\n",
       " Row(Number=53),\n",
       " Row(Number=54),\n",
       " Row(Number=55),\n",
       " Row(Number=56),\n",
       " Row(Number=57),\n",
       " Row(Number=58),\n",
       " Row(Number=59),\n",
       " Row(Number=60),\n",
       " Row(Number=61),\n",
       " Row(Number=62),\n",
       " Row(Number=63),\n",
       " Row(Number=64),\n",
       " Row(Number=65),\n",
       " Row(Number=66),\n",
       " Row(Number=67),\n",
       " Row(Number=68),\n",
       " Row(Number=69),\n",
       " Row(Number=70),\n",
       " Row(Number=71),\n",
       " Row(Number=72),\n",
       " Row(Number=73),\n",
       " Row(Number=74),\n",
       " Row(Number=75),\n",
       " Row(Number=76),\n",
       " Row(Number=77),\n",
       " Row(Number=78),\n",
       " Row(Number=79),\n",
       " Row(Number=80),\n",
       " Row(Number=81),\n",
       " Row(Number=82),\n",
       " Row(Number=83),\n",
       " Row(Number=84),\n",
       " Row(Number=85),\n",
       " Row(Number=86),\n",
       " Row(Number=87),\n",
       " Row(Number=88),\n",
       " Row(Number=89),\n",
       " Row(Number=90),\n",
       " Row(Number=91),\n",
       " Row(Number=92),\n",
       " Row(Number=93),\n",
       " Row(Number=94),\n",
       " Row(Number=95),\n",
       " Row(Number=96),\n",
       " Row(Number=97),\n",
       " Row(Number=98),\n",
       " Row(Number=99)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.range(100).toDF('Number')\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:4040'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "even_data = data.where('Number % 2 = 0')\n",
    "even_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "even_data = data.filter(even_data.Number%2 == 0)\n",
    "even_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicate pushdown on DataFrames\n",
    "If we build a large Spark job but specify a filter at the end that only requires us to\n",
    "fetch one row from our source data, the most efficient way to execute this is to access the single\n",
    "record that we need. Spark will actually optimize this for us by pushing the filter down\n",
    "automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of actions:\n",
    "\n",
    "Actions to view data in the console\n",
    "Actions to collect data to native objects in the respective language\n",
    "Actions to write to output data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_2015 = spark.read.option(\"inferSchema\",\"True\")\\\n",
    ".option('header','True')\\\n",
    ".csv('C:/Users/varsh/Downloads/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary_2015.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|\n",
      "+------------------------+-------------------+-----+\n",
      "|United States           |Romania            |15   |\n",
      "|United States           |Croatia            |1    |\n",
      "|United States           |Ireland            |344  |\n",
      "|Egypt                   |United States      |15   |\n",
      "|United States           |India              |62   |\n",
      "|United States           |Singapore          |1    |\n",
      "|United States           |Grenada            |62   |\n",
      "|Costa Rica              |United States      |588  |\n",
      "|Senegal                 |United States      |40   |\n",
      "|Moldova                 |United States      |1    |\n",
      "|United States           |Sint Maarten       |325  |\n",
      "|United States           |Marshall Islands   |39   |\n",
      "|Guyana                  |United States      |64   |\n",
      "|Malta                   |United States      |1    |\n",
      "|Anguilla                |United States      |41   |\n",
      "|Bolivia                 |United States      |30   |\n",
      "|United States           |Paraguay           |6    |\n",
      "|Algeria                 |United States      |4    |\n",
      "|Turks and Caicos Islands|United States      |230  |\n",
      "|United States           |Gibraltar          |1    |\n",
      "+------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Summary_2015.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#43 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#43 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=125]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#41,ORIGIN_COUNTRY_NAME#42,count#43] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/varsh/Downloads/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Summary_2015.sort('count').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default, when we perform a shuffle, Spark\n",
    "# outputs 200 shuffle partitions. Let’s set this value to 5 to reduce the number of the output\n",
    "# partitions from the shuffle:\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "Summary_2015.sort('count').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_2015.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|\n",
      "+------------------------+-------------------+-----+\n",
      "|United States           |Romania            |15   |\n",
      "|United States           |Croatia            |1    |\n",
      "|United States           |Ireland            |344  |\n",
      "|Egypt                   |United States      |15   |\n",
      "|United States           |India              |62   |\n",
      "|United States           |Singapore          |1    |\n",
      "|United States           |Grenada            |62   |\n",
      "|Costa Rica              |United States      |588  |\n",
      "|Senegal                 |United States      |40   |\n",
      "|Moldova                 |United States      |1    |\n",
      "|United States           |Sint Maarten       |325  |\n",
      "|United States           |Marshall Islands   |39   |\n",
      "|Guyana                  |United States      |64   |\n",
      "|Malta                   |United States      |1    |\n",
      "|Anguilla                |United States      |41   |\n",
      "|Bolivia                 |United States      |30   |\n",
      "|United States           |Paraguay           |6    |\n",
      "|Algeria                 |United States      |4    |\n",
      "|Turks and Caicos Islands|United States      |230  |\n",
      "|United States           |Gibraltar          |1    |\n",
      "+------------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from data').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can express your business logic in SQL or DataFrames (either in R, Python, Scala, or Java) and Spark will compile that logic down to an underlying plan (that you can see in the explain plan) before actually executing your code. With Spark SQL, you can register any DataFrame as a table or\n",
    "view (a temporary table) and query it using pure SQL. There is no performance difference\n",
    "between writing SQL queries or writing DataFrame code, they both “compile” to the same\n",
    "underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#44], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#44, 5), ENSURE_REQUIREMENTS, [plan_id=127]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#44], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#44] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/varsh/Downloads/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#44], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#44, 5), ENSURE_REQUIREMENTS, [plan_id=140]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#44], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#44] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/varsh/Downloads/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = Summary_2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark sql functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "Summary_2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCount = Summary_2015.select(max(\"count\"))\n",
    "\n",
    "maxCount.write.parquet('C:/Users/varsh/outputs/maxCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CountMax = spark.read.format('parquet').load('C:/Users/varsh/outputs/maxCount')\n",
    "CountMax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"data\").cache()\n",
    "\n",
    "# This returns the table as a dataframe and caches it. Caching also done post an action call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('select * from data').show()\n",
    "\n",
    "spark.table(\"data\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.show() : It will show only the content of the dataframe.\n",
    "df.show(n) \n",
    "df.collect() : It will show the content and metadata of the dataframe.\n",
    "df.take(n) : shows content and structure/metadata for a limited number of rows for a very large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "Summary_2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triple quotes allow line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select DEST_COUNTRY_NAME, sum(count) as destination_total from data\n",
    "          group by 1 order by destination_total desc limit 5''').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Zambia|      United States|    1|\n",
      "|        Venezuela|      United States|  290|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary_2015.sort(desc('DEST_COUNTRY_NAME')).show(3)\n",
    "\n",
    "Summary_2015.sort(Summary_2015.DEST_COUNTRY_NAME,ascending=False).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 3 -- To be continued -- page 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = spark.read.option('InferSchema','True').option('Header','True').csv(\"C:/Users/varsh/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|        Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084| RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077|DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "+---------+---------+-------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.createOrReplaceTempView(\"retail_table\")\n",
    "retail_schema = retail.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('InvoiceNo', StringType(), True), StructField('StockCode', StringType(), True), StructField('Description', StringType(), True), StructField('Quantity', IntegerType(), True), StructField('InvoiceDate', TimestampType(), True), StructField('UnitPrice', DoubleType(), True), StructField('CustomerID', DoubleType(), True), StructField('Country', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(retail_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------------+---------------------------+\n",
      "|CustomerID|InvoiceDate|date_add(InvoiceDate, 1)|sum((UnitPrice * Quantity))|\n",
      "+----------+-----------+------------------------+---------------------------+\n",
      "|null      |2011-01-05 |2011-01-06              |-29716.63000000005         |\n",
      "|null      |2011-05-16 |2011-05-17              |-10387.250000000062        |\n",
      "|null      |2011-02-21 |2011-02-22              |-8437.31000000004          |\n",
      "|null      |2010-12-07 |2010-12-08              |-8066.940000000085         |\n",
      "|null      |2011-08-12 |2011-08-13              |-7862.940000000068         |\n",
      "|null      |2011-07-13 |2011-07-14              |-7672.000000000031         |\n",
      "|12931.0   |2011-08-31 |2011-09-01              |-7460.950000000001         |\n",
      "|null      |2011-06-24 |2011-06-25              |-7367.370000000025         |\n",
      "|16029.0   |2011-05-03 |2011-05-04              |-6930.0                    |\n",
      "|null      |2011-09-15 |2011-09-16              |-6724.830000000022         |\n",
      "|null      |2011-03-18 |2011-03-19              |-6339.099999999995         |\n",
      "|null      |2011-08-02 |2011-08-03              |-4993.459999999994         |\n",
      "|17448.0   |2011-07-18 |2011-07-19              |-4287.63                   |\n",
      "|16029.0   |2011-10-11 |2011-10-12              |-4006.079999999999         |\n",
      "|17450.0   |2011-09-21 |2011-09-22              |-3825.3599999999997        |\n",
      "|12454.0   |2011-10-17 |2011-10-18              |-3528.3399999999997        |\n",
      "|14156.0   |2011-02-25 |2011-02-26              |-2548.8                    |\n",
      "|12748.0   |2011-09-15 |2011-09-16              |-1770.04                   |\n",
      "|15482.0   |2011-11-04 |2011-11-05              |-1652.8                    |\n",
      "|17949.0   |2011-08-22 |2011-08-23              |-1642.5                    |\n",
      "+----------+-----------+------------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "\n",
    "# spark.sql('''select CustomerID, InvoiceDate, sum(UnitPrice * Quantity)\n",
    "#            from retail_Table group by CustomerID,InvoiceDate\n",
    "#           order by sum(UnitPrice * Quantity)''').show(truncate=False)\n",
    "\n",
    "spark.sql('''select CustomerID, Date(InvoiceDate), DATE_ADD(InvoiceDate, 1),sum(UnitPrice * Quantity)\n",
    "           from retail_Table group by CustomerID,Date(InvoiceDate), DATE_ADD(InvoiceDate, 1)\n",
    "          order by sum(UnitPrice * Quantity)''').show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+-------------------+\n",
      "|CustomerId|window                                    |sum(total_cost)    |\n",
      "+----------+------------------------------------------+-------------------+\n",
      "|null      |{2011-01-05 05:30:00, 2011-01-06 05:30:00}|-29716.63000000005 |\n",
      "|null      |{2011-05-16 05:30:00, 2011-05-17 05:30:00}|-10387.250000000062|\n",
      "|null      |{2011-02-21 05:30:00, 2011-02-22 05:30:00}|-8437.31000000004  |\n",
      "|null      |{2010-12-07 05:30:00, 2010-12-08 05:30:00}|-8066.940000000085 |\n",
      "|null      |{2011-08-12 05:30:00, 2011-08-13 05:30:00}|-7862.940000000068 |\n",
      "|null      |{2011-07-13 05:30:00, 2011-07-14 05:30:00}|-7672.000000000031 |\n",
      "|12931.0   |{2011-08-31 05:30:00, 2011-09-01 05:30:00}|-7460.950000000001 |\n",
      "|null      |{2011-06-24 05:30:00, 2011-06-25 05:30:00}|-7367.370000000025 |\n",
      "|16029.0   |{2011-05-03 05:30:00, 2011-05-04 05:30:00}|-6930.0            |\n",
      "|null      |{2011-09-15 05:30:00, 2011-09-16 05:30:00}|-6724.830000000022 |\n",
      "|null      |{2011-03-18 05:30:00, 2011-03-19 05:30:00}|-6339.099999999995 |\n",
      "|null      |{2011-08-02 05:30:00, 2011-08-03 05:30:00}|-4993.459999999994 |\n",
      "|17448.0   |{2011-07-18 05:30:00, 2011-07-19 05:30:00}|-4287.63           |\n",
      "|16029.0   |{2011-10-11 05:30:00, 2011-10-12 05:30:00}|-4006.079999999999 |\n",
      "|17450.0   |{2011-09-21 05:30:00, 2011-09-22 05:30:00}|-3825.3599999999997|\n",
      "|12454.0   |{2011-10-17 05:30:00, 2011-10-18 05:30:00}|-3528.3399999999997|\n",
      "|14156.0   |{2011-02-25 05:30:00, 2011-02-26 05:30:00}|-2548.8            |\n",
      "|12748.0   |{2011-09-15 05:30:00, 2011-09-16 05:30:00}|-1770.04           |\n",
      "|15482.0   |{2011-11-04 05:30:00, 2011-11-05 05:30:00}|-1652.8            |\n",
      "|17949.0   |{2011-08-22 05:30:00, 2011-08-23 05:30:00}|-1642.5            |\n",
      "+----------+------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = retail\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")\n",
    "\n",
    "output.sort(\"sum(total_cost)\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    ".schema(retail_schema)\\\n",
    ".option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(\"C:/Users/varsh/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\\\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[CustomerId#2353, window#2378], functions=[sum(total_cost#2363)])\n",
      "+- StateStoreSave [CustomerId#2353, window#2378], state info [ checkpoint = <unknown>, runId = c830aca3-7c6e-400d-b854-edcd3b46413d, opId = 0, ver = 0, numPartitions = 5], Append, 0, 2\n",
      "   +- *(3) HashAggregate(keys=[CustomerId#2353, window#2378], functions=[merge_sum(total_cost#2363)])\n",
      "      +- StateStoreRestore [CustomerId#2353, window#2378], state info [ checkpoint = <unknown>, runId = c830aca3-7c6e-400d-b854-edcd3b46413d, opId = 0, ver = 0, numPartitions = 5], 2\n",
      "         +- *(2) HashAggregate(keys=[CustomerId#2353, window#2378], functions=[merge_sum(total_cost#2363)])\n",
      "            +- Exchange hashpartitioning(CustomerId#2353, window#2378, 5), ENSURE_REQUIREMENTS, [plan_id=1726]\n",
      "               +- *(1) HashAggregate(keys=[knownfloatingpointnormalized(normalizenanandzero(CustomerId#2353)) AS CustomerId#2353, window#2378], functions=[partial_sum(total_cost#2363)])\n",
      "                  +- *(1) Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(InvoiceDate#2351, TimestampType, LongType) - (((precisetimestampconversion(InvoiceDate#2351, TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(InvoiceDate#2351, TimestampType, LongType) - (((precisetimestampconversion(InvoiceDate#2351, TimestampType, LongType) - 0) + 86400000000) % 86400000000)) - 0) + 86400000000), LongType, TimestampType)) AS window#2378, CustomerId#2353, (UnitPrice#2352 * cast(Quantity#2350 as double)) AS total_cost#2363]\n",
      "                     +- *(1) Filter isnotnull(InvoiceDate#2351)\n",
      "                        +- StreamingRelation FileSource[C:/Users/varsh/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv], [InvoiceNo#2347, StockCode#2348, Description#2349, Quantity#2350, InvoiceDate#2351, UnitPrice#2352, CustomerID#2353, Country#2354]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x1cce7412fb0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    ".format(\"memory\")\\\n",
    ".queryName(\"customer_purchases\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming VS Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this window is built on\n",
    "event time, as well, not the time at which Spark processes the data. This was one of the\n",
    "shortcomings of Spark Streaming that Structured Streaming has resolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 4 Structured API - Page 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames Versus Datasets\n",
    "In essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and\n",
    "the “typed” Datasets. To say that DataFrames are untyped is aslightly inaccurate; they have\n",
    "types, but Spark maintains them completely and only checks whether those types line up to those\n",
    "specified in the schema at runtime. Datasets, on the other hand, check whether types conform to\n",
    "the specification at compile time. Datasets are only available to Java Virtual Machine (JVM)–\n",
    "based languages (Scala and Java)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col1|\n",
      "+----+\n",
      "|   0|\n",
      "|   1|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(2).toDF('col1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "b = ByteType()\n",
    "\n",
    "\n",
    "ByteType()\n",
    "ByteType()\n",
    "IntegerType()\n",
    "LongType()\n",
    "FloatType()\n",
    "DoubleType()\n",
    "DecimalType()\n",
    "StringType()\n",
    "BinaryType()\n",
    "BooleanType()\n",
    "TimestampType()\n",
    "DateType()\n",
    "ArrayType -- list, tuple, or array -- ArrayType(elementType,[containsNull])\n",
    "MapType -- dict -- MapType(keyType,valueType,[valueContainsNull]).\n",
    "StructType -- StructType(fields). Note:fields is a list of StructFields.\n",
    "StructField -- StructField(name,dataType, [nullable])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usercode > if code is valid > unresolved logical plan > ....\n",
    "\n",
    "This plan is unresolved\n",
    "because although your code might be valid, the tables or columns that it refers to might or might\n",
    "not exist. Spark uses the catalog, a repository of all table and DataFrame information, to resolve\n",
    "columns and tables in the analyzer.  The analyzer might reject the unresolved logical plan if the\n",
    "required table or column name does not exist in the catalog. If the analyzer can resolve it, the\n",
    "result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the\n",
    "logical plan by pushing down predicates or selections\n",
    "\n",
    "Analysis using catalog > resolved logical plan > optimised logical plan > physical plan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical plan, often called a Spark plan, specifies how the logical plan will execute\n",
    "on the cluster by generating different physical execution strategies and comparing them through\n",
    "a cost model, An example of the cost comparison might be choosing\n",
    "how to perform a given join by looking at the physical attributes of a given table (how big the\n",
    "table is or how big its partitions are)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 5. Basic Structured Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json')\\\n",
    ".load('C:/Users/varsh/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_json = spark.read.format('json')\\\n",
    ".load('C:/Users/varsh/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json')\n",
    "flight_json.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding whether you need to define a schema prior to reading in your data depends on your use case.\n",
    "For ad hoc analysis, schema-on-read usually works just fine (although at times it can be a bit slow with\n",
    "plain-text file formats like CSV or JSON). However, this can also lead to precision issues like a long\n",
    "type incorrectly set as an integer when reading in a file. When using Spark for production Extract,\n",
    "Transform, and Load (ETL), it is often a good idea to define your schemas manually, especially when\n",
    "working with untyped data sources like CSV and JSON because schema inference can vary depending\n",
    "on the type of data that you read in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    ".load(\"C:/Users/varsh/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "# col(\"someColumnName\")\n",
    "# column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that columns are expressions, but what is an expression? An expression is\n",
    "a set of transformations on one or more values in a record in a DataFrame. Think of it like a\n",
    "function that takes as input one or more column names, resolves them, and then potentially\n",
    "applies more expressions to create a single value for each record in the dataset. Importantly, this\n",
    "“single value” can actually be a complex type like a Map or Array. We’ll see more of the\n",
    "complex types in Chapter 6.\n",
    "In the simplest case, an expression, created via the expr function, is just a DataFrame column\n",
    "reference. In the simplest case, expr(\"someCol\") is equivalent to col(\"someCol\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL function - expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").schema(myManualSchema)\\\n",
    ".load(\"C:/Users/varsh/Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Transformations\n",
    "\n",
    "We can add rows or columns\n",
    "We can remove rows or columns\n",
    "We can transform a row into a column (or vice versa)\n",
    "We can change the order of rows based on the values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('flight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "page 74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+\n",
      "|    United States|    United States|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "\"*\", # all original columns\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to pass explicit values into Spark that are just a value (rather than a new\n",
    "column). This might be a constant value or something we’ll need to compare to later on. The\n",
    "way we do this is through literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the withColumn function takes two arguments: the column name and the expression\n",
    "that will create the value for that given row in the DataFrame. Interestingly, we can also rename\n",
    "a column this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can rename a column in the manner that we just described, another alternative is to\n",
    "use the withColumnRenamed method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserved character and escape character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName = df.withColumn(\n",
    "\"This Long Column-Name\",\n",
    "expr(\"ORIGIN_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'This Long Column-Name']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.selectExpr(\n",
    "\"`This Long Column-Name`\",\n",
    "\"`This Long Column-Name` as `new col`\")\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to use backticks because we’re referencing a column in an expression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|This Long Column-Name|\n",
      "+---------------------+\n",
      "|              Romania|\n",
      "|              Croatia|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.select(col(\"This Long Column-Name\")).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default Spark is case insensitive; however, you can make Spark case sensitive by setting the\n",
    "configuration:\n",
    "-- in SQL\n",
    "set spark.sql.caseSensitive true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop multiple columns by passing in multiple columns as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count', 'This Long Column-Name']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing a Column’s Type (cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint'),\n",
       " ('count2', 'string')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"String\")).dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|       United States|             Romania|   15|\n",
      "|       United States|             Ireland|  344|\n",
      "|               Egypt|       United States|   15|\n",
      "|       United States|               India|   62|\n",
      "|       United States|             Grenada|   62|\n",
      "|          Costa Rica|       United States|  588|\n",
      "|             Senegal|       United States|   40|\n",
      "|       United States|        Sint Maarten|  325|\n",
      "|       United States|    Marshall Islands|   39|\n",
      "|              Guyana|       United States|   64|\n",
      "|            Anguilla|       United States|   41|\n",
      "|             Bolivia|       United States|   30|\n",
      "|       United States|            Paraguay|    6|\n",
      "|             Algeria|       United States|    4|\n",
      "|Turks and Caicos ...|       United States|  230|\n",
      "|               Italy|       United States|  382|\n",
      "|       United States|Federated States ...|   69|\n",
      "|       United States|              Russia|  161|\n",
      "|            Pakistan|       United States|   12|\n",
      "|       United States|         Netherlands|  660|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|        Costa Rica|      United States|   588|\n",
      "|     United States|        Netherlands|   660|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|       El Salvador|      United States|   561|\n",
      "|            Mexico|      United States|  7140|\n",
      "|     United States|         Costa Rica|   608|\n",
      "|          Colombia|      United States|   873|\n",
      "|     United States|            Jamaica|   712|\n",
      "|            Panama|      United States|   510|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|     United States|              China|   920|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|     United States|      United States|370002|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States|        South Korea|   827|\n",
      "|     United States|        El Salvador|   508|\n",
      "|            Canada|      United States|  8399|\n",
      "|           Jamaica|      United States|   666|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|             Japan|      United States|  1548|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.filter(\"count>2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|        Costa Rica|      United States|   588|\n",
      "|     United States|        Netherlands|   660|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|       El Salvador|      United States|   561|\n",
      "|            Mexico|      United States|  7140|\n",
      "|     United States|         Costa Rica|   608|\n",
      "|          Colombia|      United States|   873|\n",
      "|     United States|            Jamaica|   712|\n",
      "|            Panama|      United States|   510|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|     United States|              China|   920|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|     United States|      United States|370002|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States|        South Korea|   827|\n",
      "|     United States|        El Salvador|   508|\n",
      "|            Canada|      United States|  8399|\n",
      "|           Jamaica|      United States|   666|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|             Japan|      United States|  1548|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"count < 2\").show(2)\n",
    "df.where(col(\"count\")>500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Samples\n",
    "Sometimes, you might just want to sample some random records from your DataFrame. You can\n",
    "do this by using the sample method on a DataFrame, which makes it possible for you to specify\n",
    "a fraction of rows to extract from a DataFrame and whether you’d like to sample with or without\n",
    "replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|   DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|       United States|             Romania|   15|\n",
      "|       United States|             Croatia|    1|\n",
      "|       United States|               India|   62|\n",
      "|       United States|           Singapore|    1|\n",
      "|       United States|             Grenada|   62|\n",
      "|             Senegal|       United States|   40|\n",
      "|             Moldova|       United States|    1|\n",
      "|       United States|    Marshall Islands|   39|\n",
      "|              Guyana|       United States|   64|\n",
      "|             Bolivia|       United States|   30|\n",
      "|       United States|            Paraguay|    6|\n",
      "|       United States|           Gibraltar|    1|\n",
      "|Saint Vincent and...|       United States|    1|\n",
      "|               Italy|       United States|  382|\n",
      "|       United States|Federated States ...|   69|\n",
      "|       United States|              Russia|  161|\n",
      "|       United States|         Netherlands|  660|\n",
      "|          Luxembourg|       United States|  155|\n",
      "|            Honduras|       United States|  362|\n",
      "|         El Salvador|       United States|  561|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint], DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]]\n"
     ]
    }
   ],
   "source": [
    "print(dataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating and Appending Rows (Union)\n",
    "\n",
    "page - 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "Row(\"New Country\", \"Other Country\", 5),\n",
    "Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(newDF)\\\n",
    ".where(\"count = 1\")\\\n",
    ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortWithinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"json\").load(\"C:/Users/varsh/Spark-The-Definitive-Guide/data/flight-data/json/*-summary.json\")\\\n",
    ".sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
