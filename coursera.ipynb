{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "conf = SparkConf().set(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=log4j.properties\")\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "    # .config('spark.driver.extraClassPath',\"C:\\Users\\varsh\\spark\\spark-3.3.1-bin-hadoop3\\jars\\mysql-connector-java-8.0.13.jar\") \\\n",
    "    # .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.sql('show functions')\n",
    "f.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.show(388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = spark.read.csv('C:/Users/varsh/Sample_Data/AMZN.csv',header=True,inferSchema=True)\n",
    "amazon.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = spark.read.csv('C:/Users/varsh/Sample_Data/GOOG.csv',header=True,inferSchema=True)\n",
    "google.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla = spark.read.csv('C:/Users/varsh/Sample_Data/TSLA.csv',header=True,inferSchema=True)\n",
    "tesla.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents = spark.read.csv('C:/Users/varsh/Sample_Data/IncidentLevelCSV/Individual*',header=True,inferSchema=True)\n",
    "incidents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incidents.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.defaultParallelism) \n",
    "# default value is local[*] or total no of cores available in the spark cluster\n",
    "# Only 4 tasks runs at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.cache()\n",
    "incidents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emp_schema = StructType(\n",
    "    [StructField('Emp_id',IntegerType()),\n",
    "    StructField('Emp_name',StringType()),\n",
    "    StructField('Manager_id',StringType())]\n",
    ")\n",
    "\n",
    "employees = spark.read.schema(Emp_schema).csv('C:/Users/varsh/Sample_Data/employees.csv')\n",
    "employees.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Manager_schema = StructType(\n",
    "    [StructField('Manager_id',IntegerType()),\n",
    "    StructField('Manager_name',StringType())]\n",
    ")\n",
    "l = [(1,'Karthik'),(2,'Ganesh'),(3,'Thambi'),(4,'Mayils'),(5,'Surya')]\n",
    "# r = sc.parallelize(l)\n",
    "# r.collect()\n",
    "# manager = r.toDF()\n",
    "manager = spark.createDataFrame(data=l,schema=Manager_schema)\n",
    "manager.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = employees.join(manager,employees.Manager_id == manager.Manager_id)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('Emp_name').show()\n",
    "df1.select(df1.Emp_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('Emp_name').filter(df1.Manager_name == 'Surya').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select('Emp_name').filter(df1.Manager_name == 'Surya').orderBy(df1.Emp_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.select(df1.Manager_name).groupBy(df1.Manager_name).count().orderBy(df1.Manager_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.withColumn('Abbr',df1.Manager_name.substr(1,1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla.withColumn('Volume',tesla.Volume.cast(StringType())).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incidents.select('hour','total_offense').show()\n",
    "\n",
    "def func(i):\n",
    "    if i>1:\n",
    "        return 'Major'\n",
    "    else:\n",
    "        return 'Minor'\n",
    "incidents.withColumn('Statement',when(incidents.total_offense > 1,'Major').otherwise('Minor')).select('total_offense','statement').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark SQL statement with create or replace temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.createOrReplaceTempView('manager')\n",
    "spark.sql('select * from manager').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.createOrReplaceTempView('employees')\n",
    "spark.sql('select * from employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables in default').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('create database permanent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as permanent table in spark warehouse directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.write.saveAsTable('permanent.employees')\n",
    "# gets stored in spark-warehouse directory\n",
    "# storage system - local file system\n",
    "# metastore - embedded Derby\n",
    "# creation of permanent tables is not supported in spark 1.x without hive integration\n",
    "# spark by default stores data or reads data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables in permanent').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from external DB using JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql = spark.read.format('jdbc').option('url','jdbc:mysql://localhost:3306').\\\n",
    "    option('driver','com.mysql.cj.jdbc.Driver').\\\n",
    "    option('user','user').\\\n",
    "    option('password','pass').\\\n",
    "    option('query','select * from data.country').\\\n",
    "    load()\n",
    "\n",
    "mysql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to external DB using JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.write.format('jdbc').mode('overwrite').\\\n",
    "    option('url','jdbc:mysql://localhost:3306/data').\\\n",
    "    option('driver','com.mysql.cj.jdbc.Driver').\\\n",
    "    option('user','user').\\\n",
    "    option('password','pass').\\\n",
    "    option('dbtable','manager').\\\n",
    "    save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = spark.read.option('multiline',True).json(\"hdfs://127.0.0.1:9000/data/sample_data/example_2.json\")\n",
    "json_file.printSchema()\n",
    "# By default spark will read the JSON files as a single line, below one will not throw any error.\n",
    "# {'name':'Varshini','ID':1},\n",
    "# {'name':'Santhiya','ID':2}\n",
    "# For multiline, We need to provide the option as multiline\n",
    "\n",
    "json_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.github.com/users\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "apidata = requests.request('GET','https://api.github.com/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_api = json.dumps(apidata.json(),indent=4)\n",
    "# print(len(json_api))\n",
    "# print(json_api)\n",
    "# print(type(json_api))\n",
    "file = open('C:/Users/varsh/Sample_Data/rest_api','w')\n",
    "file.write(json_api)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_api_json = spark.read.option('multiline',True).json('C:/Users/varsh/Sample_Data/rest_api')\n",
    "df_api_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive --- Data (HDFS or distributed file system), Metadata (RDBMS)\n",
    "# Hive used HQL\n",
    "# It is a data warehouse used for analysis using SQL language\n",
    "# Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
    "\n",
    "# In hive>conf>hive-site.xml set the value of hive.execution.engine to spark or tez.\n",
    "\n",
    "\n",
    "# Configuration of Hive is done by placing your hive-site.xml, core-site.xml (for security configuration), and hdfs-site.xml (for HDFS configuration) file in conf/.\n",
    "\n",
    "# When not configured by the hive-site.xml, the context automatically creates metastore_db in the current directory and creates a directory configured by spark.sql.warehouse.dir, which defaults to the directory spark-warehouse in the current directory that the Spark application is started. Note that the hive.metastore.warehouse.dir property in hive-site.xml is deprecated since Spark 2.0.0. Instead, use spark.sql.warehouse.dir to specify the default location of database in warehouse. You may need to grant write privilege to the user who starts the Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spark.sparkContext.parallelize(['a','aa','a','aa','a','b','bb','bbb','a','aa','bbb'])\n",
    "# words.collect()\n",
    "# words.count()\n",
    "words.sortBy(lambda a:a).collect()\n",
    "words.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = words.map(lambda a:(a,1))\n",
    "word_map.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce By Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_reduce = word_map.reduceByKey(lambda a,b:a+b)\n",
    "word_reduce.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RDD from external text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.sparkContext.textFile('C:/Users/varsh/Sample_Data/varshini.txt')\n",
    "text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flat Map and Save as external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fm = text.flatMap(lambda a:a.split(' '))\n",
    "text_fm.saveAsTextFile('C:/Users/varsh/Sample_Data/word_count_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = spark.sparkContext.textFile('C:/Users/varsh/Sample_Data/word_count_result')\n",
    "text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes spark might read in unicode format u' in such cases we have to change the format to encode('utf-8')\n",
    "# To convert it into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disk - Slow (in TB)\n",
    "# Ram - Fast (in GB)\n",
    "# Cache - Fastest (in KB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
