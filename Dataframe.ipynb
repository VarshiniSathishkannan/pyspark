{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pyspark-m.us-central1-f.c.nodal-empire-382814.internal:33561\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4ef8c74a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataframe from a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_page_views  = spark.sparkContext.parallelize([\n",
    "    [\"en\", \"Statue_of_Liberty\", \"2022-01-01\", 263],\n",
    "    [\"en\", \"Replicas_of_the_Statue_of_Liberty\", \"2022-01-01\", 11],\n",
    "    [\"en\", \"Statue_of_Lucille_Ball\" ,\"2022-01-01\", 6],\n",
    "    [\"en\", \"Statue_of_Liberty_National_Monument\", \"2022-01-01\", 4],\n",
    "    [\"en\", \"Statue_of_Liberty_play\"  ,\"2022-01-01\", 3],  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['en', 'Statue_of_Liberty', '2022-01-01', 263],\n",
       " ['en', 'Replicas_of_the_Statue_of_Liberty', '2022-01-01', 11],\n",
       " ['en', 'Statue_of_Lucille_Ball', '2022-01-01', 6],\n",
       " ['en', 'Statue_of_Liberty_National_Monument', '2022-01-01', 4],\n",
       " ['en', 'Statue_of_Liberty_play', '2022-01-01', 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_page_views.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_page_views_df = sample_page_views.toDF(['Language_code','title','date','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+-----+\n",
      "|Language_code|               title|      date|count|\n",
      "+-------------+--------------------+----------+-----+\n",
      "|           en|   Statue_of_Liberty|2022-01-01|  263|\n",
      "|           en|Replicas_of_the_S...|2022-01-01|   11|\n",
      "+-------------+--------------------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_page_views_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------+----------+-----+\n",
      "|Language_code|title                              |date      |count|\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "|en           |Statue_of_Liberty                  |2022-01-01|263  |\n",
      "|en           |Replicas_of_the_Statue_of_Liberty  |2022-01-01|11   |\n",
      "|en           |Statue_of_Lucille_Ball             |2022-01-01|6    |\n",
      "|en           |Statue_of_Liberty_National_Monument|2022-01-01|4    |\n",
      "|en           |Statue_of_Liberty_play             |2022-01-01|3    |\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_page_views_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the RDD underlying sample_page_views_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Language_code='en', title='Statue_of_Liberty', date='2022-01-01', count=263),\n",
       " Row(Language_code='en', title='Replicas_of_the_Statue_of_Liberty', date='2022-01-01', count=11),\n",
       " Row(Language_code='en', title='Statue_of_Lucille_Ball', date='2022-01-01', count=6),\n",
       " Row(Language_code='en', title='Statue_of_Liberty_National_Monument', date='2022-01-01', count=4),\n",
       " Row(Language_code='en', title='Statue_of_Liberty_play', date='2022-01-01', count=3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_page_views_rdd_restored = sample_page_views_df.rdd\n",
    "sample_page_views_rdd_restored.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_page_views_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_page_views_rdd_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Language_code: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_page_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe from external source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.csv('/user/root/people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+-------+------+--------------------+--------------------+----------+---------------+\n",
      "|_c0|            _c1|    _c2|    _c3|   _c4|                 _c5|                 _c6|       _c7|            _c8|\n",
      "+---+---------------+-------+-------+------+--------------------+--------------------+----------+---------------+\n",
      "|  1|88F7B33d2bcf9f5| Shelby|Terrell|  Male|elijah57@example.net|001-084-906-7849x...|1945-10-26|Games developer|\n",
      "|  2|f90cD3E76f1A9b9|Phillip|Summers|Female|bethany14@example...|   214.112.6044x4913|1910-03-24| Phytotherapist|\n",
      "+---+---------------+-------+-------+------+--------------------+--------------------+----------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+------+---------------------+----------------------+-------------+---------------+\n",
      "|Index|User Id        |First Name|Last Name|Sex   |Email                |Phone                 |Date of birth|Job Title      |\n",
      "+-----+---------------+----------+---------+------+---------------------+----------------------+-------------+---------------+\n",
      "|1    |88F7B33d2bcf9f5|Shelby    |Terrell  |Male  |elijah57@example.net |001-084-906-7849x73518|1945-10-26   |Games developer|\n",
      "|2    |f90cD3E76f1A9b9|Phillip   |Summers  |Female|bethany14@example.com|214.112.6044x4913     |1910-03-24   |Phytotherapist |\n",
      "+-----+---------------+----------+---------+------+---------------------+----------------------+-------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df = people_df.toDF('Index','User Id','First Name','Last Name','Sex','Email','Phone','Date of birth','Job Title'\n",
    ")\n",
    "people_df.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: string (nullable = true)\n",
      " |-- User Id: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Date of birth: string (nullable = true)\n",
      " |-- Job Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: timestamp (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df = spark.read.csv('/user/root/people.csv',inferSchema=True) #header = True if we have first row as header\n",
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: timestamp (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df = spark.read.option('inferSchema',True).csv('/user/root/people.csv')\n",
    "people_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'string'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string'),\n",
       " ('_c5', 'string'),\n",
       " ('_c6', 'string'),\n",
       " ('_c7', 'timestamp'),\n",
       " ('_c8', 'string')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c8: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+------+--------------------+--------------------+-------------------+---------------+\n",
      "|Index|        User Id|First Name|Last Name|   Sex|               Email|               Phone|      Date of birth|      Job Title|\n",
      "+-----+---------------+----------+---------+------+--------------------+--------------------+-------------------+---------------+\n",
      "|    1|88F7B33d2bcf9f5|    Shelby|  Terrell|  Male|elijah57@example.net|001-084-906-7849x...|1945-10-26 00:00:00|Games developer|\n",
      "|    2|f90cD3E76f1A9b9|   Phillip|  Summers|Female|bethany14@example...|   214.112.6044x4913|1910-03-24 00:00:00| Phytotherapist|\n",
      "+-----+---------------+----------+---------+------+--------------------+--------------------+-------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df = people_df.toDF('Index','User Id','First Name','Last Name','Sex','Email','Phone','Date of birth','Job Title'\n",
    ")\n",
    "people_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DF from another DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|First Name|Last Name|\n",
      "+----------+---------+\n",
      "|    Shelby|  Terrell|\n",
      "|   Phillip|  Summers|\n",
      "|  Kristine|   Travis|\n",
      "|   Yesenia| Martinez|\n",
      "|      Lori|     Todd|\n",
      "|      Erin|      Day|\n",
      "| Katherine|     Buck|\n",
      "|   Ricardo|   Hinton|\n",
      "|      Dave|  Farrell|\n",
      "|    Isaiah|    Downs|\n",
      "|    Sheila|     Ross|\n",
      "|     Stacy|   Newton|\n",
      "|     Mandy|    Blake|\n",
      "|   Bridget|     Nash|\n",
      "|   Crystal|   Farmer|\n",
      "|    Thomas|   Knight|\n",
      "|   Maurice|   Rangel|\n",
      "|     Frank|  Meadows|\n",
      "|     Alvin|     Paul|\n",
      "|     Jared| Mitchell|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_name_df = people_df['First Name','Last Name']\n",
    "people_name_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------+------+--------------------+--------------------+-------------------+---------------+\n",
      "|        User Id|First Name|Last Name|   Sex|               Email|               Phone|      Date of birth|      Job Title|\n",
      "+---------------+----------+---------+------+--------------------+--------------------+-------------------+---------------+\n",
      "|88F7B33d2bcf9f5|    Shelby|  Terrell|  Male|elijah57@example.net|001-084-906-7849x...|1945-10-26 00:00:00|Games developer|\n",
      "|f90cD3E76f1A9b9|   Phillip|  Summers|Female|bethany14@example...|   214.112.6044x4913|1910-03-24 00:00:00| Phytotherapist|\n",
      "+---------------+----------+---------+------+--------------------+--------------------+-------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "without_index = people_df.drop('Index')\n",
    "without_index.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+---------+------+--------------------+--------------------+-------------------+------------------+\n",
      "|             Id|First Name|Last Name|   Sex|               Email|               Phone|      Date of birth|         Job Title|\n",
      "+---------------+----------+---------+------+--------------------+--------------------+-------------------+------------------+\n",
      "|88F7B33d2bcf9f5|    Shelby|  Terrell|  Male|elijah57@example.net|001-084-906-7849x...|1945-10-26 00:00:00|   Games developer|\n",
      "|f90cD3E76f1A9b9|   Phillip|  Summers|Female|bethany14@example...|   214.112.6044x4913|1910-03-24 00:00:00|    Phytotherapist|\n",
      "|DbeAb8CcdfeFC2c|  Kristine|   Travis|  Male|bthompson@example...|        277.609.7938|1992-07-02 00:00:00|         Homeopath|\n",
      "|A31Bee3c201ef58|   Yesenia| Martinez|  Male|kaitlinkaiser@exa...|        584.094.6111|2017-08-03 00:00:00| Market researcher|\n",
      "|1bA7A3dc874da3c|      Lori|     Todd|  Male|buchananmanuel@ex...|   689-207-3558x7233|1938-12-01 00:00:00|Veterinary surgeon|\n",
      "+---------------+----------+---------+------+--------------------+--------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "without_index = without_index.withColumnRenamed('User Id','Id')\n",
    "without_index.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male = without_index.filter(without_index.Sex == 'Male')\n",
    "male.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female = without_index.filter(without_index.Sex == 'Female')\n",
    "female.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|First Name|Last Name|\n",
      "+----------+---------+\n",
      "|   Phillip|  Summers|\n",
      "| Katherine|     Buck|\n",
      "|    Sheila|     Ross|\n",
      "|   Bridget|     Nash|\n",
      "|    Thomas|   Knight|\n",
      "|     Jared| Mitchell|\n",
      "|Jacqueline|   Norton|\n",
      "|   Colleen| Hatfield|\n",
      "|    Janice|   Rhodes|\n",
      "|    Alfred|   Mcneil|\n",
      "|  Brittney|     Vega|\n",
      "|    Isaiah|  Camacho|\n",
      "|    Bonnie|  Andrews|\n",
      "|   Brandon|  Schmidt|\n",
      "|   Jackson|   Sparks|\n",
      "|      Gene|     Rich|\n",
      "|   Cynthia|  Wiggins|\n",
      "|     Tanya| Mckinney|\n",
      "|   Matthew|    Stone|\n",
      "|      Kirk|    Walsh|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "female.select('First Name','Last Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|Female|   53|\n",
      "|  Male|   47|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "without_index.groupBy('Sex').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|  Male|   47|\n",
      "|Female|   53|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "without_index.groupBy('Sex').count().orderBy('count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before querying a DataFrame with SQL in Spark, it must be saved to the SparkSession’s catalog. \n",
    "The following code saves the DataFrame as a local temporary view in memory. \n",
    "As long as the current SparkSession is active, we can use SparkSession.sql() to query it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+------+---------------------+----------------------+-------------------+---------------+\n",
      "|Index|User Id        |First Name|Last Name|Sex   |Email                |Phone                 |Date of birth      |Job Title      |\n",
      "+-----+---------------+----------+---------+------+---------------------+----------------------+-------------------+---------------+\n",
      "|1    |88F7B33d2bcf9f5|Shelby    |Terrell  |Male  |elijah57@example.net |001-084-906-7849x73518|1945-10-26 00:00:00|Games developer|\n",
      "|2    |f90cD3E76f1A9b9|Phillip   |Summers  |Female|bethany14@example.com|214.112.6044x4913     |1910-03-24 00:00:00|Phytotherapist |\n",
      "+-----+---------------+----------+---------+------+---------------------+----------------------+-------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'select * from people'\n",
    "spark.sql(query).show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn and inbuilt functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "cleaned_df = people_df.withColumn('Date of birth',f.to_date(people_df['Date of birth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: integer (nullable = true)\n",
      " |-- User Id: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Date of birth: date (nullable = true)\n",
      " |-- Job Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+----+--------------------+--------------------+-------------+---------------+\n",
      "|Index|        User Id|First Name|Last Name| Sex|               Email|               Phone|Date of birth|      Job Title|\n",
      "+-----+---------------+----------+---------+----+--------------------+--------------------+-------------+---------------+\n",
      "|    1|88F7B33d2bcf9f5|    Shelby|  Terrell|Male|elijah57@example.net|001-084-906-7849x...|   1945-10-26|Games developer|\n",
      "+-----+---------------+----------+---------+----+--------------------+--------------------+-------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+----+--------------------+--------------------+-------------+---------------+--------------+\n",
      "|Index|        User Id|First Name|Last Name| Sex|               Email|               Phone|Date of birth|      Job Title|     Full Name|\n",
      "+-----+---------------+----------+---------+----+--------------------+--------------------+-------------+---------------+--------------+\n",
      "|    1|88F7B33d2bcf9f5|    Shelby|  Terrell|Male|elijah57@example.net|001-084-906-7849x...|   1945-10-26|Games developer|Shelby Terrell|\n",
      "+-----+---------------+----------+---------+----+--------------------+--------------------+-------------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = cleaned_df.withColumn('Full Name',f.concat_ws(' ',cleaned_df['First Name'],cleaned_df['Last Name']))\n",
    "cleaned_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----+--------------------+--------------------+-------------+---------------+\n",
      "|        User ID|     Full Name| Sex|               Email|               Phone|Date of birth|      Job Title|\n",
      "+---------------+--------------+----+--------------------+--------------------+-------------+---------------+\n",
      "|88F7B33d2bcf9f5|Shelby Terrell|Male|elijah57@example.net|001-084-906-7849x...|   1945-10-26|Games developer|\n",
      "+---------------+--------------+----+--------------------+--------------------+-------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = cleaned_df['User ID','Full Name','Sex','Email','Phone','Date of birth','Job Title']\n",
    "cleaned_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.write.csv('/output/',mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "recover = spark.read.csv('/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+------+--------------------+--------------------+----------+---------------+\n",
      "|            _c0|            _c1|   _c2|                 _c3|                 _c4|       _c5|            _c6|\n",
      "+---------------+---------------+------+--------------------+--------------------+----------+---------------+\n",
      "|88F7B33d2bcf9f5| Shelby Terrell|  Male|elijah57@example.net|001-084-906-7849x...|1945-10-26|Games developer|\n",
      "|f90cD3E76f1A9b9|Phillip Summers|Female|bethany14@example...|   214.112.6044x4913|1910-03-24| Phytotherapist|\n",
      "+---------------+---------------+------+--------------------+--------------------+----------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recover.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It looks like this file didn’t retain information about column headers or datatypes. \n",
    "Unfortunately, there’s no way for a CSV to retain information about its format. \n",
    "Each time we read it, we’ll need to tell Spark exactly how it must be processed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there is a file format called “Parquet” that’s specially designed for big data and solves this problem among many others. \n",
    "Parquet offers efficient data compression, is faster to perform analysis on than CSV, and preserves information about a dataset’s schema. \n",
    "Let’s try saving and re-reading this file to and from Parquet instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Attribute name \"User ID\" contains invalid character(s) among \" ,;{}()\\\\n\\\\t=\". Please use alias to rename it.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o543.parquet.\n: org.apache.spark.sql.AnalysisException: Attribute name \"User ID\" contains invalid character(s) among \" ,;{}()\\n\\t=\". Please use alias to rename it.;\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:583)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:574)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:449)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:449)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:449)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.prepareWrite(ParquetFileFormat.scala:111)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:103)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:170)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:136)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:160)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:310)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-a04b8ebe1ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/output/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Attribute name \"User ID\" contains invalid character(s) among \" ,;{}()\\\\n\\\\t=\". Please use alias to rename it.;'"
     ]
    }
   ],
   "source": [
    "cleaned_df.write.parquet('/output/',mode='overwrite') # throwing error as we cannot have space in column name in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----+--------------------+--------------------+-------------+---------------+\n",
      "|        User ID|     Full Name| Sex|               Email|               Phone|Date of birth|      Job Title|\n",
      "+---------------+--------------+----+--------------------+--------------------+-------------+---------------+\n",
      "|88F7B33d2bcf9f5|Shelby Terrell|Male|elijah57@example.net|001-084-906-7849x...|   1945-10-26|Games developer|\n",
      "+---------------+--------------+----+--------------------+--------------------+-------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.withColumnRenamed('User ID','ID').withColumnRenamed('Full Name','Full_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.withColumnRenamed('Date of birth','DOB').withColumnRenamed('Job Title','Job_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.write.parquet('/output/',mode='overwrite') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
