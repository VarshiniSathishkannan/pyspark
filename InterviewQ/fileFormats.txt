Spark SQL defaults to reading and writing data in Snappy compressed Parquet files. 

Below file formats are available in spark by default:
1. Parquet
2. CSV
3. ORC
4. JSON
5. AVRO

Parquet is in efficient columnar file format that enables Spark to only read the data it needs to execute an application. This is an important advantage when working with large datasets. 

Other columnar formats, such as Apache ORC, also perform well.

For non-columnar data, Apache Avro provides an efficient binary-row file format. Although typically slower than Parquet, Avro's performance is better than text based formats,such as CSV or JSON.
