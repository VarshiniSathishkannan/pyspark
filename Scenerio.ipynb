{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 dataframes with uneven columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Merge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "input1AddDF = input1DF.withColumn('Gender',lit(None))\n",
    "input1AddDF.show()\n",
    "\n",
    "result = input1AddDF.union(input2DF)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField('Name',StringType(),True), # 3rd option is nullable is true or not\n",
    "    StructField('Age',IntegerType(),True),\n",
    "    StructField('Gender',StringType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).schema(schema).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.option('header',True).csv('sample_data\\input2.csv',schema=schema)\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.union(input2DF)\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.join(input2DF,on=['Name','Age'],how='outer')\n",
    "result.show()\n",
    "\n",
    "result = input1DF.join(input2DF,[input1DF.Name==input2DF.Name,input1DF.Age==input2DF.Age],how='outer').select(input1DF.Name,input1DF.Age,input2DF.Gender)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best approach - Automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "listA = set(input1DF.columns)-set(input2DF.columns)\n",
    "listB = set(input2DF.columns)-set(input1DF.columns)\n",
    "\n",
    "for i in listA:\n",
    "    input2DF = input2DF.withColumn(i,lit(None))\n",
    "\n",
    "for i in listB:\n",
    "    input1DF = input1DF.withColumn(i,lit(None))\n",
    "    \n",
    "resut = input1DF.union(input2DF)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply line break every 5th occurance from | delimited input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |chk                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "\n",
    "input = spark.read.csv('sample_data\\input.txt')\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn(\"chk\",regexp_replace(\"_c0\",\"(.*?\\\\|){4}\",\"$0-\"))\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn('col_explode',explode(split('chk','\\|-')))\n",
    "\n",
    "input.select(input.col_explode).show()\n",
    "\n",
    "result = input.select(input.col_explode)\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|split(col_explode, \\|, -1)|\n",
      "+--------------------------+\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(split('col_explode','\\|')).show()\n",
    "\n",
    "result.rdd.map(lambda i:i).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.rdd.map(lambda i:len(i)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+----+---+\n",
      "|Name|Qualification|S.no|Age|\n",
      "+----+-------------+----+---+\n",
      "|   A|           BE|   1| 25|\n",
      "|   B|          BSC|   2| 27|\n",
      "|   A|           BE|   1| 25|\n",
      "|   B|          BSC|   2| 27|\n",
      "|   A|           BE|   1| 25|\n",
      "|   B|          BSC|   2| 27|\n",
      "|   A|           BE|   1| 25|\n",
      "|   B|          BSC|   2| 27|\n",
      "|   A|           BE|   1| 25|\n",
      "|   B|          BSC|   2| 27|\n",
      "+----+-------------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_rdd = result.rdd.map(lambda i:i[0].split('|'))\n",
    "\n",
    "result = result_rdd.toDF(['Name','Qualification','S.no','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files recursively under Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                      |\n",
      "+--------+---+--------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "+--------+---+--------------------------------------------------------------+\n",
      "\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                             |\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Mo      |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Arv     |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Rish    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Anus    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Gaya    |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                             |\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Mo      |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Arv     |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Rish    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Anus    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Gaya    |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).load('sample_data/recursive').withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "paths = [\n",
    "    'sample_data/recursive',\n",
    "    'sample_data/recursive/level1'\n",
    "]\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).load(paths).withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).option('recursiveFileLookup',True).load(paths).withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one returns results faster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrameReader.csv() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter()\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msort()\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrameReader.csv() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "#1\n",
    "df = spark.read.csv()\n",
    "df = df.filter()\n",
    "df = df.sort()\n",
    "df.count()\n",
    "#2\n",
    "df = spark.read.csv()\n",
    "df = df.sort()\n",
    "df = df.filter()\n",
    "df.count()\n",
    "#3\n",
    "df = spark.read.csv()\n",
    "df.sort()\n",
    "df = df.filter()\n",
    "df = df.sort()\n",
    "df.count()\n",
    "#4\n",
    "df = spark.read.csv()\n",
    "df.sort()\n",
    "df = df.sort()\n",
    "df = df.filter()\n",
    "df.count()\n",
    "\n",
    "# Predicate pushdown pushes the filter to the start even if sort is given first\n",
    "\n",
    "# option A performs better, Caching took time, this is not the right case for doing caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferschema Vs Schema definition\n",
    "\n",
    "Always define schema manually since inferschema scans the table twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|768805383|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|     12691.0|                777|        11914.0|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|818770008|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|      8256.0|                864|         7392.0|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|713982108|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|      3418.0|                  0|         3418.0|               2.594|           1887|            20|              2.333|                  0.0|\n",
      "|769911858|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|      3313.0|               2517|          796.0|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|709106358|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|      4716.0|                  0|         4716.0|               2.175|            816|            28|                2.5|                  0.0|\n",
      "|713061558|Existing Customer|          44|     M|              2|       Graduate|       Married|    $40K - $60K|         Blue|            36|                       3|                     1|                    2|      4010.0|               1247|         2763.0|               1.376|           1088|            24|              0.846|                0.311|\n",
      "|810347208|Existing Customer|          51|     M|              4|        Unknown|       Married|        $120K +|         Gold|            46|                       6|                     1|                    3|     34516.0|               2264|        32252.0|               1.975|           1330|            31|              0.722|                0.066|\n",
      "|818906208|Existing Customer|          32|     M|              0|    High School|       Unknown|    $60K - $80K|       Silver|            27|                       2|                     2|                    2|     29081.0|               1396|        27685.0|               2.204|           1538|            36|              0.714|                0.048|\n",
      "|710930508|Existing Customer|          37|     M|              3|     Uneducated|        Single|    $60K - $80K|         Blue|            36|                       5|                     2|                    0|     22352.0|               2517|        19835.0|               3.355|           1350|            24|              1.182|                0.113|\n",
      "|719661558|Existing Customer|          48|     M|              2|       Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     3|                    3|     11656.0|               1677|         9979.0|               1.524|           1441|            32|              0.882|                0.144|\n",
      "|708790833|Existing Customer|          42|     M|              5|     Uneducated|       Unknown|        $120K +|         Blue|            31|                       5|                     3|                    2|      6748.0|               1467|         5281.0|               0.831|           1201|            42|               0.68|                0.217|\n",
      "|710821833|Existing Customer|          65|     M|              1|        Unknown|       Married|    $40K - $60K|         Blue|            54|                       6|                     2|                    3|      9095.0|               1587|         7508.0|               1.433|           1314|            26|              1.364|                0.174|\n",
      "|710599683|Existing Customer|          56|     M|              1|        College|        Single|   $80K - $120K|         Blue|            36|                       3|                     6|                    0|     11751.0|                  0|        11751.0|               3.397|           1539|            17|               3.25|                  0.0|\n",
      "|816082233|Existing Customer|          35|     M|              3|       Graduate|       Unknown|    $60K - $80K|         Blue|            30|                       5|                     1|                    3|      8547.0|               1666|         6881.0|               1.163|           1311|            33|                2.0|                0.195|\n",
      "|712396908|Existing Customer|          57|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            48|                       5|                     2|                    2|      2436.0|                680|         1756.0|                1.19|           1570|            29|              0.611|                0.279|\n",
      "|714885258|Existing Customer|          44|     M|              4|        Unknown|       Unknown|   $80K - $120K|         Blue|            37|                       5|                     1|                    2|      4234.0|                972|         3262.0|               1.707|           1348|            27|                1.7|                 0.23|\n",
      "|709967358|Existing Customer|          48|     M|              4|  Post-Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     2|                    3|     30367.0|               2362|        28005.0|               1.708|           1671|            27|              0.929|                0.078|\n",
      "|753327333|Existing Customer|          41|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            34|                       4|                     4|                    1|     13535.0|               1291|        12244.0|               0.653|           1028|            21|              1.625|                0.095|\n",
      "|806160108|Existing Customer|          61|     M|              1|    High School|       Married|    $40K - $60K|         Blue|            56|                       2|                     2|                    3|      3193.0|               2517|          676.0|               1.831|           1336|            30|              1.143|                0.788|\n",
      "|709327383|Existing Customer|          45|     F|              2|       Graduate|       Married|        Unknown|         Blue|            37|                       6|                     1|                    2|     14470.0|               1157|        13313.0|               0.966|           1207|            21|              0.909|                 0.08|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/BankChurners.csv')\n",
    "\n",
    "bank.show()\n",
    "\n",
    "bank.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = bank.repartition(4)\n",
    "input.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|partition_id|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|708721533|Existing Customer|          56|     F|              1|       Graduate|       Married| Less than $40K|         Blue|            36|                       2|                     1|                    2|      5158.0|               1148|         4010.0|               0.637|          13113|           128|              0.882|                0.223|           0|\n",
      "|720362058|Attrited Customer|          57|     M|              2|     Uneducated|        Single|    $60K - $80K|       Silver|            49|                       4|                     3|                    3|     32182.0|                  0|        32182.0|                0.52|           1937|            46|              0.704|                  0.0|           0|\n",
      "|708360108|Existing Customer|          50|     M|              1|        College|        Single|    $40K - $60K|         Blue|            41|                       3|                     1|                    1|      2800.0|               1338|         1462.0|               0.818|           4869|            90|                0.8|                0.478|           0|\n",
      "|713090658|Existing Customer|          58|     F|              2|     Uneducated|        Single|        Unknown|         Blue|            44|                       1|                     2|                    2|      2599.0|               1407|         1192.0|               0.727|           8061|            98|              0.531|                0.541|           0|\n",
      "|714943233|Existing Customer|          49|     M|              2|      Doctorate|       Married|    $40K - $60K|         Blue|            31|                       2|                     2|                    2|      1915.0|               1545|          370.0|               0.775|           3911|            80|              0.778|                0.807|           0|\n",
      "|717052158|Existing Customer|          39|     M|              3|    High School|        Single|    $60K - $80K|         Blue|            32|                       3|                     2|                    2|     21718.0|               2386|        19332.0|               0.743|           3492|            66|              0.692|                 0.11|           0|\n",
      "|794227308|Existing Customer|          56|     M|              2|        College|       Married|   $80K - $120K|         Blue|            50|                       1|                     1|                    3|     10214.0|               1433|         8781.0|                0.64|           4493|            79|              0.881|                 0.14|           0|\n",
      "|710932683|Existing Customer|          55|     F|              2|    High School|        Single| Less than $40K|         Blue|            36|                       2|                     1|                    2|      6394.0|                  0|         6394.0|               0.656|           3470|            69|              0.917|                  0.0|           0|\n",
      "|712347258|Existing Customer|          43|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            35|                       3|                     3|                    2|      2209.0|                  0|         2209.0|               0.749|           4419|            68|              0.619|                  0.0|           0|\n",
      "|713205108|Existing Customer|          42|     F|              4|       Graduate|       Married|    $40K - $60K|         Blue|            27|                       3|                     1|                    1|      1950.0|               1062|          888.0|                0.48|           3873|            76|              0.854|                0.545|           0|\n",
      "|713808633|Existing Customer|          31|     F|              0|     Uneducated|      Divorced| Less than $40K|       Silver|            23|                       2|                     2|                    1|     10850.0|               1873|         8977.0|               0.995|          13794|           127|              0.789|                0.173|           0|\n",
      "|826168083|Existing Customer|          49|     F|              4|        Unknown|      Divorced| Less than $40K|         Blue|            45|                       3|                     3|                    3|      1757.0|               1307|          450.0|               0.769|           4750|            72|              0.756|                0.744|           0|\n",
      "|721258158|Existing Customer|          65|     F|              1|     Uneducated|        Single|        Unknown|         Blue|            51|                       3|                     3|                    5|     11037.0|               1491|         9546.0|               0.651|           2429|            63|               0.75|                0.135|           0|\n",
      "|713906508|Existing Customer|          42|     F|              3|        Unknown|        Single| Less than $40K|         Blue|            36|                       6|                     3|                    3|      2837.0|                834|         2003.0|               0.427|           3892|            69|              0.408|                0.294|           0|\n",
      "|740784258|Existing Customer|          47|     F|              2|        College|        Single| Less than $40K|         Blue|            40|                       4|                     3|                    3|      1789.0|               1088|          701.0|               0.683|           4191|            90|              0.525|                0.608|           0|\n",
      "|717504258|Existing Customer|          38|     M|              2|        College|       Married|    $40K - $60K|         Blue|            25|                       4|                     1|                    4|      3735.0|               1199|         2536.0|               0.886|           1573|            35|               0.75|                0.321|           0|\n",
      "|717529233|Attrited Customer|          40|     F|              3|       Graduate|        Single| Less than $40K|         Blue|            30|                       4|                     3|                    2|      3319.0|                  0|         3319.0|               0.262|           4840|            44|              0.375|                  0.0|           0|\n",
      "|716421633|Attrited Customer|          43|     F|              3|       Graduate|       Married| Less than $40K|         Blue|            36|                       4|                     2|                    4|      3523.0|                  0|         3523.0|               0.973|           3174|            44|              0.222|                  0.0|           0|\n",
      "|715593783|Existing Customer|          52|     M|              1|        Unknown|       Married|   $80K - $120K|         Blue|            36|                       4|                     2|                    2|     14858.0|               1594|        13264.0|                0.51|           4286|            72|              0.636|                0.107|           0|\n",
      "|717098208|Existing Customer|          48|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            41|                       5|                     2|                    2|      5848.0|                950|         4898.0|               0.726|           1961|            51|              0.545|                0.162|           0|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 2531|\n",
      "|           1| 2532|\n",
      "|           2| 2532|\n",
      "|           3| 2532|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "input = input.withColumn('partition_id',spark_partition_id())\n",
    "input.show()\n",
    "\n",
    "countByPartition = input.groupBy(input.partition_id).count()\n",
    "countByPartition.show()\n",
    "\n",
    "# Data is evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = bank.repartition(200,\"Card_Category\")\n",
    "input.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|partition_id|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|788386908|Existing Customer|          48|     M|              3|       Graduate|       Unknown|   $80K - $120K|     Platinum|            41|                       3|                     2|                    2|     34516.0|               1531|        32985.0|               0.862|           1156|            29|              1.071|                0.044|          38|\n",
      "|779519058|Existing Customer|          51|     F|              3|  Post-Graduate|      Divorced|        Unknown|     Platinum|            34|                       3|                     1|                    2|     34516.0|               1578|        32938.0|               0.725|           1929|            40|              0.481|                0.046|          38|\n",
      "|708654933|Attrited Customer|          51|     F|              2|      Doctorate|       Married| Less than $40K|     Platinum|            42|                       3|                     2|                    3|     15987.0|                193|        15794.0|               0.435|           2021|            46|              0.394|                0.012|          38|\n",
      "|714754533|Existing Customer|          51|     M|              0|      Doctorate|        Single|        $120K +|     Platinum|            44|                       3|                     2|                    3|     34516.0|               1925|        32591.0|               0.764|           8012|            87|              0.582|                0.056|          38|\n",
      "|714077583|Attrited Customer|          43|     M|              3|  Post-Graduate|       Married|    $40K - $60K|     Platinum|            31|                       2|                     3|                    4|     23981.0|                593|        23388.0|               0.987|           4758|            65|              0.512|                0.025|          38|\n",
      "|709269708|Existing Customer|          45|     M|              4|       Graduate|        Single|        $120K +|     Platinum|            34|                       2|                     2|                    1|     34516.0|               1488|        33028.0|               0.732|           7281|            95|              0.532|                0.043|          38|\n",
      "|719071158|Attrited Customer|          54|     F|              0|       Graduate|        Single|        Unknown|     Platinum|            38|                       2|                     2|                    2|     34516.0|                  0|        34516.0|               0.695|           3901|            54|              0.421|                  0.0|          38|\n",
      "|711364233|Existing Customer|          41|     M|              4|        Unknown|       Married|    $60K - $80K|     Platinum|            26|                       1|                     1|                    2|     34516.0|               1559|        32957.0|               0.759|           8888|           104|              0.763|                0.045|          38|\n",
      "|823848183|Existing Customer|          39|     F|              1|        Unknown|        Single| Less than $40K|     Platinum|            35|                       1|                     2|                    3|     15987.0|               1494|        14493.0|               0.731|           8438|            92|               0.84|                0.093|          38|\n",
      "|714190983|Attrited Customer|          51|     F|              2|       Graduate|        Single|        Unknown|     Platinum|            32|                       2|                     3|                    3|     34516.0|                531|        33985.0|                0.98|           5418|            65|              0.711|                0.015|          38|\n",
      "|770848308|Existing Customer|          45|     M|              2|       Graduate|       Married|    $60K - $80K|     Platinum|            31|                       2|                     2|                    1|     34516.0|               1308|        33208.0|               0.746|           8773|           105|               0.78|                0.038|          38|\n",
      "|713870958|Existing Customer|          56|     F|              3|     Uneducated|        Single|        Unknown|     Platinum|            46|                       2|                     3|                    2|     34516.0|                  0|        34516.0|               0.887|           8416|            93|              0.632|                  0.0|          38|\n",
      "|709319658|Attrited Customer|          48|     F|              4|       Graduate|        Single| Less than $40K|     Platinum|            37|                       5|                     3|                    4|     15987.0|                  0|        15987.0|               0.827|           7681|            71|               0.69|                  0.0|          38|\n",
      "|715017858|Existing Customer|          44|     M|              2|     Uneducated|       Married|        $120K +|     Platinum|            36|                       2|                     1|                    3|     34516.0|               1421|        33095.0|               0.744|          14465|           114|              0.754|                0.041|          38|\n",
      "|771626283|Existing Customer|          54|     M|              2|  Post-Graduate|       Married|    $60K - $80K|     Platinum|            42|                       1|                     3|                    1|     34516.0|               1996|        32520.0|               0.988|          15033|           114|               0.81|                0.058|          38|\n",
      "|710362233|Existing Customer|          48|     M|              3|        Unknown|       Unknown|   $80K - $120K|     Platinum|            40|                       1|                     2|                    3|     34516.0|               1723|        32793.0|               0.628|          13853|            98|              0.782|                 0.05|          38|\n",
      "|709814883|Existing Customer|          45|     F|              2|    High School|        Single| Less than $40K|     Platinum|            36|                       1|                     3|                    1|     15987.0|               2262|        13725.0|               0.714|          15513|           127|              0.649|                0.141|          38|\n",
      "|794494308|Existing Customer|          42|     M|              3|     Uneducated|       Married|        $120K +|     Platinum|            23|                       3|                     4|                    3|     34516.0|               2070|        32446.0|                0.88|          13781|           102|              0.545|                 0.06|          38|\n",
      "|709969758|Existing Customer|          43|     M|              3|       Graduate|        Single|    $60K - $80K|     Platinum|            35|                       4|                     3|                    3|     34516.0|               1774|        32742.0|               0.667|          13966|           115|              0.667|                0.051|          38|\n",
      "|714949758|Existing Customer|          51|     F|              3|       Graduate|        Single|        Unknown|     Platinum|            42|                       3|                     1|                    4|     34516.0|               1913|        32603.0|               0.851|          16712|           123|              0.708|                0.055|          38|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|          38|   20|\n",
      "|          94|  555|\n",
      "|         128| 9436|\n",
      "|         146|  116|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = input.withColumn('partition_id',spark_partition_id())\n",
    "input.show()\n",
    "\n",
    "countByPartition = input.groupBy(input.partition_id).count()\n",
    "countByPartition.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column methods\n",
    "\n",
    "\n",
    "Method\tReturns\tUse Case\n",
    "df.columns\tList of strings\tQuick column list\n",
    "df.schema.names\tList of strings\tSame as above\n",
    "df.dtypes\tList of (name, type) tuples\tView names + types\n",
    "df.schema\tStructType\tFull schema object\n",
    "df.printSchema()\tPrinted tree\tVisual inspection\n",
    "[f.name for f in df.schema.fields]\tList of names\tCustom logic over schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate expriy date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+\n",
      "| id|rechargeDate|validity|\n",
      "+---+------------+--------+\n",
      "|  1|    20200511|      20|\n",
      "|  2|    20200119|      13|\n",
      "|  3|    20200405|     120|\n",
      "+---+------------+--------+\n",
      "\n",
      "StructType([StructField('id', IntegerType(), True), StructField('rechargeDate', IntegerType(), True), StructField('validity', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "exp_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/expiry.csv')\n",
    "\n",
    "exp_df.show()\n",
    "print(exp_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+----------+\n",
      "| id|rechargeDate|validity|      date|\n",
      "+---+------------+--------+----------+\n",
      "|  1|    20200511|      20|2020-05-11|\n",
      "|  2|    20200119|      13|2020-01-19|\n",
      "|  3|    20200405|     120|2020-04-05|\n",
      "+---+------------+--------+----------+\n",
      "\n",
      "+---+------------+--------+----------+----------+\n",
      "| id|rechargeDate|validity|      date|expiryDate|\n",
      "+---+------------+--------+----------+----------+\n",
      "|  1|    20200511|      20|2020-05-11|2020-05-31|\n",
      "|  2|    20200119|      13|2020-01-19|2020-02-01|\n",
      "|  3|    20200405|     120|2020-04-05|2020-08-03|\n",
      "+---+------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,to_date,col,expr\n",
    "\n",
    "exp_df = exp_df.withColumn('date',to_date(col('rechargeDate').cast('string'),\"yyyyMMdd\"))\n",
    "exp_df.show()\n",
    "\n",
    "exp_df = exp_df.withColumn('expiryDate',date_add('date','validity'))\n",
    "exp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 complex dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = true)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+----------------+----+\n",
      "|       Education|Name|\n",
      "+----------------+----+\n",
      "|{28, BCOM, 2013}|Azar|\n",
      "|  {24, BE, 2021}|Amol|\n",
      "+----------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Education', 'Name']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1 = spark.read.json('sample_data/input1.json',multiLine=True)\n",
    "json2 = spark.read.json('sample_data/input2.json',multiLine=True)\n",
    "\n",
    "\n",
    "json1.printSchema()\n",
    "json1.show()\n",
    "json1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Education', StructType([StructField('Age', LongType(), True), StructField('Qualification', StringType(), True), StructField('Year', LongType(), True)]), True), StructField('Name', StringType(), True)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, lit, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def flatten_struct(schema, prefix=\"\"):\n",
    "    result = []\n",
    "    for elem in schema:\n",
    "        if isinstance(elem.dataType,StructType):\n",
    "            result+=flatten_struct(elem.dataType, prefix+elem.name+\".\")\n",
    "        else:\n",
    "            result.append(col(prefix+elem.name).alias(prefix+elem.name))\n",
    "    return result\n",
    "\n",
    "l1 = flatten_struct(json1.schema)\n",
    "l2 = flatten_struct(json2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = []\n",
    "col2 = []\n",
    "for i in l1:\n",
    "    col1.append(str(i).replace(\"AS\",\"'\").split(\"'\")[1].strip())\n",
    "for i in l2:\n",
    "    col2.append(str(i).replace(\"AS\",\"'\").split(\"'\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Education.Age', 'Education.Qualification', 'Education.Year', 'Name']\n",
      "['Education.Qualification', 'Education.Year', 'Name']\n",
      "{'Education.Age'}\n",
      "LongType()\n",
      "['Qualification', 'Year']\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "|{BSC, 2012, null}|Benita|\n",
      "|{MSC, 2022, null}| Bavya|\n",
      "+-----------------+------+\n",
      "\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "|{null, BSC, 2012}|Benita|\n",
      "|{null, MSC, 2022}| Bavya|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(col1)\n",
    "print(col2)\n",
    "diff = set(col1)-set(col2)\n",
    "print(diff)\n",
    "\n",
    "for i in diff:\n",
    "    if('.' in i):\n",
    "        c,cn = i.split(\".\")\n",
    "        s_type = json1.schema[c].dataType[cn].dataType\n",
    "        print(s_type)\n",
    "        s_fields = json2.schema[c].dataType.names\n",
    "        print(s_fields)\n",
    "        inDf = json2.withColumn(c,struct(*([col(c)[record].alias(record) for record in s_fields] + [lit(None).cast(s_type).alias(cn)])))\n",
    "        s_fields = sorted(inDf.schema[c].dataType.names)\n",
    "        inDf.show()\n",
    "        inDf = inDf.withColumn(c,struct(*([col(c)[record].alias(record) for record in s_fields])))\n",
    "        inDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = false)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = true)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "| {28, BCOM, 2013}|  Azar|\n",
      "|   {24, BE, 2021}|  Amol|\n",
      "|{null, BSC, 2012}|Benita|\n",
      "|{null, MSC, 2022}| Bavya|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json1.printSchema()\n",
    "outputDf = json1.union(inDf)\n",
    "outputDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speculative Execution in Spark\n",
    "\n",
    "Speculative execution in Apache Spark is a performance optimization technique that aims to handle slow or straggling tasks during a job execution. In distributed systems like Spark, tasks are run in parallel across multiple workers. However, due to various reasons (e.g., resource contention, network latency, node failures), some tasks may take much longer to complete than others. These slow tasks are known as \"stragglers.\"\n",
    "\n",
    "Speculative execution helps mitigate the impact of these straggler tasks by launching duplicate copies of the slow tasks on other nodes. The first copy that finishes is considered the result, and the others are discarded. This helps reduce the overall job completion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf('spark.speculation',True) # Enables or disables speculative execution (default is false).\n",
    "spark.conf('spark.speculation.interval',200) # The frequency at which Spark checks for straggler tasks.\n",
    "spark.conf('spark.speculation.multiplier',5) # threshold = median_task_duration * spark.speculation.multiplier\n",
    "# Where:\n",
    "\n",
    "# median_task_duration is the time it takes for most tasks to finish in the same stage.\n",
    "\n",
    "# spark.speculation.multiplier is the factor that defines how much longer a task can take before it is considered a straggler.\n",
    "\n",
    "# If you have spark.speculation.multiplier = 1.5 and the median time for tasks in a stage is 10 seconds, the threshold for considering a task as a straggler would be:\n",
    "\n",
    "# threshold = 10 * 1.5 = 15 seconds\n",
    "# In this case, if any task takes longer than 15 seconds to complete, Spark will consider it a candidate for speculative execution and will launch a duplicate copy of the task on another executor\n",
    "\n",
    "spark.conf('spark.speculation.quantile',0.75) # The quantile of tasks that are considered for speculative execution (e.g., if you set this to 0.75, Spark will speculate on the slowest 25% of tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to handle corrupt/bad records\n",
    "\n",
    "Modes in spark.read()\n",
    "1. PERMISSIVE\n",
    "2. FAILFAST\n",
    "3. DROPMALFORMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "| emp_no|emp_name|department|\n",
      "+-------+--------+----------+\n",
      "|      1| Murugan|        IT|\n",
      "|invalid| invalid|      null|\n",
      "|      2|  Kannan|   Finance|\n",
      "|      3|   Mohan|      null|\n",
      "|      4|   Pavan|        HR|\n",
      "+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o659.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 123.0 failed 1 times, most recent failure: Lost task 0.0 in stage 123.0 (TID 432) (localhost executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data/badRecords.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o659.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 123.0 failed 1 times, most recent failure: Lost task 0.0 in stage 123.0 (TID 432) (localhost executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('mode','FAILFAST').csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('mode','DROPMALFORMED').csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     3|   Mohan|      null|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('emp_no',IntegerType()),\n",
    "        StructField('emp_name',StringType(),True),\n",
    "        StructField('department',StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.option('mode','DROPMALFORMED').schema(schema=schema).csv('sample_data/badRecords.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|  null| invalid|      null|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     3|   Mohan|      null|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('emp_no',IntegerType()),\n",
    "        StructField('emp_name',StringType(),True),\n",
    "        StructField('department',StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.schema(schema=schema).csv('sample_data/badRecords.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to save bad records from a json to a file\n",
    "\n",
    "Option 1: Use mode(\"PERMISSIVE\") and filter out null records to capture bad records.\n",
    "\n",
    "Option 2: Define a custom schema and use exception handling to capture bad records.\n",
    "\n",
    "Option 3: Use RDD transformations like map and filter to catch bad records and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.emp_no.isNull()).write.save('sample_data/bad_records.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       Name|Age|\n",
      "+-----------+---+\n",
      "|Varshini, S| 25|\n",
      "|   Jothi, S| 52|\n",
      "| Sathish, S| 55|\n",
      "| Neelesh, S| 29|\n",
      "+-----------+---+\n",
      "\n",
      "+-----------+---+-------+\n",
      "|       Name|Age|Initial|\n",
      "+-----------+---+-------+\n",
      "|Varshini, S| 25|      S|\n",
      "|   Jothi, S| 52|      S|\n",
      "| Sathish, S| 55|      S|\n",
      "| Neelesh, S| 29|      S|\n",
      "+-----------+---+-------+\n",
      "\n",
      "+--------+---+-------+\n",
      "|    Name|Age|Initial|\n",
      "+--------+---+-------+\n",
      "|Varshini| 25|      S|\n",
      "|   Jothi| 52|      S|\n",
      "| Sathish| 55|      S|\n",
      "| Neelesh| 29|      S|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = spark.read.option('delimiter','~|').csv('sample_data/multidelimiter.csv',header=True)\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('Initial',split('Name',',')[1])\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('Name',split('Name',',')[0])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map vs FlatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Skewness\n",
    "\n",
    "In spark-3, AQE handles it by itself\n",
    "\n",
    "In spark-2:\n",
    "\n",
    "1. we can try with repartition - Splits the data equally\n",
    "\n",
    "Repartitioning by a more evenly distributed column\n",
    "df = df.repartition(\"region_id\")\n",
    "\n",
    "Or just increasing partitions to spread out skew\n",
    "df = df.repartition(100)\n",
    "\n",
    "Challenges:\n",
    "\n",
    "df.repartition(\"user_id\")\n",
    "If user_id is skewed (like 90% of rows are for user_1), all that data still ends up on one partition  just a different one. So:\n",
    "\n",
    "You've shuffled the data \n",
    "\n",
    "But the skew is still there \n",
    "\n",
    " Repartitioning just moves the problem to a different executor.\n",
    "\n",
    "Skew Happens During a Join\n",
    "This is a big one.\n",
    "\n",
    "Even if both DataFrames are repartitioned before the join:\n",
    "\n",
    "df1 = df1.repartition(\"user_id\")\n",
    "df2 = df2.repartition(\"user_id\")\n",
    "df1.join(df2, \"user_id\")\n",
    "If user_id = user_1 exists millions of times in both tables, all matching rows go to one task during the shuffle phase. So again:\n",
    "\n",
    "Partitioning was done \n",
    "\n",
    "But data volume per key caused skew \n",
    "\n",
    " What helps instead: salting, or broadcast joins if one side is small.\n",
    "\n",
    "Not Enough Partitions to Spread the Load\n",
    "If you do:\n",
    "\n",
    "df.repartition(4)\n",
    "...but your data has millions of rows, and one key dominates, you're not helping much.\n",
    "\n",
    "Even worse  repartitioning with too few partitions can make the skew worse, because youre squeezing more data into fewer slots.\n",
    "\n",
    " Use more partitions, and sometimes pair with salting.\n",
    "\n",
    "4. You Repartition by a Column That's Also Skewed\n",
    "If you repartition by a column thats also skewed, it just moves the skew around:\n",
    "\n",
    "\n",
    "df.repartition(\"country\")  # But 90% of users are from \"US\"\n",
    "Now all the \"US\" rows go to one partition. Not helpful.\n",
    "\n",
    " Better to repartition on a less skewed column or combine columns to reduce concentration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Salting\n",
    "\n",
    "purchases DataFrame:\n",
    "\n",
    "user_id\tpurchase_amount\n",
    "user_1\t100\n",
    "user_1\t50\n",
    "user_1\t200\n",
    "user_2\t30\n",
    "user_3\t70\n",
    "Notice how user_1 is skewed  lots of rows for just one key.\n",
    "\n",
    " Step 1: Add a salt\n",
    "We want to break user_1 into multiple salted keys so Spark can parallelize the work.\n",
    "\n",
    "We'll assign a random salt value (between 0 and 1, for this demo  can be more in real world).\n",
    "\n",
    "Let's randomly assign:\n",
    "\n",
    "user_id\tpurchase_amount\tsalt\tuser_id_salted\n",
    "user_1\t100\t0\tuser_1_0\n",
    "user_1\t50\t1\tuser_1_1\n",
    "user_1\t200\t0\tuser_1_0\n",
    "user_2\t30\t0\tuser_2_0\n",
    "user_3\t70\t1\tuser_3_1\n",
    "This spreads the user_1 records across two partitions (user_1_0 and user_1_1).\n",
    "\n",
    " Step 2: GroupBy on the salted key\n",
    "Now, do the aggregation:\n",
    "\n",
    "grouped = salted_df.groupBy(\"user_id_salted\").agg(sum(\"purchase_amount\").alias(\"partial_sum\"))\n",
    "Result:\n",
    "\n",
    "user_id_salted\tpartial_sum\n",
    "user_1_0\t100 + 200 = 300\n",
    "user_1_1\t50\n",
    "user_2_0\t30\n",
    "user_3_1\t70\n",
    "Nice! Now the work was split between user_1_0 and user_1_1, avoiding a bottleneck.\n",
    "\n",
    " Step 3: Extract original key and re-group\n",
    "Now we strip off the salt (e.g., split on _), and group by the original user_id:\n",
    "\n",
    "final_df = grouped \\\n",
    "    .withColumn(\"user_id\", split(col(\"user_id_salted\"), \"_\")[0]) \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(sum(\"partial_sum\").alias(\"total_sum\"))\n",
    "Result:\n",
    "\n",
    "user_id\ttotal_sum\n",
    "user_1\t300 + 50 = 350\n",
    "user_2\t30\n",
    "user_3\t70\n",
    " That's your final answer, and youve handled the skewed user_1 cleanly and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning\n",
    "\n",
    "spark.read.parquet()\tBased on file count & block size\n",
    "groupBy, join, distinct, etc.\tspark.sql.shuffle.partitions (default: 200)\n",
    "df.rdd.getNumPartitions()\tReturns current partition count\n",
    "df.write.parquet(\"s3://your-bucket/output/\") Will write 200 or df.rdd.getNumPartitions() files (usually .snappy.parquet files) to that path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small files problem\n",
    "\n",
    "In Spark, every partition = one output file (usually .parquet, .json, .csv, etc.).\n",
    "\n",
    "So if your DataFrame has 10,000 tiny partitions (maybe because each partition only has a few rows), then:\n",
    "\n",
    "df.write.parquet(\"s3://bucket/output/\")\n",
    " Creates 10,000 small files.\n",
    "\n",
    "1. Too Many Files Overwhelm Metadata Systems\n",
    "Systems like HDFS, S3, Hive Metastore hate dealing with thousands of tiny files.\n",
    "\n",
    "Every file = a metadata entry.\n",
    "\n",
    "This bloats memory usage in NameNode (HDFS) or slows down listing/querying (S3, Glue, Hive).\n",
    "\n",
    "2. Bad Performance When Reading\n",
    "When you query later (e.g., using Spark, Athena, Presto), it opens lots of small files.\n",
    "\n",
    "This means lots of I/O operations, network calls, and latency.\n",
    "\n",
    "Youll see poor parallelism: thousands of files  tasks that finish in milliseconds but cause tons of overhead.\n",
    "\n",
    "3. Wasted Storage and Throughput\n",
    "Each file has some overhead (compression headers, footers, etc.).\n",
    "\n",
    "Small files don't utilize block storage well (esp. in HDFS), wasting space and reducing write efficiency.\n",
    "\n",
    "Best Practices\n",
    "\n",
    "File Format\tTarget File Size\n",
    "Parquet\t128 MB - 1 GB\n",
    "ORC\t256 MB - 1 GB\n",
    "CSV/JSON\tSmaller, but still 10+ MB recommended\n",
    "\n",
    " Large files = fewer tasks, better compression, better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the json content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------------------------+\n",
      "|date      |status |request                               |\n",
      "+----------+-------+--------------------------------------+\n",
      "|2025-04-10|success|\"{\"response\": {\"message_id\": \"msg_001\"|\n",
      "|2025-04-10|error  |\"{\"response\": {\"message_id\": \"msg_002\"|\n",
      "|2025-04-09|success|\"{\"response\": {\"message_id\": \"msg_003\"|\n",
      "|2025-04-09|error  |\"{\"response\": {\"message_id\": \"msg_004\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_005\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_006\"|\n",
      "|2025-04-07|error  |\"{\"response\": {\"message_id\": \"msg_007\"|\n",
      "|2025-04-07|success|\"{\"response\": {\"message_id\": \"msg_008\"|\n",
      "+----------+-------+--------------------------------------+\n",
      "\n",
      "+----------+-------+--------------------------------------+\n",
      "|date      |status |request                               |\n",
      "+----------+-------+--------------------------------------+\n",
      "|2025-04-10|success|\"{\"response\": {\"message_id\": \"msg_001\"|\n",
      "|2025-04-10|error  |\"{\"response\": {\"message_id\": \"msg_002\"|\n",
      "|2025-04-09|success|\"{\"response\": {\"message_id\": \"msg_003\"|\n",
      "|2025-04-09|error  |\"{\"response\": {\"message_id\": \"msg_004\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_005\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_006\"|\n",
      "|2025-04-07|error  |\"{\"response\": {\"message_id\": \"msg_007\"|\n",
      "|2025-04-07|success|\"{\"response\": {\"message_id\": \"msg_008\"|\n",
      "+----------+-------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/json.csv',header=True)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Value is truncated\n",
    "\n",
    "# These two options  quote and escape  are  when dealing with complex strings (like embedded JSON) in CSV files.\n",
    "\n",
    "# option(\"quote\", '\"')\n",
    "# This tells Spark:\n",
    "\n",
    "# \"Text between this character is considered as a single field  even if it has commas.\"\n",
    "\n",
    "# option(\"escape\", '\"')\n",
    "# This tells Spark:\n",
    "\n",
    "# \"If this character appears inside a quoted field, treat it as part of the string  not the end of the quote.\"\n",
    "\n",
    "# quote\tDefines boundaries of a field  usually \".\n",
    "# escape\tTells Spark how to handle quotes inside a quoted field.\n",
    "\n",
    "df = spark.read.option('quote','\"').csv('sample_data/json.csv',header=True)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Even this is not working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON spec requires double quotes for strings and keys\n",
    "Valid JSON:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{\"response\": {\"message_id\": \"msg_001\"}}\n",
    "Invalid JSON:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{'response': {'message_id': 'msg_001'}}\n",
    "Tools like from_json() in Spark (and any proper JSON parser) will reject single quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------------+\n",
      "|date      |status |request                                                                             |\n",
      "+----------+-------+------------------------------------------------------------------------------------+\n",
      "|2025-04-10|success|{\"response\": {\"message_id\": \"msg_001\", \"latitude\": 37.7749, \"longitude\": -122.4194}}|\n",
      "|2025-04-10|error  |{\"response\": {\"message_id\": \"msg_002\", \"latitude\": 40.7128, \"longitude\": -74.0060}} |\n",
      "|2025-04-09|success|{\"response\": {\"message_id\": \"msg_003\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|\n",
      "|2025-04-09|error  |{\"response\": {\"message_id\": \"msg_004\", \"latitude\": 51.5074, \"longitude\": -0.1278}}  |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_005\", \"latitude\": 48.8566, \"longitude\": 2.3522}}   |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_006\", \"latitude\": 52.5200, \"longitude\": 13.4050}}  |\n",
      "|2025-04-07|error  |{\"response\": {\"message_id\": \"msg_007\", \"latitude\": 35.6895, \"longitude\": 139.6917}} |\n",
      "|2025-04-07|success|{\"response\": {\"message_id\": \"msg_008\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|\n",
      "+----------+-------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option('quote','\\'').csv('sample_data/json.csv',header=True)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method-1: Using json_tuple\n",
    "\n",
    "We should know the fields to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+\n",
      "|      date| status|             request|                  c0|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|2025-04-10|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-10|  error|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-09|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-09|  error|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-07|  error|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-07|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "\n",
      "+----------+-------+--------------------+\n",
      "|      date| status|                  c0|\n",
      "+----------+-------+--------------------+\n",
      "|2025-04-10|success|{\"message_id\":\"ms...|\n",
      "|2025-04-10|  error|{\"message_id\":\"ms...|\n",
      "|2025-04-09|success|{\"message_id\":\"ms...|\n",
      "|2025-04-09|  error|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|\n",
      "|2025-04-07|  error|{\"message_id\":\"ms...|\n",
      "|2025-04-07|success|{\"message_id\":\"ms...|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "df.select('*',json_tuple('request','response')).show()\n",
    "\n",
    "df1 = df.select('*',json_tuple('request','response')).drop('request')\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+----------+--------+---------+\n",
      "|      date| status|                  c0|message_id|latitude|longitude|\n",
      "+----------+-------+--------------------+----------+--------+---------+\n",
      "|2025-04-10|success|{\"message_id\":\"ms...|   msg_001| 37.7749|-122.4194|\n",
      "|2025-04-10|  error|{\"message_id\":\"ms...|   msg_002| 40.7128|  -74.006|\n",
      "|2025-04-09|success|{\"message_id\":\"ms...|   msg_003| 34.0522|-118.2437|\n",
      "|2025-04-09|  error|{\"message_id\":\"ms...|   msg_004| 51.5074|  -0.1278|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|   msg_005| 48.8566|   2.3522|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|   msg_006|   52.52|   13.405|\n",
      "|2025-04-07|  error|{\"message_id\":\"ms...|   msg_007| 35.6895| 139.6917|\n",
      "|2025-04-07|success|{\"message_id\":\"ms...|   msg_008| 34.0522|-118.2437|\n",
      "+----------+-------+--------------------+----------+--------+---------+\n",
      "\n",
      "+----------+-------+----------+--------+---------+\n",
      "|      date| status|message_id|latitude|longitude|\n",
      "+----------+-------+----------+--------+---------+\n",
      "|2025-04-10|success|   msg_001| 37.7749|-122.4194|\n",
      "|2025-04-10|  error|   msg_002| 40.7128|  -74.006|\n",
      "|2025-04-09|success|   msg_003| 34.0522|-118.2437|\n",
      "|2025-04-09|  error|   msg_004| 51.5074|  -0.1278|\n",
      "|2025-04-08|success|   msg_005| 48.8566|   2.3522|\n",
      "|2025-04-08|success|   msg_006|   52.52|   13.405|\n",
      "|2025-04-07|  error|   msg_007| 35.6895| 139.6917|\n",
      "|2025-04-07|success|   msg_008| 34.0522|-118.2437|\n",
      "+----------+-------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('*',json_tuple('c0','message_id','latitude','longitude').alias('message_id','latitude','longitude')).show()\n",
    "\n",
    "df2 = df1.select('*',json_tuple('c0','message_id','latitude','longitude').alias('message_id','latitude','longitude')).drop('c0')\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method-2: Using from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|date      |status |request                                                                             |request_parsed                 |\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|2025-04-10|success|{\"response\": {\"message_id\": \"msg_001\", \"latitude\": 37.7749, \"longitude\": -122.4194}}|{{msg_001, 37.7749, -122.4194}}|\n",
      "|2025-04-10|error  |{\"response\": {\"message_id\": \"msg_002\", \"latitude\": 40.7128, \"longitude\": -74.0060}} |{{msg_002, 40.7128, -74.006}}  |\n",
      "|2025-04-09|success|{\"response\": {\"message_id\": \"msg_003\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{msg_003, 34.0522, -118.2437}}|\n",
      "|2025-04-09|error  |{\"response\": {\"message_id\": \"msg_004\", \"latitude\": 51.5074, \"longitude\": -0.1278}}  |{{msg_004, 51.5074, -0.1278}}  |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_005\", \"latitude\": 48.8566, \"longitude\": 2.3522}}   |{{msg_005, 48.8566, 2.3522}}   |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_006\", \"latitude\": 52.5200, \"longitude\": 13.4050}}  |{{msg_006, 52.52, 13.405}}     |\n",
      "|2025-04-07|error  |{\"response\": {\"message_id\": \"msg_007\", \"latitude\": 35.6895, \"longitude\": 139.6917}} |{{msg_007, 35.6895, 139.6917}} |\n",
      "|2025-04-07|success|{\"response\": {\"message_id\": \"msg_008\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{msg_008, 34.0522, -118.2437}}|\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "\n",
      "+----------+-------+----------+--------+---------+\n",
      "|date      |status |message_id|latitude|longitude|\n",
      "+----------+-------+----------+--------+---------+\n",
      "|2025-04-10|success|msg_001   |37.7749 |-122.4194|\n",
      "|2025-04-10|error  |msg_002   |40.7128 |-74.006  |\n",
      "|2025-04-09|success|msg_003   |34.0522 |-118.2437|\n",
      "|2025-04-09|error  |msg_004   |51.5074 |-0.1278  |\n",
      "|2025-04-08|success|msg_005   |48.8566 |2.3522   |\n",
      "|2025-04-08|success|msg_006   |52.52   |13.405   |\n",
      "|2025-04-07|error  |msg_007   |35.6895 |139.6917 |\n",
      "|2025-04-07|success|msg_008   |34.0522 |-118.2437|\n",
      "+----------+-------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json,col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"response\", StructType([\n",
    "        StructField(\"message_id\", StringType()),\n",
    "        StructField(\"latitude\", DoubleType()),\n",
    "        StructField(\"longitude\", DoubleType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "df_parsed = df.withColumn(\"request_parsed\", from_json(col(\"request\"), json_schema))\n",
    "df_parsed.show(truncate=False)\n",
    "\n",
    "df_flat = df_parsed.select(\n",
    "    \"date\",\n",
    "    \"status\",\n",
    "    col(\"request_parsed.response.message_id\").alias(\"message_id\"),\n",
    "    col(\"request_parsed.response.latitude\").alias(\"latitude\"),\n",
    "    col(\"request_parsed.response.longitude\").alias(\"longitude\")\n",
    ")\n",
    "\n",
    "df_flat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above case we are manually giving the schema, but we can also get it from the data and pass it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('response', StructType([StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True), StructField('message_id', StringType(), True)]), True)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col('request').alias('jsonCol')).rdd.map(lambda x:x.jsonCol).collect()\n",
    "\n",
    "df_schema = spark.read.json(df.select(col('request').alias('jsonCol')).rdd.map(lambda x: x.jsonCol)).schema\n",
    "\n",
    "df_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|date      |status |request                                                                             |request_parsed                 |\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|2025-04-10|success|{\"response\": {\"message_id\": \"msg_001\", \"latitude\": 37.7749, \"longitude\": -122.4194}}|{{37.7749, -122.4194, msg_001}}|\n",
      "|2025-04-10|error  |{\"response\": {\"message_id\": \"msg_002\", \"latitude\": 40.7128, \"longitude\": -74.0060}} |{{40.7128, -74.006, msg_002}}  |\n",
      "|2025-04-09|success|{\"response\": {\"message_id\": \"msg_003\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{34.0522, -118.2437, msg_003}}|\n",
      "|2025-04-09|error  |{\"response\": {\"message_id\": \"msg_004\", \"latitude\": 51.5074, \"longitude\": -0.1278}}  |{{51.5074, -0.1278, msg_004}}  |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_005\", \"latitude\": 48.8566, \"longitude\": 2.3522}}   |{{48.8566, 2.3522, msg_005}}   |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_006\", \"latitude\": 52.5200, \"longitude\": 13.4050}}  |{{52.52, 13.405, msg_006}}     |\n",
      "|2025-04-07|error  |{\"response\": {\"message_id\": \"msg_007\", \"latitude\": 35.6895, \"longitude\": 139.6917}} |{{35.6895, 139.6917, msg_007}} |\n",
      "|2025-04-07|success|{\"response\": {\"message_id\": \"msg_008\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{34.0522, -118.2437, msg_008}}|\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "\n",
      "+----------+-------+----------+--------+---------+\n",
      "|date      |status |message_id|latitude|longitude|\n",
      "+----------+-------+----------+--------+---------+\n",
      "|2025-04-10|success|msg_001   |37.7749 |-122.4194|\n",
      "|2025-04-10|error  |msg_002   |40.7128 |-74.006  |\n",
      "|2025-04-09|success|msg_003   |34.0522 |-118.2437|\n",
      "|2025-04-09|error  |msg_004   |51.5074 |-0.1278  |\n",
      "|2025-04-08|success|msg_005   |48.8566 |2.3522   |\n",
      "|2025-04-08|success|msg_006   |52.52   |13.405   |\n",
      "|2025-04-07|error  |msg_007   |35.6895 |139.6917 |\n",
      "|2025-04-07|success|msg_008   |34.0522 |-118.2437|\n",
      "+----------+-------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parsed = df.withColumn(\"request_parsed\", from_json(col(\"request\"), df_schema))\n",
    "df_parsed.show(truncate=False)\n",
    "\n",
    "df_flat = df_parsed.select(\n",
    "    \"date\",\n",
    "    \"status\",\n",
    "    col(\"request_parsed.response.message_id\").alias(\"message_id\"),\n",
    "    col(\"request_parsed.response.latitude\").alias(\"latitude\"),\n",
    "    col(\"request_parsed.response.longitude\").alias(\"longitude\")\n",
    ")\n",
    "\n",
    "df_flat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      " |-- request_parsed: struct (nullable = true)\n",
      " |    |-- response: struct (nullable = true)\n",
      " |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |-- message_id: string (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+--------------------+--------+---------+----------+\n",
      "|      date| status|             request|      request_parsed|latitude|longitude|message_id|\n",
      "+----------+-------+--------------------+--------------------+--------+---------+----------+\n",
      "|2025-04-10|success|{\"response\": {\"me...|{{37.7749, -122.4...| 37.7749|-122.4194|   msg_001|\n",
      "|2025-04-10|  error|{\"response\": {\"me...|{{40.7128, -74.00...| 40.7128|  -74.006|   msg_002|\n",
      "|2025-04-09|success|{\"response\": {\"me...|{{34.0522, -118.2...| 34.0522|-118.2437|   msg_003|\n",
      "|2025-04-09|  error|{\"response\": {\"me...|{{51.5074, -0.127...| 51.5074|  -0.1278|   msg_004|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{{48.8566, 2.3522...| 48.8566|   2.3522|   msg_005|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{{52.52, 13.405, ...|   52.52|   13.405|   msg_006|\n",
      "|2025-04-07|  error|{\"response\": {\"me...|{{35.6895, 139.69...| 35.6895| 139.6917|   msg_007|\n",
      "|2025-04-07|success|{\"response\": {\"me...|{{34.0522, -118.2...| 34.0522|-118.2437|   msg_008|\n",
      "+----------+-------+--------------------+--------------------+--------+---------+----------+\n",
      "\n",
      "+----------+-------+--------+---------+----------+\n",
      "|      date| status|latitude|longitude|message_id|\n",
      "+----------+-------+--------+---------+----------+\n",
      "|2025-04-10|success| 37.7749|-122.4194|   msg_001|\n",
      "|2025-04-10|  error| 40.7128|  -74.006|   msg_002|\n",
      "|2025-04-09|success| 34.0522|-118.2437|   msg_003|\n",
      "|2025-04-09|  error| 51.5074|  -0.1278|   msg_004|\n",
      "|2025-04-08|success| 48.8566|   2.3522|   msg_005|\n",
      "|2025-04-08|success|   52.52|   13.405|   msg_006|\n",
      "|2025-04-07|  error| 35.6895| 139.6917|   msg_007|\n",
      "|2025-04-07|success| 34.0522|-118.2437|   msg_008|\n",
      "+----------+-------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parsed.printSchema()\n",
    "json_col = df_parsed.schema['request_parsed'].dataType.names[0]\n",
    "check = \"request_parsed.\"+json_col+\".*\"\n",
    "check\n",
    "df_parsed.select('*',col(check)).show()\n",
    "df_parsed.select('*',col(check)).drop('request').drop('request_parsed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common records in file1 and file2\n",
    "\n",
    "We can do this in many ways:\n",
    "- set operations: union, unionAll, distinct, intersect, intersectAll, subtract, exceptAll\n",
    "- join operations: leftouter, rightouter, leftsemi, leftanti, cartesian\n",
    "\n",
    "set operations can be only used if both the tables has same no of columns\n",
    "\n",
    "Both ultimately result in same time and operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|rollno|          name|    mobile|\n",
      "+------+--------------+----------+\n",
      "|   101|      John Doe|9876543210|\n",
      "|   102|    Jane Smith|9123456789|\n",
      "|   103|  Robert Brown|9988776655|\n",
      "|   104|   Alice Green|9012345678|\n",
      "|   105|  Mike Johnson|9870011223|\n",
      "|   106|  Linda Taylor|9345678901|\n",
      "|   107|  David Wilson|9001122334|\n",
      "|   108|    Emma Davis|9212345678|\n",
      "|   109|  Chris Martin|9812345678|\n",
      "|   110|    Sophia Lee|8899001122|\n",
      "|   111|  Daniel Lewis|9700112233|\n",
      "|   112|     Mia Clark|9456789012|\n",
      "|   113|   Noah Walker|9332211455|\n",
      "|   114|   Olivia Hall|9543217890|\n",
      "|   115|    Liam Young|9877003456|\n",
      "|   116|      Ava King|9445566778|\n",
      "|   117|  Ethan Wright|9789012345|\n",
      "|   118|Isabella Scott|9311234567|\n",
      "|   119|   Logan Adams|9654321098|\n",
      "|   120|   Emily Baker|9876001122|\n",
      "+------+--------------+----------+\n",
      "\n",
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   105|Mike Johnson|9870011223|\n",
      "|   106|Linda Taylor|9345678901|\n",
      "|   107|David Wilson|9001122334|\n",
      "|   108|  Emma Davis|9212345678|\n",
      "|   109|Chris Martin|9812345678|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA = spark.read.option('header',True).option('inferSchema',True).csv('sample_data/studentA.csv')\n",
    "studentB = spark.read.option('header',True).option('inferSchema',True).csv('sample_data/studentB.csv')\n",
    "studentA.show()\n",
    "studentB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   105|Mike Johnson|9870011223|\n",
      "|   106|Linda Taylor|9345678901|\n",
      "|   107|David Wilson|9001122334|\n",
      "|   108|  Emma Davis|9212345678|\n",
      "|   109|Chris Martin|9812345678|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.join(studentB,studentA.rollno == studentB.rollno,'inner').select(studentA.rollno,studentA.name,studentA.mobile).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using set operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   109|Chris Martin|9812345678|\n",
      "|   101|    John Doe|9876543210|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   106|Linda Taylor|9345678901|\n",
      "|   107|David Wilson|9001122334|\n",
      "|   105|Mike Johnson|9870011223|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   108|  Emma Davis|9212345678|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.intersectAll(studentB).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records present in one file but not in another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|rollno|          name|    mobile|\n",
      "+------+--------------+----------+\n",
      "|   110|    Sophia Lee|8899001122|\n",
      "|   111|  Daniel Lewis|9700112233|\n",
      "|   112|     Mia Clark|9456789012|\n",
      "|   113|   Noah Walker|9332211455|\n",
      "|   114|   Olivia Hall|9543217890|\n",
      "|   115|    Liam Young|9877003456|\n",
      "|   116|      Ava King|9445566778|\n",
      "|   117|  Ethan Wright|9789012345|\n",
      "|   118|Isabella Scott|9311234567|\n",
      "|   119|   Logan Adams|9654321098|\n",
      "|   120|   Emily Baker|9876001122|\n",
      "+------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.join(studentB,studentA.rollno == studentB.rollno,'leftanti').select(studentA.rollno,studentA.name,studentA.mobile).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|rollno|          name|    mobile|\n",
      "+------+--------------+----------+\n",
      "|   116|      Ava King|9445566778|\n",
      "|   112|     Mia Clark|9456789012|\n",
      "|   114|   Olivia Hall|9543217890|\n",
      "|   118|Isabella Scott|9311234567|\n",
      "|   111|  Daniel Lewis|9700112233|\n",
      "|   117|  Ethan Wright|9789012345|\n",
      "|   110|    Sophia Lee|8899001122|\n",
      "|   120|   Emily Baker|9876001122|\n",
      "|   113|   Noah Walker|9332211455|\n",
      "|   115|    Liam Young|9877003456|\n",
      "|   119|   Logan Adams|9654321098|\n",
      "+------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.exceptAll(studentB).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---------+-------+-------+-----+\n",
      "|rollno|math|physics|chemistry|biology|english|total|\n",
      "+------+----+-------+---------+-------+-------+-----+\n",
      "|   101|  78|     82|       76|     85|     90|  411|\n",
      "|   102|  88|     79|       84|     81|     86|  418|\n",
      "|   103|  65|     70|       72|     68|     74|  349|\n",
      "|   104|  92|     88|       91|     90|     89|  450|\n",
      "|   105|  55|     60|       58|     62|     59|  294|\n",
      "|   106|  73|     75|       78|     74|     76|  376|\n",
      "|   107|  80|     83|       79|     81|     82|  405|\n",
      "|   108|  90|     88|       85|     89|     87|  439|\n",
      "|   109|  66|     68|       64|     70|     72|  340|\n",
      "|   110|  85|     87|       84|     86|     88|  430|\n",
      "|   111|  77|     74|       79|     76|     78|  384|\n",
      "|   112|  69|     71|       68|     70|     72|  350|\n",
      "|   113|  94|     95|       93|     96|     92|  470|\n",
      "|   114|  58|     62|       60|     59|     61|  300|\n",
      "|   115|  87|     89|       85|     88|     90|  439|\n",
      "|   116|  72|     74|       70|     73|     75|  364|\n",
      "|   117|  60|     65|       62|     64|     66|  317|\n",
      "|   118|  83|     81|       80|     82|     85|  411|\n",
      "|   119|  70|     68|       72|     71|     69|  350|\n",
      "|   120|  91|     90|       89|     88|     87|  445|\n",
      "+------+----+-------+---------+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks = spark.read.csv('sample_data/marks.csv',header=True,inferSchema=True)\n",
    "marks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:1 - Using create_map and explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'math'>, Column<'math'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>, Column<'biology'>, Column<'biology'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>, Column<'biology'>, Column<'biology'>, Column<'english'>, Column<'english'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>, Column<'biology'>, Column<'biology'>, Column<'english'>, Column<'english'>]\n",
      "+------+---------+-----+\n",
      "|rollno|  subject|marks|\n",
      "+------+---------+-----+\n",
      "|   101|     math|   78|\n",
      "|   101|  physics|   82|\n",
      "|   101|chemistry|   76|\n",
      "|   101|  biology|   85|\n",
      "|   101|  english|   90|\n",
      "|   102|     math|   88|\n",
      "|   102|  physics|   79|\n",
      "|   102|chemistry|   84|\n",
      "|   102|  biology|   81|\n",
      "|   102|  english|   86|\n",
      "|   103|     math|   65|\n",
      "|   103|  physics|   70|\n",
      "|   103|chemistry|   72|\n",
      "|   103|  biology|   68|\n",
      "|   103|  english|   74|\n",
      "|   104|     math|   92|\n",
      "|   104|  physics|   88|\n",
      "|   104|chemistry|   91|\n",
      "|   104|  biology|   90|\n",
      "|   104|  english|   89|\n",
      "+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map,lit,col,explode\n",
    "\n",
    "subject_cols = [\"math\", \"physics\", \"chemistry\", \"biology\", \"english\"]\n",
    "\n",
    "# Build map of subject -> marks\n",
    "pairs = []\n",
    "for sub in subject_cols:\n",
    "    pairs.extend([lit(sub), col(sub)])\n",
    "    print(pairs)\n",
    "\n",
    "print(pairs)\n",
    "\n",
    "df_map = marks.select(\"rollno\", explode(create_map(*pairs)).alias(\"subject\", \"marks\"))\n",
    "df_map.show()\n",
    "\n",
    "# 1.  create_map()  Build a Map from Column Pairs\n",
    "\n",
    "# from pyspark.sql.functions import create_map, lit, col\n",
    "# Lets say you have:\n",
    "\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     (101, 78, 82, 90)\n",
    "# ], [\"rollno\", \"math\", \"physics\", \"english\"])\n",
    "# To build a map like this:\n",
    "\n",
    "\n",
    "# {\n",
    "#   \"math\": 78,\n",
    "#   \"physics\": 82,\n",
    "#   \"english\": 90\n",
    "# }\n",
    "# You use create_map():\n",
    "\n",
    "\n",
    "# create_map(\n",
    "#     lit(\"math\"), col(\"math\"),\n",
    "#     lit(\"physics\"), col(\"physics\"),\n",
    "#     lit(\"english\"), col(\"english\")\n",
    "# )\n",
    "# The lit() gives you the key, the col() gives you the value.\n",
    "\n",
    "# 2.  explode()  Turn Map Entries into Rows\n",
    "# Now that we have a column thats a map, explode() will take each key-value pair in that map and create a new row.\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# df.select(\n",
    "#     \"rollno\",\n",
    "#     explode(\n",
    "#         create_map(\n",
    "#             lit(\"math\"), col(\"math\"),\n",
    "#             lit(\"physics\"), col(\"physics\"),\n",
    "#             lit(\"english\"), col(\"english\")\n",
    "#         )\n",
    "#     ).alias(\"subject\", \"marks\")\n",
    "# )\n",
    "#  Output:\n",
    "# text\n",
    "# Copy\n",
    "# Edit\n",
    "# +-------+--------+-----+\n",
    "# |rollno |subject |marks|\n",
    "# +-------+--------+-----+\n",
    "# |101    |math    |78   |\n",
    "# |101    |physics |82   |\n",
    "# |101    |english |90   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'math', math, 'physics', physics, 'chemistry', chemistry, 'biology', biology, 'english', english\n",
      "+------+---------+-----+\n",
      "|rollno|  subject|marks|\n",
      "+------+---------+-----+\n",
      "|   101|     math|   78|\n",
      "|   101|  physics|   82|\n",
      "|   101|chemistry|   76|\n",
      "|   101|  biology|   85|\n",
      "|   101|  english|   90|\n",
      "|   102|     math|   88|\n",
      "|   102|  physics|   79|\n",
      "|   102|chemistry|   84|\n",
      "|   102|  biology|   81|\n",
      "|   102|  english|   86|\n",
      "|   103|     math|   65|\n",
      "|   103|  physics|   70|\n",
      "|   103|chemistry|   72|\n",
      "|   103|  biology|   68|\n",
      "|   103|  english|   74|\n",
      "|   104|     math|   92|\n",
      "|   104|  physics|   88|\n",
      "|   104|chemistry|   91|\n",
      "|   104|  biology|   90|\n",
      "|   104|  english|   89|\n",
      "+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subject_list = [\"math\", \"physics\", \"chemistry\", \"biology\", \"english\"]\n",
    "num_subjects = len(subject_list)\n",
    "\n",
    "stack_expr = \", \".join([f\"'{sub}', {sub}\" for sub in subject_list])\n",
    "print(stack_expr)\n",
    "\n",
    "df_unpivoted = marks.selectExpr(\"rollno\", f\"stack({num_subjects}, {stack_expr}) as (subject, marks)\")\n",
    "df_unpivoted.show()\n",
    "\n",
    "# The stack(n, col1, col2, ..., coln) function is used to unpivot columns  it turns columns into rows.\n",
    "\n",
    "# Think of it as:\n",
    "\n",
    "# \"For each row in the original DataFrame, stack will output n new rows, using pairs of values I give it.\"\n",
    "\n",
    "#  Let's say you have this data:\n",
    "# text\n",
    "# Copy\n",
    "# Edit\n",
    "# +-------+-----+--------+----------+\n",
    "# |rollno |math |physics |english   |\n",
    "# +-------+-----+--------+----------+\n",
    "# |101    | 78  | 82     | 90       |\n",
    "# You want to convert it to:\n",
    "\n",
    "# text\n",
    "# Copy\n",
    "# Edit\n",
    "# +-------+--------+-------+\n",
    "# |rollno |subject |marks  |\n",
    "# +-------+--------+-------+\n",
    "# |101    |math    |78     |\n",
    "# |101    |physics |82     |\n",
    "# |101    |english |90     |\n",
    "#  Step-by-step breakdown of stack\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# df.selectExpr(\"rollno\", \"stack(3, 'math', math, 'physics', physics, 'english', english) as (subject, marks)\")\n",
    "#  Here's what it's doing:\n",
    "# stack(3, ...):\n",
    "#  Create 3 rows per original row (one per subject)\n",
    "\n",
    "# The arguments come in pairs:  First is a literal (the subject name)\n",
    "#  Second is the column (marks value)\n",
    "\n",
    "# So this:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# stack(3,\n",
    "#   'math', math,\n",
    "#   'physics', physics,\n",
    "#   'english', english\n",
    "# )\n",
    "# Is telling Spark:\n",
    "\n",
    "# \"For each row, create 3 new rows like:\n",
    "\n",
    "# ('math', value of math)\n",
    "\n",
    "# ('physics', value of physics)\n",
    "\n",
    "# ('english', value of english)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again pivot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+-------+----+-------+-----+\n",
      "|rollno|biology|chemistry|english|math|physics|total|\n",
      "+------+-------+---------+-------+----+-------+-----+\n",
      "|   108|     89|       85|     87|  90|     88|  439|\n",
      "|   115|     88|       85|     90|  87|     89|  439|\n",
      "|   101|     85|       76|     90|  78|     82|  411|\n",
      "|   103|     68|       72|     74|  65|     70|  349|\n",
      "|   111|     76|       79|     78|  77|     74|  384|\n",
      "|   120|     88|       89|     87|  91|     90|  445|\n",
      "|   117|     64|       62|     66|  60|     65|  317|\n",
      "|   112|     70|       68|     72|  69|     71|  350|\n",
      "|   107|     81|       79|     82|  80|     83|  405|\n",
      "|   114|     59|       60|     61|  58|     62|  300|\n",
      "|   102|     81|       84|     86|  88|     79|  418|\n",
      "|   113|     96|       93|     92|  94|     95|  470|\n",
      "|   109|     70|       64|     72|  66|     68|  340|\n",
      "|   105|     62|       58|     59|  55|     60|  294|\n",
      "|   110|     86|       84|     88|  85|     87|  430|\n",
      "|   106|     74|       78|     76|  73|     75|  376|\n",
      "|   116|     73|       70|     75|  72|     74|  364|\n",
      "|   119|     71|       72|     69|  70|     68|  350|\n",
      "|   118|     82|       80|     85|  83|     81|  411|\n",
      "|   104|     90|       91|     89|  92|     88|  450|\n",
      "+------+-------+---------+-------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "pivotBack = df_unpivoted.groupBy('rollno').pivot('subject').max('marks')\n",
    "\n",
    "df_with_total = pivotBack.withColumn(\n",
    "    \"total\",\n",
    "    coalesce(col(\"math\"), lit(0)) +\n",
    "    coalesce(col(\"physics\"), lit(0)) +\n",
    "    coalesce(col(\"chemistry\"), lit(0)) +\n",
    "    coalesce(col(\"english\"), lit(0)) +\n",
    "    coalesce(col(\"biology\"), lit(0))\n",
    ")\n",
    "\n",
    "df_with_total.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "+------+------------+----------+\n",
      "\n",
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/duplicates.csv',header=True,inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.groupBy('rollno','name','mobile').count().where(\"count > 1\").drop('count').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+----+\n",
      "|rollno|        name|    mobile|rank|\n",
      "+------+------------+----------+----+\n",
      "|   104| Alice Green|9012345678|   2|\n",
      "|   102|  Jane Smith|9123456789|   2|\n",
      "|   101|    John Doe|9876543210|   2|\n",
      "|   103|Robert Brown|9988776655|   2|\n",
      "+------+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "win = Window.partitionBy('name').orderBy(col('name'))\n",
    "\n",
    "df.withColumn('rank',row_number().over(win)).filter('rank == 2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "| Name|Age|      Education|\n",
      "+-----+---+---------------+\n",
      "| Azar| 24|     MBA,BE,HSC|\n",
      "| Babu| 28|           null|\n",
      "| Mani| 22|MBA,BSE,Diploma|\n",
      "|Mohan| 29|   MBA,BArch,SC|\n",
      "+-----+---+---------------+\n",
      "\n",
      "+-----+---+---------+-----+\n",
      "| Name|Age|Education|Index|\n",
      "+-----+---+---------+-----+\n",
      "| Azar| 24|       BE|    0|\n",
      "| Azar| 24|      HSC|    1|\n",
      "| Azar| 24|      MBA|    2|\n",
      "| Babu| 28|     None|    0|\n",
      "| Mani| 22|      BSE|    0|\n",
      "| Mani| 22|  Diploma|    1|\n",
      "| Mani| 22|      MBA|    2|\n",
      "|Mohan| 29|    BArch|    0|\n",
      "|Mohan| 29|      MBA|    1|\n",
      "|Mohan| 29|       SC|    2|\n",
      "+-----+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = spark.read.option('quote','\\'').csv('sample_data/education.csv',header=True,inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "result = df.withColumn('Education',explode(split(coalesce(col('Education'),lit('None')),',')))\n",
    "\n",
    "wind = Window.partitionBy('Name').orderBy('Education')\n",
    "\n",
    "result.withColumn('Index',row_number().over(wind)-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| Name|Age|Education|\n",
      "+-----+---+---------+\n",
      "| Azar| 24|      MBA|\n",
      "| Azar| 24|       BE|\n",
      "| Azar| 24|      HSC|\n",
      "| Babu| 28|     null|\n",
      "| Mani| 22|      MBA|\n",
      "| Mani| 22|      BSE|\n",
      "| Mani| 22|  Diploma|\n",
      "|Mohan| 29|      MBA|\n",
      "|Mohan| 29|    BArch|\n",
      "|Mohan| 29|       SC|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "df.withColumn('Education',explode_outer(split(col('Education'),','))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambiguous column name during flattening\n",
    "\n",
    "Spark will automatically detect duplicate columns and while reading it throws error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------+\n",
      "|            delivery|    mobile|           name|\n",
      "+--------------------+----------+---------------+\n",
      "|{123 Park Ave, Ne...|9876543210|       John Doe|\n",
      "|{456 Elm St, Los ...|9988776655|    Alice Smith|\n",
      "|{789 Oak Dr, Chic...|9870011223|Michael Johnson|\n",
      "+--------------------+----------+---------------+\n",
      "\n",
      "root\n",
      " |-- delivery: struct (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|    mobile|           name|             address|    mobile|         name|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|9876543210|       John Doe|123 Park Ave, New...|9123456789|     Jane Doe|\n",
      "|9988776655|    Alice Smith|456 Elm St, Los A...|9012345678|    Bob Smith|\n",
      "|9870011223|Michael Johnson|789 Oak Dr, Chica...|9345678901|Sarah Johnson|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('multiline',True).json('sample_data/ambicol.json')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df1 = df.select('*','delivery.*').drop('delivery')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'name' is ambiguous, could be: name, name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \n\u001b[0;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[0;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Reference 'name' is ambiguous, could be: name, name."
     ]
    }
   ],
   "source": [
    "df1.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|    mobile|           name|             address|    mobile|       name_1|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|9876543210|       John Doe|123 Park Ave, New...|9123456789|     Jane Doe|\n",
      "|9988776655|    Alice Smith|456 Elm St, Los A...|9012345678|    Bob Smith|\n",
      "|9870011223|Michael Johnson|789 Oak Dr, Chica...|9345678901|Sarah Johnson|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = df1.columns\n",
    "d = {}\n",
    "for i in range(len(l)):\n",
    "    if(d.get(l[i])):\n",
    "        l[i] = l[i] + '_' + str(d.get(l[i]))\n",
    "    else:\n",
    "        d[l[i]] = i \n",
    "\n",
    "df2 = df1.toDF(*l)\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD to DF and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "|   c|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "|   c|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# l = ['a','b','c'] X wrong\n",
    "\n",
    "l = [['a'],['b'],['c']]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.collect()\n",
    "\n",
    "df = spark.createDataFrame(rdd,['name'])\n",
    "df.show()\n",
    "\n",
    "rdd.toDF(['name']).show()\n",
    "\n",
    "# Your code is almost correct, but theres a small mismatch in how you're converting the RDD to a DataFrame  specifically, the RDD elements are strings ('a', 'b', 'c'), but Spark expects rows or tuples when creating a DataFrame with a schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='a'), Row(name='b'), Row(name='c')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = df.rdd\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove N lines from a file while reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+\n",
      "|id |name   |city       |\n",
      "+---+-------+-----------+\n",
      "|1  |Alice  |New York   |\n",
      "|2  |Bob    |Los Angeles|\n",
      "|3  |Charlie|Chicago    |\n",
      "|4  |Diana  |Houston    |\n",
      "|5  |Evan   |Phoenix    |\n",
      "+---+-------+-----------+\n",
      "\n",
      "+---------------------------------------+\n",
      "|value                                  |\n",
      "+---------------------------------------+\n",
      "|# This is a sample CSV file            |\n",
      "|# Generated for testing multi-line skip|\n",
      "|# Author: OpenAI                       |\n",
      "|# Date: 2025-04-10                     |\n",
      "|# Below is the actual data             |\n",
      "|                                       |\n",
      "|id,name,city                           |\n",
      "|1,Alice,New York                       |\n",
      "|2,Bob,Los Angeles                      |\n",
      "|3,Charlie,Chicago                      |\n",
      "|4,Diana,Houston                        |\n",
      "|5,Evan,Phoenix                         |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True) \\\n",
    "               .option(\"comment\",\"#\") \\\n",
    "               .csv(\"sample_data/unwanted.csv\")\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df = spark.read.format('text').load(\"sample_data/unwanted.csv\")\n",
    "               \n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile('sample_data/unwanted.csv',1)\n",
    "\n",
    "def skip_first_five_lines(partition_index, lines):\n",
    "    if partition_index == 0:\n",
    "        return iter(list(lines)[6:])  # Skip first 5 lines in first partition\n",
    "    else:\n",
    "        return lines\n",
    "\n",
    "filtered_rdd = rdd.mapPartitionsWithIndex(skip_first_five_lines)\n",
    "filtered_rdd.collect()\n",
    "\n",
    "header = filtered_rdd.first().split(\",\")\n",
    "data_rdd = filtered_rdd.filter(lambda line: line != \",\".join(header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row('1', 'Alice', 'New York')>,\n",
       " <Row('2', 'Bob', 'Los Angeles')>,\n",
       " <Row('3', 'Charlie', 'Chicago')>,\n",
       " <Row('4', 'Diana', 'Houston')>,\n",
       " <Row('5', 'Evan', 'Phoenix')>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rows_rdd = data_rdd.map(lambda line: line.split(\",\")).map(lambda parts: Row(*parts))\n",
    "\n",
    "rows_rdd.collect()\n",
    "\n",
    "# Why are we using Row()?\n",
    "# When creating a DataFrame from an RDD, Spark expects each record (row) to be:\n",
    "\n",
    "# a tuple, or\n",
    "\n",
    "# a Row object\n",
    "\n",
    "# This helps Spark associate values with column names when a schema is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rows_rdd, header)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from multiple directories omitting some \n",
    "\n",
    "method1:\n",
    "list of input paths\n",
    "[\"data1/*.csv\",\"data2/*.csv\"]\n",
    "\n",
    "method2:\n",
    "use regex pattern\n",
    "\"data[1-3]*/*.csv\"\n",
    "\n",
    "method3:\n",
    "use curly braces\n",
    "\"data{1,2,3}*/*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if DF is empty or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = spark.read.csv('sample_data/input1.csv',header=True,inferSchema=True)\n",
    "\n",
    "input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = input.filter('age > 26')\n",
    "records.show()\n",
    "\n",
    "records.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(not records.count()):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(not records.first()):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(not records.take(0)):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(records.rdd.isEmpty()):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rdd.isEmpty()\tChecks if the underlying RDD is empty (efficient)\tLarge DataFrames (best for performance)\n",
    "# df.isEmpty()\tChecks if the DataFrame is empty (available in Spark 3.x and later)\tRecommended for Spark 3.x\n",
    "# df.count()\tReturns row count (slower, scans the entire DataFrame)\tSmall DataFrames, less efficient\n",
    "# df.head() / df.take(1)\tChecks if the first row exists (quick check)\tQuick, small DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators - Alternative for df.count\n",
    "\n",
    "A PySpark accumulator is a shared variable that workers can add to, but only the driver can read from.\n",
    "\n",
    " Use it to:\n",
    "\n",
    "Count rows\n",
    "\n",
    "Count errors, nulls, or bad data\n",
    "\n",
    "Track totals (e.g., bytes processed)\n",
    "\n",
    "Workers can only add\tNo read access inside workers\n",
    "Only reliable in actions\tThey only update when an action is triggered (like .collect(), .count())\n",
    "No guarantees in transformations\tLazy eval means accumulator values aren't reliable in .map() unless an action forces execution\n",
    "\n",
    "\n",
    " When to Use df.count()\n",
    " Ideal for:\n",
    "Total number of rows\n",
    "\n",
    "Size check after filtering or joins\n",
    "\n",
    "Fast, declarative counting\n",
    "\n",
    "df.filter(\"country = 'India'\").count()\n",
    "\n",
    " When to Use Accumulators\n",
    "\n",
    " Ideal for:\n",
    "Counting specific conditions (e.g. number of rows with nulls, invalid formats)\n",
    "\n",
    "Tracking custom metrics during transformation\n",
    "\n",
    "Debugging data pipeline quality issues\n",
    "\n",
    "error_count = sc.accumulator(0)\n",
    "\n",
    "def validate(row):\n",
    "    global error_count\n",
    "    if row['email'] is None:\n",
    "        error_count += 1\n",
    "    return row\n",
    "\n",
    "df.rdd.map(validate).count()\n",
    "print(\"Null emails:\", error_count.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10127"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "record_count = spark.sparkContext.accumulator(0)\n",
    "age_count = spark.sparkContext.accumulator(0)\n",
    "\n",
    "bank = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/BankChurners.csv')\n",
    "\n",
    "bank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10127\n"
     ]
    }
   ],
   "source": [
    "bank.rdd.foreach(lambda x:record_count.add(1))\n",
    "\n",
    "print(record_count.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7730"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.filter('Customer_Age > 40').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7730\n"
     ]
    }
   ],
   "source": [
    "age_count = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def check(row):\n",
    "    if(row['Customer_Age']>40):\n",
    "        age_count.add(1)\n",
    "    return row\n",
    "\n",
    "bank.rdd.map(check).collect()\n",
    "\n",
    "print(age_count.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking Data - Sensitive data\n",
    "\n",
    "Use UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "+------+------------+----------+\n",
      "\n",
      "root\n",
      " |-- rollno: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n",
      "+------+------------+----------+-------------+\n",
      "|rollno|        name|    mobile|mobile_masked|\n",
      "+------+------------+----------+-------------+\n",
      "|   101|    John Doe|9876543210|  98*******10|\n",
      "|   102|  Jane Smith|9123456789|  91*******89|\n",
      "|   103|Robert Brown|9988776655|  99*******55|\n",
      "|   104| Alice Green|9012345678|  90*******78|\n",
      "|   101|    John Doe|9876543210|  98*******10|\n",
      "|   102|  Jane Smith|9123456789|  91*******89|\n",
      "|   103|Robert Brown|9988776655|  99*******55|\n",
      "|   104| Alice Green|9012345678|  90*******78|\n",
      "+------+------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf,col\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField('rollno',IntegerType()),\n",
    "    StructField('name',StringType()),\n",
    "    StructField('mobile',StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "unmask = spark.read.schema(schema=schema).csv('sample_data/duplicates.csv',header=True)\n",
    "\n",
    "unmask.show()\n",
    "\n",
    "unmask.printSchema()\n",
    "\n",
    "def mobile_mask(col):\n",
    "    new = col[0:2] + ('*' * 7) + col[-2] + col[-1]\n",
    "    return new\n",
    "    \n",
    "# print(mobile_mask('9876543210'))\n",
    "\n",
    "mask_udf = udf(mobile_mask,StringType())\n",
    "\n",
    "mask_df = unmask.withColumn('mobile_masked',mask_udf(col('mobile')))\n",
    "\n",
    "mask_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+\n",
      "|customer_id|transaction_type|amount|\n",
      "+-----------+----------------+------+\n",
      "|        101|          credit| 500.0|\n",
      "|        102|           debit| 200.0|\n",
      "|        101|           debit| 150.0|\n",
      "|        103|          credit|1000.0|\n",
      "|        102|          credit| 300.0|\n",
      "|        104|           debit| 120.0|\n",
      "|        105|          credit| 750.0|\n",
      "|        103|           debit| 400.0|\n",
      "|        104|          credit| 250.0|\n",
      "|        101|          credit| 100.0|\n",
      "+-----------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (101, 'credit', 500.00),\n",
    "    (102, 'debit', 200.00),\n",
    "    (101, 'debit', 150.00),\n",
    "    (103, 'credit', 1000.00),\n",
    "    (102, 'credit', 300.00),\n",
    "    (104, 'debit', 120.00),\n",
    "    (105, 'credit', 750.00),\n",
    "    (103, 'debit', 400.00),\n",
    "    (104, 'credit', 250.00),\n",
    "    (101, 'credit', 100.00),\n",
    "]\n",
    "\n",
    "columns = ['customer_id', 'transaction_type', 'amount']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+\n",
      "|customer_id|transaction_type|amount|\n",
      "+-----------+----------------+------+\n",
      "|        101|          credit| 500.0|\n",
      "|        102|           debit| 200.0|\n",
      "|        101|           debit| 150.0|\n",
      "|        103|          credit|1000.0|\n",
      "|        102|          credit| 300.0|\n",
      "|        104|           debit| 120.0|\n",
      "|        105|          credit| 750.0|\n",
      "|        103|           debit| 400.0|\n",
      "|        104|          credit| 250.0|\n",
      "|        101|          credit| 100.0|\n",
      "+-----------+----------------+------+\n",
      "\n",
      "+-----------+---------------+\n",
      "|customer_id|sum(amount_chk)|\n",
      "+-----------+---------------+\n",
      "|        101|          450.0|\n",
      "|        102|          100.0|\n",
      "|        103|          600.0|\n",
      "|        104|          130.0|\n",
      "|        105|          750.0|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df1 = df.withColumn('amount_chk',when(col('transaction_type')=='credit',col('amount')).otherwise(-1*col('amount')))\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1.select('customer_id','amount_chk').groupBy('customer_id').sum('amount_chk').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|customer_id|sum(amount_chk)|\n",
      "+-----------+---------------+\n",
      "|        101|          450.0|\n",
      "|        102|          100.0|\n",
      "|        103|          600.0|\n",
      "|        104|          130.0|\n",
      "|        105|          750.0|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df1.groupBy('customer_id').agg(sum('amount_chk')).alias('total balance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+\n",
      "|customer_id|credit|debit|\n",
      "+-----------+------+-----+\n",
      "|        103|1000.0|400.0|\n",
      "|        104| 250.0|120.0|\n",
      "|        105| 750.0| null|\n",
      "|        101| 600.0|150.0|\n",
      "|        102| 300.0|200.0|\n",
      "+-----------+------+-----+\n",
      "\n",
      "+-----------+------------------------------------------+\n",
      "|customer_id|(coalesce(credit, 0) - coalesce(debit, 0))|\n",
      "+-----------+------------------------------------------+\n",
      "|        103|                                     600.0|\n",
      "|        104|                                     130.0|\n",
      "|        105|                                     750.0|\n",
      "|        101|                                     450.0|\n",
      "|        102|                                     100.0|\n",
      "+-----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('customer_id').pivot('transaction_type').agg(sum('amount'))\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df2.selectExpr('customer_id','coalesce(credit,0)-coalesce(debit,0)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "union and unionByName\n",
    "\n",
    "union \n",
    "\n",
    "merge the 2 dataframes with respect to the position of the column and datatype of the column \n",
    "\n",
    "requires both the tables to have same no of columns\n",
    "\n",
    "column names should be of same order \n",
    "\n",
    "but unionByName will match based on the column names and even if there are different no of columns, it will take null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition(col) Vs PartitionBy(col)\n",
    "\n",
    "PartitionBy can we used only with write, creates separate folders for each partition. There wont be any shuffle with partitionBy, it will directly push the data to the folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files recursively\n",
    "\n",
    "use option('recursiveFileLookup',true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date time handling in schema using inferschema\n",
    "\n",
    "use option('timestampFormat','M/d/yyy')\n",
    "use option('dateformat','M/d/yyy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top N Products per Category by Revenue\n",
    "\n",
    "Expected Output Example\n",
    "category\tproduct_name\ttotal_revenue\n",
    "Electronics\tLaptop\t450\n",
    "Electronics\tPhone\t300\n",
    "Furniture\tDesk\t400\n",
    "Furniture\tChair\t400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format('csv').option('inferSchema',True).option('header',True).load('sample_data/sales.csv')\n",
    "products = spark.read.format('csv').option('inferSchema',True).option('header',True).load('sample_data/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+-----+-------------------+\n",
      "|sale_id|product_id|quantity|price|          sale_date|\n",
      "+-------+----------+--------+-----+-------------------+\n",
      "|      1|       101|       2|  100|2024-01-01 00:00:00|\n",
      "|      2|       102|       3|  150|2024-01-01 00:00:00|\n",
      "|      3|       103|       5|   80|2024-01-02 00:00:00|\n",
      "|      4|       101|       1|  100|2024-01-03 00:00:00|\n",
      "|      5|       104|       2|  200|2024-01-03 00:00:00|\n",
      "+-------+----------+--------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "|product_id|product_name|   category|\n",
      "+----------+------------+-----------+\n",
      "|       101|       Phone|Electronics|\n",
      "|       102|      Laptop|Electronics|\n",
      "|       103|        Desk|  Furniture|\n",
      "|       104|       Chair|  Furniture|\n",
      "+----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|product_id|total_sales|\n",
      "+----------+-----------+\n",
      "|       101|        300|\n",
      "|       103|        400|\n",
      "|       102|        450|\n",
      "|       104|        400|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "total_Sales = sales.withColumn('total_sales',col('quantity')*col('price')).select('product_id','total_sales').groupBy('product_id').agg(sum('total_sales').alias('total_sales'))\n",
    "\n",
    "\n",
    "total_Sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----------+\n",
      "|product_name|total_sales|   category|\n",
      "+------------+-----------+-----------+\n",
      "|      Laptop|        450|Electronics|\n",
      "|        Desk|        400|  Furniture|\n",
      "|       Chair|        400|  Furniture|\n",
      "+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "output = total_Sales.join(products,on='product_id',how='inner').withColumn('rank',rank().over(Window().partitionBy('category').orderBy(desc('total_sales')))).filter(col('rank') == 1).select('product_name','total_sales','category')\n",
    "\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capitalise each name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|varshini|\n",
      "|  2| sathish|\n",
      "|  3|   jothi|\n",
      "|  4| neelesh|\n",
      "|  5| kasturi|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Varshini|\n",
      "|  2| Sathish|\n",
      "|  3|   Jothi|\n",
      "|  4| Neelesh|\n",
      "|  5| Kasturi|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Varshini|\n",
      "|  2| Sathish|\n",
      "|  3|   Jothi|\n",
      "|  4| Neelesh|\n",
      "|  5| Kasturi|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap,udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "Persons = spark.read.format('csv').option('inferSchema',True).option('header',True).load('sample_data/capitalise.csv')\n",
    "\n",
    "Persons.show()\n",
    "\n",
    "output = Persons.withColumn('name',initcap(col('name')))\n",
    "\n",
    "output.show()\n",
    "\n",
    "# using UDF\n",
    "\n",
    "def capitalise(c):\n",
    "    return c[0].upper()+c[1:]\n",
    "\n",
    "capitalise_udf = udf(capitalise,StringType())\n",
    "output = Persons.withColumn('name',capitalise_udf(col('name')))\n",
    "\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple filter conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Varshini|\n",
      "|  2| Sathish|\n",
      "|  3|   Jothi|\n",
      "|  4| Neelesh|\n",
      "|  5| Kasturi|\n",
      "+---+--------+\n",
      "\n",
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  4|Neelesh|\n",
      "|  5|Kasturi|\n",
      "+---+-------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Varshini|\n",
      "|  4| Neelesh|\n",
      "+---+--------+\n",
      "\n",
      "+---+--------+\n",
      "| id|    name|\n",
      "+---+--------+\n",
      "|  1|Varshini|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chained (second filter works on the resultset from first filter)\n",
    "output.printSchema()\n",
    "output.show()\n",
    "output.filter(col('id')>2).filter(col('id')>3).show()\n",
    "output.filter((col('id')==1) | (col('id')==4)).show()\n",
    "output.filter((col('id')==1) & (col('name')=='Varshini')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used read formats \n",
    "\n",
    "How to Use Formats with .format()\n",
    "You can specify the format explicitly:\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"path/to/file.json\")\n",
    "Or use shorthand methods like .csv(), .json(), .parquet(), etc.\n",
    "\n",
    "Additional Formats via Packages\n",
    "Some formats need extra Spark packages:\n",
    "\n",
    "Format\tPackage / Maven Coordinate\n",
    "Avro\torg.apache.spark:spark-avro_2.12\n",
    "XML\tcom.databricks:spark-xml_2.12\n",
    "Delta Lake\tio.delta:delta-core_2.12\n",
    "\n",
    "You add them when launching Spark, e.g.:\n",
    "\n",
    "spark-submit --packages com.databricks:spark-xml_2.12:0.15.0 ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\tComma-separated values\t\n",
    "spark.read.csv(\"file.csv\", header=True)\n",
    "\n",
    "# JSON\tJavaScript Object Notation\t\n",
    "\n",
    "spark.read.json(\"file.json\")\n",
    "\n",
    "# Parquet\tColumnar storage format optimized for speed\t\n",
    "\n",
    "spark.read.parquet(\"file.parquet\")\n",
    "\n",
    "# Avro\tRow-based binary format (requires package)\t\n",
    "\n",
    "spark.read.format(\"avro\").load(\"file.avro\")\n",
    "\n",
    "# ORC\tOptimized row columnar storage (mainly Hive)\t\n",
    "\n",
    "spark.read.orc(\"file.orc\")\n",
    "\n",
    "# Text\tPlain text files\n",
    "\t\n",
    "spark.read.text(\"file.txt\")\n",
    "\n",
    "# XML\tXML format (requires additional package)\n",
    "\t\n",
    "spark.read.format(\"xml\").option(\"rowTag\",\"tag\").load(\"file.xml\")\n",
    "\n",
    "# Delta Lake\tTransactional storage layer on Parquet\t\n",
    "spark.read.format(\"delta\").load(\"path_to_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from Kafka data source\n",
    "\n",
    "Reading from Kafka using Spark is a common pattern for real-time streaming data pipelines. Spark provides built-in support for Kafka via Structured Streaming.\n",
    "\n",
    "1. Setup SparkSession with Kafka support\n",
    "Make sure you include the Kafka package when launching Spark:\n",
    "\n",
    "spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1 your_script.py\n",
    "\n",
    "Or if you are running in a notebook environment, ensure the package is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic\"\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "# Kafka data comes as key and value in binary format, so cast to string if needed\n",
    "df_string = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Show data in streaming console for debugging\n",
    "query = df_string.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# In PySpark Structured Streaming, .awaitTermination() is a method used to block your driver program until your streaming query is stopped, fails, or you manually terminate it. When you start a streaming query with .start(), Spark runs it asynchronously in the background. If you don't explicitly wait for it to finish, your script may exit immediately, especially in standalone or local mode.\n",
    "\n",
    "#.start() begins streaming in the background\n",
    "\n",
    "# .awaitTermination() pauses your main thread (driver program) and waits for:\n",
    "\n",
    "# A termination signal (Ctrl+C or query.stop())\n",
    "\n",
    "# A failure (e.g., write path inaccessible)\n",
    "\n",
    "# Or a timeout (optional)\n",
    "\n",
    "# You can parse the value column (which is the message payload) as JSON or any format you expect:\n",
    "\n",
    "# Define schema of your Kafka message JSON\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", IntegerType()) \\\n",
    "    .add(\"name\", StringType()) \\\n",
    "    .add(\"city\", StringType())\n",
    "\n",
    "parsed_df = df_string.withColumn(\"jsonData\", from_json(col(\"value\"), schema)) \\\n",
    "    .select(\"jsonData.*\")\n",
    "\n",
    "parsed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sink in Spark Structured Streaming\n",
    "\n",
    "In Structured Streaming, a sink is the destination where the results of your streaming computations are written or outputted.\n",
    "\n",
    "Sink Type\tUse Case / Description\t                                Format\n",
    "Console\tFor debugging, shows streaming data on terminal\t            \"console\"\n",
    "File\tWrite data as Parquet, CSV, JSON, etc. to a directory\t    \"parquet\", \"csv\"\n",
    "Kafka\tWrite transformed data back to a Kafka topic\t            \"kafka\"\n",
    "Memory\tStore results in-memory (for debugging via SQL queries)\t    \"memory\"\n",
    "Foreach\tCustom logic per row (e.g., send to DB or API)\t            \"foreach\"\n",
    "Delta\tWrite to a Delta Lake table (ACID-compliant)\t            \"delta\"\n",
    "JDBC\tWrite to databases like MySQL, Postgres, etc.\t            \"jdbc\" (via foreachBatch)\n",
    "\n",
    "Output Modes\n",
    "Each sink supports different output modes:\n",
    "\n",
    "Mode\tDescription\n",
    "append\tOnly new rows since the last trigger\n",
    "complete\tThe full result table (only for aggregations)\n",
    "update\tOnly rows that changed since the last trigger (not all sinks supports update like file sink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Console Sink (for debugging)\n",
    "\n",
    "df.writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "\n",
    "# File Sink\n",
    "\n",
    "# Write streaming data to file:\n",
    "\n",
    "df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"output/path\") \\\n",
    "    .option(\"checkpointLocation\", \"chkpoint/path\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Kafka Sink\n",
    "# Write data back to Kafka:\n",
    "\n",
    "\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"output_topic\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "# Foreach / ForeachBatch Sink\n",
    "# Use this for custom logic, like saving to a database or external service:\n",
    "\n",
    "\n",
    "def write_to_db(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost/test\") \\\n",
    "        .option(\"dbtable\", \"stream_table\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"pass\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "df.writeStream \\\n",
    "    .foreachBatch(write_to_db) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Checkpointing in Spark Structured Streaming?\n",
    "\n",
    "Checkpointing is the process of saving the state and progress of your streaming application to durable storage (like HDFS or S3). It allows Spark to recover from failures and maintain fault tolerance by resuming from the last known good state instead of starting from scratch.\n",
    "\n",
    "Fault Tolerance: Restarts from the last successful processing point.\n",
    "\n",
    "Stateful Operations: Required for operations like update, window, or aggregations.\n",
    "\n",
    "Exactly-Once Guarantees: Helps avoid duplicate processing.\n",
    "\n",
    "What Gets Stored in a Checkpoint?\n",
    "\n",
    "Offsets of sources (like Kafka topic offsets)\n",
    "\n",
    "State data (if using stateful transformations like groupBy, window, etc.)\n",
    "\n",
    "Metadata about the query (e.g., schema, batch ID, etc.)\n",
    "\n",
    "Case\tIs Checkpointing Required?\n",
    "Stateless query (append)\tOptional\n",
    "Stateful query (e.g., groupBy)\tRequired\n",
    "Recovery from failure\tRequired\n",
    "Using output sinks like Kafka\tStrongly recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available read options for structured streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. File Source\n",
    "# Supports: csv, json, parquet, orc, text, delta, etc.\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/path/to/directory\")\n",
    "    \n",
    "#  Spark watches the directory for new files (it does not re-read old ones).\n",
    "\n",
    "# 2. Kafka\n",
    "# Reads from one or more Kafka topics.\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"host:port\") \\\n",
    "    .option(\"subscribe\", \"topic1\") \\\n",
    "    .load()\n",
    "    \n",
    "# 3. Socket Source (for testing)\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Reads lines from a TCP socket. Good for learning/testing only.\n",
    "\n",
    "# 4. Rate Source (generate rows for testing)\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 5) \\\n",
    "    .load()\n",
    "    \n",
    "# Emits a stream of timestamps and values.\n",
    "\n",
    "# 5. Delta Lake (Change Data Capture)\n",
    "# If you're using Delta Lake, you can stream from it.\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/delta/table/path\")\n",
    "    \n",
    "    \n",
    "# Optional Read Options\n",
    "# These can be used across sources:\n",
    "\n",
    "# Option\tDescription\n",
    "# maxFilesPerTrigger\tLimit file ingestion rate (file source)\n",
    "# includeTimestamp\tAdd Kafka message timestamp\n",
    "# failOnDataLoss\tHandle missing data (Kafka, files)\n",
    "# startingOffsets\tFor Kafka: \"latest\" or \"earliest\"\n",
    "# latestFirst\tRead newest files first (some sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readStream on Delta\n",
    "\n",
    "Reading from Delta Lake using readStream in Spark Structured Streaming is a powerful feature that enables incremental reads of new data, much like how you'd read from Kafka or a file stream  but with transactional consistency and schema evolution support.\n",
    "\n",
    "It monitors a Delta table or path and streams new data as it's written to the table (via append, merge, etc.).\n",
    "\n",
    "Its like a CDC (Change Data Capture) mechanism  you get only new rows appended to the Delta table.\n",
    "\n",
    "Option\tDescription\n",
    ".option(\"maxFilesPerTrigger\", n)\tLimit ingestion rate per micro-batch\n",
    ".option(\"ignoreChanges\", \"true\")\tIgnore updates/deletes (for append-only)\n",
    ".option(\"readChangeData\", \"true\")\tEnable CDC mode (see below)\n",
    "\n",
    "Delta with streaming supports:\n",
    "\n",
    "Append (most common): Only new data appended\n",
    "\n",
    "Update/Complete: Used with aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hudi on structured streaming\n",
    "\n",
    "Apache Hudi and Delta Lake are both transactional data lake formats, but they have different architectures and APIs, so the Delta-specific options (like readChangeData, readStream, etc.) do not directly work with Hudi.\n",
    "\n",
    "Instead, Hudi has its own way of handling streaming reads, incremental queries, and CDC.\n",
    "\n",
    "1.  Incremental Query (not full streaming)\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    "    .option(\"hoodie.datasource.read.begin.instanttime\", \"20240101000000\") \\\n",
    "    .load(\"/path/to/hudi/table\")\n",
    "\n",
    "Option\t                    Meaning\n",
    "query.type = incremental\tOnly get new/changed records\n",
    "begin.instanttime\t        Start reading from this commit time\n",
    "end.instanttime (optional)\tStop reading at this commit time\n",
    "\n",
    " This is useful for micro-batch streaming pipelines using Spark jobs in a scheduled way.\n",
    "\n",
    "2.  Structured Streaming on Hudi\n",
    "As of Hudi 0.9+, Structured Streaming support is limited but growing. You can try:\n",
    "\n",
    "\n",
    "spark.readStream \\\n",
    "    .format(\"hudi\") \\\n",
    "    .load(\"/path/to/hudi/table\")\n",
    "\n",
    " This does not support incremental CDC-style changes yet like Delta's readChangeData. It's more suited for append-only style reading.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from kafka using batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaBatchRead\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from Kafka in batch\n",
    "df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"my-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Extract key, value, timestamp\n",
    "parsed_df = df.selectExpr(\n",
    "    \"CAST(key AS STRING)\", \n",
    "    \"CAST(value AS STRING)\", \n",
    "    \"timestamp\"\n",
    ")\n",
    "\n",
    "parsed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offsets management\n",
    "\n",
    "Does spark.read (batch) store Kafka offsets?\n",
    "No, when you use spark.read (i.e., batch mode) with Kafka, Spark does not automatically store or manage offsets.\n",
    "\n",
    " Why?\n",
    "Because Spark assumes you're doing stateless one-time reads  so it has no checkpointing, no offset tracking, and no resume logic built-in for batch mode.\n",
    "\n",
    " You are responsible for managing which offsets you read.\n",
    "\n",
    " How to Manage Offsets in Batch Mode\n",
    "To safely consume Kafka data in batch (especially in production), you must manually track offsets between runs.\n",
    "\n",
    " Option 1: Hardcode startingOffsets and endingOffsets\n",
    "\n",
    ".option(\"startingOffsets\", \"\"\"{\"my-topic\":{\"0\":123}}\"\"\") \\\n",
    ".option(\"endingOffsets\", \"\"\"{\"my-topic\":{\"0\":150}}\"\"\")\n",
    "You can:\n",
    "\n",
    "Read the offsets from a database, Delta table, or file\n",
    "\n",
    "Save the latest processed offset after each batch\n",
    "\n",
    " Option 2: Store Kafka offsets yourself\n",
    "\n",
    "Step 1: Read current offset from storage\n",
    "offset = read_offset_from_db_or_file()\n",
    "\n",
    "Step 2: Read batch from Kafka\n",
    "df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"my-topic\") \\\n",
    "    .option(\"startingOffsets\", offset) \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "Step 3: Process & write data\n",
    "df.write.parquet(\"/path/output\")\n",
    "\n",
    "Step 4: Capture & save the latest offsets\n",
    "latest_offset = get_offset_from_df(df)\n",
    "write_offset_to_db_or_file(latest_offset)\n",
    "\n",
    " Option 3: Use readStream Instead (Streaming with Checkpointing)\n",
    "\n",
    "If you want Spark to automatically track offsets, use structured streaming with:\n",
    "\n",
    ".writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"checkpointLocation\", \"/checkpoints/my-kafka-app\") \\\n",
    "  .start(\"/delta/output\")\n",
    "  \n",
    "Then Spark manages offsets via the checkpoint directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar storage\n",
    "\n",
    "parquet, avro and orc has metadata available with data, we cannot view the data as such "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "\n",
    "input1DF.write.format('parquet').save('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n",
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('output')\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time():\n",
    "        start_time = time.time()\n",
    "        func()\n",
    "        end_time = time.time()\n",
    "        return(f\"execution time = {(end_time - start_time)*1000} ms\")\n",
    "    print(inner_get_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time = 255.58853149414062 ms\n",
      "execution time = 200.47974586486816 ms\n"
     ]
    }
   ],
   "source": [
    "@get_time\n",
    "def x():\n",
    "    df = spark.read.format('parquet').load('output')\n",
    "    df.count()\n",
    "\n",
    "@get_time\n",
    "def y():\n",
    "    df = spark.read.format('parquet').load('output')\n",
    "    df.select('Name').count()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand array to multiple columns intead of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+\n",
      "| id|val_0|val_1|val_2|\n",
      "+---+-----+-----+-----+\n",
      "|  1|   10|   20|   30|\n",
      "|  2|   40|   50|   60|\n",
      "+---+-----+-----+-----+\n",
      "\n",
      "+---+-----+-----+-----+\n",
      "| id|val_0|val_1|val_2|\n",
      "+---+-----+-----+-----+\n",
      "|  1|   10|   20|   30|\n",
      "|  2|   40|   50|   60|\n",
      "+---+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample DataFrame with array column\n",
    "data = [\n",
    "    (1, [10, 20, 30]),\n",
    "    (2, [40, 50, 60])\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"id\", \"values\"])\n",
    "\n",
    "# Extract array elements to new columns\n",
    "df_expanded = df.select(\n",
    "    \"id\",\n",
    "    col(\"values\")[0].alias(\"val_0\"),\n",
    "    col(\"values\")[1].alias(\"val_1\"),\n",
    "    col(\"values\")[2].alias(\"val_2\")\n",
    ")\n",
    "\n",
    "df_expanded.show()\n",
    "\n",
    "# +---+-----+-----+-----+\n",
    "# | id|val_0|val_1|val_2|\n",
    "# +---+-----+-----+-----+\n",
    "# |  1|   10|   20|   30|\n",
    "# |  2|   40|   50|   60|\n",
    "# +---+-----+-----+-----+\n",
    "\n",
    "#  Dynamic (Optional)\n",
    "# If you're not sure how many items are in the array, you can:\n",
    "\n",
    "# Use .schema and inspect\n",
    "\n",
    "# Programmatically generate columns using a loop:\n",
    "\n",
    "\n",
    "num_elements = 3  # assume you know or compute it\n",
    "df_expanded = df.select(\n",
    "    [\"id\"] + [col(\"values\")[i].alias(f\"val_{i}\") for i in range(num_elements)]\n",
    ")\n",
    "\n",
    "df_expanded.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
