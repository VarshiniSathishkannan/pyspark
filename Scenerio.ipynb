{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 dataframes with uneven columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Merge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "input1AddDF = input1DF.withColumn('Gender',lit(None))\n",
    "input1AddDF.show()\n",
    "\n",
    "result = input1AddDF.union(input2DF)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField('Name',StringType(),True), # 3rd option is nullable is true or not\n",
    "    StructField('Age',IntegerType(),True),\n",
    "    StructField('Gender',StringType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).schema(schema).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.option('header',True).csv('sample_data\\input2.csv',schema=schema)\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.union(input2DF)\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.join(input2DF,on=['Name','Age'],how='outer')\n",
    "result.show()\n",
    "\n",
    "result = input1DF.join(input2DF,[input1DF.Name==input2DF.Name,input1DF.Age==input2DF.Age],how='outer').select(input1DF.Name,input1DF.Age,input2DF.Gender)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best approach - Automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "listA = set(input1DF.columns)-set(input2DF.columns)\n",
    "listB = set(input2DF.columns)-set(input1DF.columns)\n",
    "\n",
    "for i in listA:\n",
    "    input2DF = input2DF.withColumn(i,lit(None))\n",
    "\n",
    "for i in listB:\n",
    "    input1DF = input1DF.withColumn(i,lit(None))\n",
    "    \n",
    "resut = input1DF.union(input2DF)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply line break every 5th occurance from | delimited input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |chk                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "\n",
    "input = spark.read.csv('sample_data\\input.txt')\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn(\"chk\",regexp_replace(\"_c0\",\"(.*?\\\\|){4}\",\"$0-\"))\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn('col_explode',explode(split('chk','\\|-')))\n",
    "\n",
    "input.select(input.col_explode).show()\n",
    "\n",
    "result = input.select(input.col_explode)\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|split(col_explode, \\|, -1)|\n",
      "+--------------------------+\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(split('col_explode','\\|')).show()\n",
    "\n",
    "result.rdd.map(lambda i:i).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rdd.map(lambda i:len(i)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = result.rdd.map(lambda i:i[0].split('|'))\n",
    "\n",
    "result = result_rdd.toDF(['Name','Qualification','S.no','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files recursively under Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                      |\n",
      "+--------+---+--------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "+--------+---+--------------------------------------------------------------+\n",
      "\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                             |\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Mo      |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Arv     |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Rish    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Anus    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Gaya    |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                             |\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Mo      |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Arv     |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Rish    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Anus    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Gaya    |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).load('sample_data/recursive').withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "paths = [\n",
    "    'sample_data/recursive',\n",
    "    'sample_data/recursive/level1'\n",
    "]\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).load(paths).withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).option('recursiveFileLookup',True).load(paths).withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one returns results faster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df = spark.read.csv()\n",
    "df = df.filter()\n",
    "df = df.sort()\n",
    "df.count()\n",
    "#2\n",
    "df = spark.read.csv()\n",
    "df = df.sort()\n",
    "df = df.filter()\n",
    "df.count()\n",
    "#3\n",
    "df = spark.read.csv()\n",
    "df.sort()\n",
    "df = df.filter()\n",
    "df = df.sort()\n",
    "df.count()\n",
    "#4\n",
    "df = spark.read.csv()\n",
    "df.sort()\n",
    "df = df.sort()\n",
    "df = df.filter()\n",
    "df.count()\n",
    "\n",
    "# Predicate pushdown pushes the filter to the start even if sort is given first\n",
    "\n",
    "# option A performs better, Caching took time, this is not the right case for doing caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferschema Vs Schema definition\n",
    "\n",
    "Always define schema manually since inferschema scans the table twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|768805383|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|     12691.0|                777|        11914.0|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|818770008|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|      8256.0|                864|         7392.0|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|713982108|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|      3418.0|                  0|         3418.0|               2.594|           1887|            20|              2.333|                  0.0|\n",
      "|769911858|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|      3313.0|               2517|          796.0|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|709106358|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|      4716.0|                  0|         4716.0|               2.175|            816|            28|                2.5|                  0.0|\n",
      "|713061558|Existing Customer|          44|     M|              2|       Graduate|       Married|    $40K - $60K|         Blue|            36|                       3|                     1|                    2|      4010.0|               1247|         2763.0|               1.376|           1088|            24|              0.846|                0.311|\n",
      "|810347208|Existing Customer|          51|     M|              4|        Unknown|       Married|        $120K +|         Gold|            46|                       6|                     1|                    3|     34516.0|               2264|        32252.0|               1.975|           1330|            31|              0.722|                0.066|\n",
      "|818906208|Existing Customer|          32|     M|              0|    High School|       Unknown|    $60K - $80K|       Silver|            27|                       2|                     2|                    2|     29081.0|               1396|        27685.0|               2.204|           1538|            36|              0.714|                0.048|\n",
      "|710930508|Existing Customer|          37|     M|              3|     Uneducated|        Single|    $60K - $80K|         Blue|            36|                       5|                     2|                    0|     22352.0|               2517|        19835.0|               3.355|           1350|            24|              1.182|                0.113|\n",
      "|719661558|Existing Customer|          48|     M|              2|       Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     3|                    3|     11656.0|               1677|         9979.0|               1.524|           1441|            32|              0.882|                0.144|\n",
      "|708790833|Existing Customer|          42|     M|              5|     Uneducated|       Unknown|        $120K +|         Blue|            31|                       5|                     3|                    2|      6748.0|               1467|         5281.0|               0.831|           1201|            42|               0.68|                0.217|\n",
      "|710821833|Existing Customer|          65|     M|              1|        Unknown|       Married|    $40K - $60K|         Blue|            54|                       6|                     2|                    3|      9095.0|               1587|         7508.0|               1.433|           1314|            26|              1.364|                0.174|\n",
      "|710599683|Existing Customer|          56|     M|              1|        College|        Single|   $80K - $120K|         Blue|            36|                       3|                     6|                    0|     11751.0|                  0|        11751.0|               3.397|           1539|            17|               3.25|                  0.0|\n",
      "|816082233|Existing Customer|          35|     M|              3|       Graduate|       Unknown|    $60K - $80K|         Blue|            30|                       5|                     1|                    3|      8547.0|               1666|         6881.0|               1.163|           1311|            33|                2.0|                0.195|\n",
      "|712396908|Existing Customer|          57|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            48|                       5|                     2|                    2|      2436.0|                680|         1756.0|                1.19|           1570|            29|              0.611|                0.279|\n",
      "|714885258|Existing Customer|          44|     M|              4|        Unknown|       Unknown|   $80K - $120K|         Blue|            37|                       5|                     1|                    2|      4234.0|                972|         3262.0|               1.707|           1348|            27|                1.7|                 0.23|\n",
      "|709967358|Existing Customer|          48|     M|              4|  Post-Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     2|                    3|     30367.0|               2362|        28005.0|               1.708|           1671|            27|              0.929|                0.078|\n",
      "|753327333|Existing Customer|          41|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            34|                       4|                     4|                    1|     13535.0|               1291|        12244.0|               0.653|           1028|            21|              1.625|                0.095|\n",
      "|806160108|Existing Customer|          61|     M|              1|    High School|       Married|    $40K - $60K|         Blue|            56|                       2|                     2|                    3|      3193.0|               2517|          676.0|               1.831|           1336|            30|              1.143|                0.788|\n",
      "|709327383|Existing Customer|          45|     F|              2|       Graduate|       Married|        Unknown|         Blue|            37|                       6|                     1|                    2|     14470.0|               1157|        13313.0|               0.966|           1207|            21|              0.909|                 0.08|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/BankChurners.csv')\n",
    "\n",
    "bank.show()\n",
    "\n",
    "bank.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = bank.repartition(4)\n",
    "input.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|partition_id|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|708721533|Existing Customer|          56|     F|              1|       Graduate|       Married| Less than $40K|         Blue|            36|                       2|                     1|                    2|      5158.0|               1148|         4010.0|               0.637|          13113|           128|              0.882|                0.223|           0|\n",
      "|720362058|Attrited Customer|          57|     M|              2|     Uneducated|        Single|    $60K - $80K|       Silver|            49|                       4|                     3|                    3|     32182.0|                  0|        32182.0|                0.52|           1937|            46|              0.704|                  0.0|           0|\n",
      "|708360108|Existing Customer|          50|     M|              1|        College|        Single|    $40K - $60K|         Blue|            41|                       3|                     1|                    1|      2800.0|               1338|         1462.0|               0.818|           4869|            90|                0.8|                0.478|           0|\n",
      "|713090658|Existing Customer|          58|     F|              2|     Uneducated|        Single|        Unknown|         Blue|            44|                       1|                     2|                    2|      2599.0|               1407|         1192.0|               0.727|           8061|            98|              0.531|                0.541|           0|\n",
      "|714943233|Existing Customer|          49|     M|              2|      Doctorate|       Married|    $40K - $60K|         Blue|            31|                       2|                     2|                    2|      1915.0|               1545|          370.0|               0.775|           3911|            80|              0.778|                0.807|           0|\n",
      "|717052158|Existing Customer|          39|     M|              3|    High School|        Single|    $60K - $80K|         Blue|            32|                       3|                     2|                    2|     21718.0|               2386|        19332.0|               0.743|           3492|            66|              0.692|                 0.11|           0|\n",
      "|794227308|Existing Customer|          56|     M|              2|        College|       Married|   $80K - $120K|         Blue|            50|                       1|                     1|                    3|     10214.0|               1433|         8781.0|                0.64|           4493|            79|              0.881|                 0.14|           0|\n",
      "|710932683|Existing Customer|          55|     F|              2|    High School|        Single| Less than $40K|         Blue|            36|                       2|                     1|                    2|      6394.0|                  0|         6394.0|               0.656|           3470|            69|              0.917|                  0.0|           0|\n",
      "|712347258|Existing Customer|          43|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            35|                       3|                     3|                    2|      2209.0|                  0|         2209.0|               0.749|           4419|            68|              0.619|                  0.0|           0|\n",
      "|713205108|Existing Customer|          42|     F|              4|       Graduate|       Married|    $40K - $60K|         Blue|            27|                       3|                     1|                    1|      1950.0|               1062|          888.0|                0.48|           3873|            76|              0.854|                0.545|           0|\n",
      "|713808633|Existing Customer|          31|     F|              0|     Uneducated|      Divorced| Less than $40K|       Silver|            23|                       2|                     2|                    1|     10850.0|               1873|         8977.0|               0.995|          13794|           127|              0.789|                0.173|           0|\n",
      "|826168083|Existing Customer|          49|     F|              4|        Unknown|      Divorced| Less than $40K|         Blue|            45|                       3|                     3|                    3|      1757.0|               1307|          450.0|               0.769|           4750|            72|              0.756|                0.744|           0|\n",
      "|721258158|Existing Customer|          65|     F|              1|     Uneducated|        Single|        Unknown|         Blue|            51|                       3|                     3|                    5|     11037.0|               1491|         9546.0|               0.651|           2429|            63|               0.75|                0.135|           0|\n",
      "|713906508|Existing Customer|          42|     F|              3|        Unknown|        Single| Less than $40K|         Blue|            36|                       6|                     3|                    3|      2837.0|                834|         2003.0|               0.427|           3892|            69|              0.408|                0.294|           0|\n",
      "|740784258|Existing Customer|          47|     F|              2|        College|        Single| Less than $40K|         Blue|            40|                       4|                     3|                    3|      1789.0|               1088|          701.0|               0.683|           4191|            90|              0.525|                0.608|           0|\n",
      "|717504258|Existing Customer|          38|     M|              2|        College|       Married|    $40K - $60K|         Blue|            25|                       4|                     1|                    4|      3735.0|               1199|         2536.0|               0.886|           1573|            35|               0.75|                0.321|           0|\n",
      "|717529233|Attrited Customer|          40|     F|              3|       Graduate|        Single| Less than $40K|         Blue|            30|                       4|                     3|                    2|      3319.0|                  0|         3319.0|               0.262|           4840|            44|              0.375|                  0.0|           0|\n",
      "|716421633|Attrited Customer|          43|     F|              3|       Graduate|       Married| Less than $40K|         Blue|            36|                       4|                     2|                    4|      3523.0|                  0|         3523.0|               0.973|           3174|            44|              0.222|                  0.0|           0|\n",
      "|715593783|Existing Customer|          52|     M|              1|        Unknown|       Married|   $80K - $120K|         Blue|            36|                       4|                     2|                    2|     14858.0|               1594|        13264.0|                0.51|           4286|            72|              0.636|                0.107|           0|\n",
      "|717098208|Existing Customer|          48|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            41|                       5|                     2|                    2|      5848.0|                950|         4898.0|               0.726|           1961|            51|              0.545|                0.162|           0|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 2531|\n",
      "|           1| 2532|\n",
      "|           2| 2532|\n",
      "|           3| 2532|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "input = input.withColumn('partition_id',spark_partition_id())\n",
    "input.show()\n",
    "\n",
    "countByPartition = input.groupBy(input.partition_id).count()\n",
    "countByPartition.show()\n",
    "\n",
    "# Data is evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = bank.repartition(200,\"Card_Category\")\n",
    "input.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|partition_id|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|788386908|Existing Customer|          48|     M|              3|       Graduate|       Unknown|   $80K - $120K|     Platinum|            41|                       3|                     2|                    2|     34516.0|               1531|        32985.0|               0.862|           1156|            29|              1.071|                0.044|          38|\n",
      "|779519058|Existing Customer|          51|     F|              3|  Post-Graduate|      Divorced|        Unknown|     Platinum|            34|                       3|                     1|                    2|     34516.0|               1578|        32938.0|               0.725|           1929|            40|              0.481|                0.046|          38|\n",
      "|708654933|Attrited Customer|          51|     F|              2|      Doctorate|       Married| Less than $40K|     Platinum|            42|                       3|                     2|                    3|     15987.0|                193|        15794.0|               0.435|           2021|            46|              0.394|                0.012|          38|\n",
      "|714754533|Existing Customer|          51|     M|              0|      Doctorate|        Single|        $120K +|     Platinum|            44|                       3|                     2|                    3|     34516.0|               1925|        32591.0|               0.764|           8012|            87|              0.582|                0.056|          38|\n",
      "|714077583|Attrited Customer|          43|     M|              3|  Post-Graduate|       Married|    $40K - $60K|     Platinum|            31|                       2|                     3|                    4|     23981.0|                593|        23388.0|               0.987|           4758|            65|              0.512|                0.025|          38|\n",
      "|709269708|Existing Customer|          45|     M|              4|       Graduate|        Single|        $120K +|     Platinum|            34|                       2|                     2|                    1|     34516.0|               1488|        33028.0|               0.732|           7281|            95|              0.532|                0.043|          38|\n",
      "|719071158|Attrited Customer|          54|     F|              0|       Graduate|        Single|        Unknown|     Platinum|            38|                       2|                     2|                    2|     34516.0|                  0|        34516.0|               0.695|           3901|            54|              0.421|                  0.0|          38|\n",
      "|711364233|Existing Customer|          41|     M|              4|        Unknown|       Married|    $60K - $80K|     Platinum|            26|                       1|                     1|                    2|     34516.0|               1559|        32957.0|               0.759|           8888|           104|              0.763|                0.045|          38|\n",
      "|823848183|Existing Customer|          39|     F|              1|        Unknown|        Single| Less than $40K|     Platinum|            35|                       1|                     2|                    3|     15987.0|               1494|        14493.0|               0.731|           8438|            92|               0.84|                0.093|          38|\n",
      "|714190983|Attrited Customer|          51|     F|              2|       Graduate|        Single|        Unknown|     Platinum|            32|                       2|                     3|                    3|     34516.0|                531|        33985.0|                0.98|           5418|            65|              0.711|                0.015|          38|\n",
      "|770848308|Existing Customer|          45|     M|              2|       Graduate|       Married|    $60K - $80K|     Platinum|            31|                       2|                     2|                    1|     34516.0|               1308|        33208.0|               0.746|           8773|           105|               0.78|                0.038|          38|\n",
      "|713870958|Existing Customer|          56|     F|              3|     Uneducated|        Single|        Unknown|     Platinum|            46|                       2|                     3|                    2|     34516.0|                  0|        34516.0|               0.887|           8416|            93|              0.632|                  0.0|          38|\n",
      "|709319658|Attrited Customer|          48|     F|              4|       Graduate|        Single| Less than $40K|     Platinum|            37|                       5|                     3|                    4|     15987.0|                  0|        15987.0|               0.827|           7681|            71|               0.69|                  0.0|          38|\n",
      "|715017858|Existing Customer|          44|     M|              2|     Uneducated|       Married|        $120K +|     Platinum|            36|                       2|                     1|                    3|     34516.0|               1421|        33095.0|               0.744|          14465|           114|              0.754|                0.041|          38|\n",
      "|771626283|Existing Customer|          54|     M|              2|  Post-Graduate|       Married|    $60K - $80K|     Platinum|            42|                       1|                     3|                    1|     34516.0|               1996|        32520.0|               0.988|          15033|           114|               0.81|                0.058|          38|\n",
      "|710362233|Existing Customer|          48|     M|              3|        Unknown|       Unknown|   $80K - $120K|     Platinum|            40|                       1|                     2|                    3|     34516.0|               1723|        32793.0|               0.628|          13853|            98|              0.782|                 0.05|          38|\n",
      "|709814883|Existing Customer|          45|     F|              2|    High School|        Single| Less than $40K|     Platinum|            36|                       1|                     3|                    1|     15987.0|               2262|        13725.0|               0.714|          15513|           127|              0.649|                0.141|          38|\n",
      "|794494308|Existing Customer|          42|     M|              3|     Uneducated|       Married|        $120K +|     Platinum|            23|                       3|                     4|                    3|     34516.0|               2070|        32446.0|                0.88|          13781|           102|              0.545|                 0.06|          38|\n",
      "|709969758|Existing Customer|          43|     M|              3|       Graduate|        Single|    $60K - $80K|     Platinum|            35|                       4|                     3|                    3|     34516.0|               1774|        32742.0|               0.667|          13966|           115|              0.667|                0.051|          38|\n",
      "|714949758|Existing Customer|          51|     F|              3|       Graduate|        Single|        Unknown|     Platinum|            42|                       3|                     1|                    4|     34516.0|               1913|        32603.0|               0.851|          16712|           123|              0.708|                0.055|          38|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|          38|   20|\n",
      "|          94|  555|\n",
      "|         128| 9436|\n",
      "|         146|  116|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = input.withColumn('partition_id',spark_partition_id())\n",
    "input.show()\n",
    "\n",
    "countByPartition = input.groupBy(input.partition_id).count()\n",
    "countByPartition.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate expriy date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+\n",
      "| id|rechargeDate|validity|\n",
      "+---+------------+--------+\n",
      "|  1|    20200511|      20|\n",
      "|  2|    20200119|      13|\n",
      "|  3|    20200405|     120|\n",
      "+---+------------+--------+\n",
      "\n",
      "StructType([StructField('id', IntegerType(), True), StructField('rechargeDate', IntegerType(), True), StructField('validity', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "exp_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/expiry.csv')\n",
    "\n",
    "exp_df.show()\n",
    "print(exp_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+----------+\n",
      "| id|rechargeDate|validity|      date|\n",
      "+---+------------+--------+----------+\n",
      "|  1|    20200511|      20|2020-05-11|\n",
      "|  2|    20200119|      13|2020-01-19|\n",
      "|  3|    20200405|     120|2020-04-05|\n",
      "+---+------------+--------+----------+\n",
      "\n",
      "+---+------------+--------+----------+----------+\n",
      "| id|rechargeDate|validity|      date|expiryDate|\n",
      "+---+------------+--------+----------+----------+\n",
      "|  1|    20200511|      20|2020-05-11|2020-05-31|\n",
      "|  2|    20200119|      13|2020-01-19|2020-02-01|\n",
      "|  3|    20200405|     120|2020-04-05|2020-08-03|\n",
      "+---+------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,to_date,col,expr\n",
    "\n",
    "exp_df = exp_df.withColumn('date',to_date(col('rechargeDate').cast('string'),\"yyyyMMdd\"))\n",
    "exp_df.show()\n",
    "\n",
    "exp_df = exp_df.withColumn('expiryDate',date_add('date','validity'))\n",
    "exp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 complex dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = true)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+----------------+----+\n",
      "|       Education|Name|\n",
      "+----------------+----+\n",
      "|{28, BCOM, 2013}|Azar|\n",
      "|  {24, BE, 2021}|Amol|\n",
      "+----------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Education', 'Name']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1 = spark.read.json('sample_data/input1.json',multiLine=True)\n",
    "json2 = spark.read.json('sample_data/input2.json',multiLine=True)\n",
    "\n",
    "\n",
    "json1.printSchema()\n",
    "json1.show()\n",
    "json1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Education', StructType([StructField('Age', LongType(), True), StructField('Qualification', StringType(), True), StructField('Year', LongType(), True)]), True), StructField('Name', StringType(), True)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, lit, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def flatten_struct(schema, prefix=\"\"):\n",
    "    result = []\n",
    "    for elem in schema:\n",
    "        if isinstance(elem.dataType,StructType):\n",
    "            result+=flatten_struct(elem.dataType, prefix+elem.name+\".\")\n",
    "        else:\n",
    "            result.append(col(prefix+elem.name).alias(prefix+elem.name))\n",
    "    return result\n",
    "\n",
    "l1 = flatten_struct(json1.schema)\n",
    "l2 = flatten_struct(json2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = []\n",
    "col2 = []\n",
    "for i in l1:\n",
    "    col1.append(str(i).replace(\"AS\",\"'\").split(\"'\")[1].strip())\n",
    "for i in l2:\n",
    "    col2.append(str(i).replace(\"AS\",\"'\").split(\"'\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Education.Age', 'Education.Qualification', 'Education.Year', 'Name']\n",
      "['Education.Qualification', 'Education.Year', 'Name']\n",
      "{'Education.Age'}\n",
      "LongType()\n",
      "['Qualification', 'Year']\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "|{BSC, 2012, null}|Benita|\n",
      "|{MSC, 2022, null}| Bavya|\n",
      "+-----------------+------+\n",
      "\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "|{null, BSC, 2012}|Benita|\n",
      "|{null, MSC, 2022}| Bavya|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(col1)\n",
    "print(col2)\n",
    "diff = set(col1)-set(col2)\n",
    "print(diff)\n",
    "\n",
    "for i in diff:\n",
    "    if('.' in i):\n",
    "        c,cn = i.split(\".\")\n",
    "        s_type = json1.schema[c].dataType[cn].dataType\n",
    "        print(s_type)\n",
    "        s_fields = json2.schema[c].dataType.names\n",
    "        print(s_fields)\n",
    "        inDf = json2.withColumn(c,struct(*([col(c)[record].alias(record) for record in s_fields] + [lit(None).cast(s_type).alias(cn)])))\n",
    "        s_fields = sorted(inDf.schema[c].dataType.names)\n",
    "        inDf.show()\n",
    "        inDf = inDf.withColumn(c,struct(*([col(c)[record].alias(record) for record in s_fields])))\n",
    "        inDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = false)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = true)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "| {28, BCOM, 2013}|  Azar|\n",
      "|   {24, BE, 2021}|  Amol|\n",
      "|{null, BSC, 2012}|Benita|\n",
      "|{null, MSC, 2022}| Bavya|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json1.printSchema()\n",
    "outputDf = json1.union(inDf)\n",
    "outputDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speculative Execution in Spark\n",
    "\n",
    "Speculative execution in Apache Spark is a performance optimization technique that aims to handle slow or straggling tasks during a job execution. In distributed systems like Spark, tasks are run in parallel across multiple workers. However, due to various reasons (e.g., resource contention, network latency, node failures), some tasks may take much longer to complete than others. These slow tasks are known as \"stragglers.\"\n",
    "\n",
    "Speculative execution helps mitigate the impact of these straggler tasks by launching duplicate copies of the slow tasks on other nodes. The first copy that finishes is considered the result, and the others are discarded. This helps reduce the overall job completion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf('spark.speculation',True) # Enables or disables speculative execution (default is false).\n",
    "spark.conf('spark.speculation.interval',200) # The frequency at which Spark checks for straggler tasks.\n",
    "spark.conf('spark.speculation.multiplier',5) # threshold = median_task_duration * spark.speculation.multiplier\n",
    "# Where:\n",
    "\n",
    "# median_task_duration is the time it takes for most tasks to finish in the same stage.\n",
    "\n",
    "# spark.speculation.multiplier is the factor that defines how much longer a task can take before it is considered a straggler.\n",
    "\n",
    "# If you have spark.speculation.multiplier = 1.5 and the median time for tasks in a stage is 10 seconds, the threshold for considering a task as a straggler would be:\n",
    "\n",
    "# threshold = 10 * 1.5 = 15 seconds\n",
    "# In this case, if any task takes longer than 15 seconds to complete, Spark will consider it a candidate for speculative execution and will launch a duplicate copy of the task on another executor\n",
    "\n",
    "spark.conf('spark.speculation.quantile',0.75) # The quantile of tasks that are considered for speculative execution (e.g., if you set this to 0.75, Spark will speculate on the slowest 25% of tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to handle corrupt/bad records\n",
    "\n",
    "Modes in spark.read()\n",
    "1. PERMISSIVE\n",
    "2. FAILFAST\n",
    "3. DROPMALFORMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "| emp_no|emp_name|department|\n",
      "+-------+--------+----------+\n",
      "|      1| Murugan|        IT|\n",
      "|invalid| invalid|      null|\n",
      "|      2|  Kannan|   Finance|\n",
      "|      3|   Mohan|      null|\n",
      "|      4|   Pavan|        HR|\n",
      "+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o659.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 123.0 failed 1 times, most recent failure: Lost task 0.0 in stage 123.0 (TID 432) (localhost executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data/badRecords.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o659.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 123.0 failed 1 times, most recent failure: Lost task 0.0 in stage 123.0 (TID 432) (localhost executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('mode','FAILFAST').csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('mode','DROPMALFORMED').csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     3|   Mohan|      null|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('emp_no',IntegerType()),\n",
    "        StructField('emp_name',StringType(),True),\n",
    "        StructField('department',StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.option('mode','DROPMALFORMED').schema(schema=schema).csv('sample_data/badRecords.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|  null| invalid|      null|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     3|   Mohan|      null|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('emp_no',IntegerType()),\n",
    "        StructField('emp_name',StringType(),True),\n",
    "        StructField('department',StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.schema(schema=schema).csv('sample_data/badRecords.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
