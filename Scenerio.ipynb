{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 dataframes with uneven columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Merge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "input1AddDF = input1DF.withColumn('Gender',lit(None))\n",
    "input1AddDF.show()\n",
    "\n",
    "result = input1AddDF.union(input2DF)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField('Name',StringType(),True), # 3rd option is nullable is true or not\n",
    "    StructField('Age',IntegerType(),True),\n",
    "    StructField('Gender',StringType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).schema(schema).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.option('header',True).csv('sample_data\\input2.csv',schema=schema)\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.union(input2DF)\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.join(input2DF,on=['Name','Age'],how='outer')\n",
    "result.show()\n",
    "\n",
    "result = input1DF.join(input2DF,[input1DF.Name==input2DF.Name,input1DF.Age==input2DF.Age],how='outer').select(input1DF.Name,input1DF.Age,input2DF.Gender)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best approach - Automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "listA = set(input1DF.columns)-set(input2DF.columns)\n",
    "listB = set(input2DF.columns)-set(input1DF.columns)\n",
    "\n",
    "for i in listA:\n",
    "    input2DF = input2DF.withColumn(i,lit(None))\n",
    "\n",
    "for i in listB:\n",
    "    input1DF = input1DF.withColumn(i,lit(None))\n",
    "    \n",
    "resut = input1DF.union(input2DF)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply line break every 5th occurance from | delimited input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |chk                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "\n",
    "input = spark.read.csv('sample_data\\input.txt')\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn(\"chk\",regexp_replace(\"_c0\",\"(.*?\\\\|){4}\",\"$0-\"))\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn('col_explode',explode(split('chk','\\|-')))\n",
    "\n",
    "input.select(input.col_explode).show()\n",
    "\n",
    "result = input.select(input.col_explode)\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|split(col_explode, \\|, -1)|\n",
      "+--------------------------+\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(split('col_explode','\\|')).show()\n",
    "\n",
    "result.rdd.map(lambda i:i).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rdd.map(lambda i:len(i)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = result.rdd.map(lambda i:i[0].split('|'))\n",
    "\n",
    "result = result_rdd.toDF(['Name','Qualification','S.no','Age']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
