{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 dataframes with uneven columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Merge').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "input1AddDF = input1DF.withColumn('Gender',lit(None))\n",
    "input1AddDF.show()\n",
    "\n",
    "result = input1AddDF.union(input2DF)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|  null|\n",
      "|  Arvind| 24|  null|\n",
      "|Rishitha| 24|  null|\n",
      "|  Anusha| 24|  null|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField('Name',StringType(),True), # 3rd option is nullable is true or not\n",
    "    StructField('Age',IntegerType(),True),\n",
    "    StructField('Gender',StringType(),True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "input1DF = spark.read.format('csv').option('header',True).schema(schema).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.option('header',True).csv('sample_data\\input2.csv',schema=schema)\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.union(input2DF)\n",
    "result.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "result = input1DF.join(input2DF,on=['Name','Age'],how='outer')\n",
    "result.show()\n",
    "\n",
    "result = input1DF.join(input2DF,[input1DF.Name==input2DF.Name,input1DF.Age==input2DF.Age],how='outer').select(input1DF.Name,input1DF.Age,input2DF.Gender)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best approach - Automated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "| Monisha| 23|     F|\n",
      "|  Arvind| 24|     M|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n",
      "+--------+---+------+\n",
      "|    Name|Age|Gender|\n",
      "+--------+---+------+\n",
      "|  Anusha| 24|  null|\n",
      "|  Arvind| 24|     M|\n",
      "| Gayatri| 25|  null|\n",
      "| Monisha| 23|     F|\n",
      "|Rishitha| 24|     F|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1DF = spark.read.format('csv').option('header',True).load('sample_data\\input1.csv')\n",
    "input1DF.show()\n",
    "input2DF = spark.read.format('csv').option('header',True).load('sample_data\\input2.csv')\n",
    "input2DF.show()\n",
    "\n",
    "listA = set(input1DF.columns)-set(input2DF.columns)\n",
    "listB = set(input2DF.columns)-set(input1DF.columns)\n",
    "\n",
    "for i in listA:\n",
    "    input2DF = input2DF.withColumn(i,lit(None))\n",
    "\n",
    "for i in listB:\n",
    "    input1DF = input1DF.withColumn(i,lit(None))\n",
    "    \n",
    "resut = input1DF.union(input2DF)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply line break every 5th occurance from | delimited input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                     |chk                                                                                                              |\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|B|BSC|2|27|A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|-A|BE|1|25|-B|BSC|2|27|\n",
      "+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|col_explode|\n",
      "+-----------+\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "|  A|BE|1|25|\n",
      "| B|BSC|2|27|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "\n",
    "input = spark.read.csv('sample_data\\input.txt')\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn(\"chk\",regexp_replace(\"_c0\",\"(.*?\\\\|){4}\",\"$0-\"))\n",
    "\n",
    "input.show(truncate=False)\n",
    "\n",
    "input = input.withColumn('col_explode',explode(split('chk','\\|-')))\n",
    "\n",
    "input.select(input.col_explode).show()\n",
    "\n",
    "result = input.select(input.col_explode)\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|split(col_explode, \\|, -1)|\n",
      "+--------------------------+\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "|            [A, BE, 1, 25]|\n",
      "|           [B, BSC, 2, 27]|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27'),\n",
       " Row(col_explode='A|BE|1|25'),\n",
       " Row(col_explode='B|BSC|2|27')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select(split('col_explode','\\|')).show()\n",
    "\n",
    "result.rdd.map(lambda i:i).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rdd.map(lambda i:len(i)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = result.rdd.map(lambda i:i[0].split('|'))\n",
    "\n",
    "result = result_rdd.toDF(['Name','Qualification','S.no','Age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files recursively under Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                      |\n",
      "+--------+---+--------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv|\n",
      "+--------+---+--------------------------------------------------------------+\n",
      "\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                             |\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Mo      |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Arv     |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Rish    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Anus    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Gaya    |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Name    |Age|filepath                                                             |\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "|Monisha |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Arvind  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Rishitha|24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Anusha  |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Gayatri |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/file1.csv       |\n",
      "|Mo      |23 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Arv     |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Rish    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Anus    |24 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "|Gaya    |25 |file:///c:/Users/varsh/pyspark/sample_data/recursive/level1/file2.csv|\n",
      "+--------+---+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).load('sample_data/recursive').withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "paths = [\n",
    "    'sample_data/recursive',\n",
    "    'sample_data/recursive/level1'\n",
    "]\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).load(paths).withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)\n",
    "\n",
    "df1 = spark.read.format('csv').option('header',True).option('recursiveFileLookup',True).load(paths).withColumn('filepath',input_file_name())\n",
    "\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one returns results faster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df = spark.read.csv()\n",
    "df = df.filter()\n",
    "df = df.sort()\n",
    "df.count()\n",
    "#2\n",
    "df = spark.read.csv()\n",
    "df = df.sort()\n",
    "df = df.filter()\n",
    "df.count()\n",
    "#3\n",
    "df = spark.read.csv()\n",
    "df.sort()\n",
    "df = df.filter()\n",
    "df = df.sort()\n",
    "df.count()\n",
    "#4\n",
    "df = spark.read.csv()\n",
    "df.sort()\n",
    "df = df.sort()\n",
    "df = df.filter()\n",
    "df.count()\n",
    "\n",
    "# Predicate pushdown pushes the filter to the start even if sort is given first\n",
    "\n",
    "# option A performs better, Caching took time, this is not the right case for doing caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferschema Vs Schema definition\n",
    "\n",
    "Always define schema manually since inferschema scans the table twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "|768805383|Existing Customer|          45|     M|              3|    High School|       Married|    $60K - $80K|         Blue|            39|                       5|                     1|                    3|     12691.0|                777|        11914.0|               1.335|           1144|            42|              1.625|                0.061|\n",
      "|818770008|Existing Customer|          49|     F|              5|       Graduate|        Single| Less than $40K|         Blue|            44|                       6|                     1|                    2|      8256.0|                864|         7392.0|               1.541|           1291|            33|              3.714|                0.105|\n",
      "|713982108|Existing Customer|          51|     M|              3|       Graduate|       Married|   $80K - $120K|         Blue|            36|                       4|                     1|                    0|      3418.0|                  0|         3418.0|               2.594|           1887|            20|              2.333|                  0.0|\n",
      "|769911858|Existing Customer|          40|     F|              4|    High School|       Unknown| Less than $40K|         Blue|            34|                       3|                     4|                    1|      3313.0|               2517|          796.0|               1.405|           1171|            20|              2.333|                 0.76|\n",
      "|709106358|Existing Customer|          40|     M|              3|     Uneducated|       Married|    $60K - $80K|         Blue|            21|                       5|                     1|                    0|      4716.0|                  0|         4716.0|               2.175|            816|            28|                2.5|                  0.0|\n",
      "|713061558|Existing Customer|          44|     M|              2|       Graduate|       Married|    $40K - $60K|         Blue|            36|                       3|                     1|                    2|      4010.0|               1247|         2763.0|               1.376|           1088|            24|              0.846|                0.311|\n",
      "|810347208|Existing Customer|          51|     M|              4|        Unknown|       Married|        $120K +|         Gold|            46|                       6|                     1|                    3|     34516.0|               2264|        32252.0|               1.975|           1330|            31|              0.722|                0.066|\n",
      "|818906208|Existing Customer|          32|     M|              0|    High School|       Unknown|    $60K - $80K|       Silver|            27|                       2|                     2|                    2|     29081.0|               1396|        27685.0|               2.204|           1538|            36|              0.714|                0.048|\n",
      "|710930508|Existing Customer|          37|     M|              3|     Uneducated|        Single|    $60K - $80K|         Blue|            36|                       5|                     2|                    0|     22352.0|               2517|        19835.0|               3.355|           1350|            24|              1.182|                0.113|\n",
      "|719661558|Existing Customer|          48|     M|              2|       Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     3|                    3|     11656.0|               1677|         9979.0|               1.524|           1441|            32|              0.882|                0.144|\n",
      "|708790833|Existing Customer|          42|     M|              5|     Uneducated|       Unknown|        $120K +|         Blue|            31|                       5|                     3|                    2|      6748.0|               1467|         5281.0|               0.831|           1201|            42|               0.68|                0.217|\n",
      "|710821833|Existing Customer|          65|     M|              1|        Unknown|       Married|    $40K - $60K|         Blue|            54|                       6|                     2|                    3|      9095.0|               1587|         7508.0|               1.433|           1314|            26|              1.364|                0.174|\n",
      "|710599683|Existing Customer|          56|     M|              1|        College|        Single|   $80K - $120K|         Blue|            36|                       3|                     6|                    0|     11751.0|                  0|        11751.0|               3.397|           1539|            17|               3.25|                  0.0|\n",
      "|816082233|Existing Customer|          35|     M|              3|       Graduate|       Unknown|    $60K - $80K|         Blue|            30|                       5|                     1|                    3|      8547.0|               1666|         6881.0|               1.163|           1311|            33|                2.0|                0.195|\n",
      "|712396908|Existing Customer|          57|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            48|                       5|                     2|                    2|      2436.0|                680|         1756.0|                1.19|           1570|            29|              0.611|                0.279|\n",
      "|714885258|Existing Customer|          44|     M|              4|        Unknown|       Unknown|   $80K - $120K|         Blue|            37|                       5|                     1|                    2|      4234.0|                972|         3262.0|               1.707|           1348|            27|                1.7|                 0.23|\n",
      "|709967358|Existing Customer|          48|     M|              4|  Post-Graduate|        Single|   $80K - $120K|         Blue|            36|                       6|                     2|                    3|     30367.0|               2362|        28005.0|               1.708|           1671|            27|              0.929|                0.078|\n",
      "|753327333|Existing Customer|          41|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            34|                       4|                     4|                    1|     13535.0|               1291|        12244.0|               0.653|           1028|            21|              1.625|                0.095|\n",
      "|806160108|Existing Customer|          61|     M|              1|    High School|       Married|    $40K - $60K|         Blue|            56|                       2|                     2|                    3|      3193.0|               2517|          676.0|               1.831|           1336|            30|              1.143|                0.788|\n",
      "|709327383|Existing Customer|          45|     F|              2|       Graduate|       Married|        Unknown|         Blue|            37|                       6|                     1|                    2|     14470.0|               1157|        13313.0|               0.966|           1207|            21|              0.909|                 0.08|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/BankChurners.csv')\n",
    "\n",
    "bank.show()\n",
    "\n",
    "bank.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = bank.repartition(4)\n",
    "input.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|partition_id|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|708721533|Existing Customer|          56|     F|              1|       Graduate|       Married| Less than $40K|         Blue|            36|                       2|                     1|                    2|      5158.0|               1148|         4010.0|               0.637|          13113|           128|              0.882|                0.223|           0|\n",
      "|720362058|Attrited Customer|          57|     M|              2|     Uneducated|        Single|    $60K - $80K|       Silver|            49|                       4|                     3|                    3|     32182.0|                  0|        32182.0|                0.52|           1937|            46|              0.704|                  0.0|           0|\n",
      "|708360108|Existing Customer|          50|     M|              1|        College|        Single|    $40K - $60K|         Blue|            41|                       3|                     1|                    1|      2800.0|               1338|         1462.0|               0.818|           4869|            90|                0.8|                0.478|           0|\n",
      "|713090658|Existing Customer|          58|     F|              2|     Uneducated|        Single|        Unknown|         Blue|            44|                       1|                     2|                    2|      2599.0|               1407|         1192.0|               0.727|           8061|            98|              0.531|                0.541|           0|\n",
      "|714943233|Existing Customer|          49|     M|              2|      Doctorate|       Married|    $40K - $60K|         Blue|            31|                       2|                     2|                    2|      1915.0|               1545|          370.0|               0.775|           3911|            80|              0.778|                0.807|           0|\n",
      "|717052158|Existing Customer|          39|     M|              3|    High School|        Single|    $60K - $80K|         Blue|            32|                       3|                     2|                    2|     21718.0|               2386|        19332.0|               0.743|           3492|            66|              0.692|                 0.11|           0|\n",
      "|794227308|Existing Customer|          56|     M|              2|        College|       Married|   $80K - $120K|         Blue|            50|                       1|                     1|                    3|     10214.0|               1433|         8781.0|                0.64|           4493|            79|              0.881|                 0.14|           0|\n",
      "|710932683|Existing Customer|          55|     F|              2|    High School|        Single| Less than $40K|         Blue|            36|                       2|                     1|                    2|      6394.0|                  0|         6394.0|               0.656|           3470|            69|              0.917|                  0.0|           0|\n",
      "|712347258|Existing Customer|          43|     F|              2|       Graduate|       Married| Less than $40K|         Blue|            35|                       3|                     3|                    2|      2209.0|                  0|         2209.0|               0.749|           4419|            68|              0.619|                  0.0|           0|\n",
      "|713205108|Existing Customer|          42|     F|              4|       Graduate|       Married|    $40K - $60K|         Blue|            27|                       3|                     1|                    1|      1950.0|               1062|          888.0|                0.48|           3873|            76|              0.854|                0.545|           0|\n",
      "|713808633|Existing Customer|          31|     F|              0|     Uneducated|      Divorced| Less than $40K|       Silver|            23|                       2|                     2|                    1|     10850.0|               1873|         8977.0|               0.995|          13794|           127|              0.789|                0.173|           0|\n",
      "|826168083|Existing Customer|          49|     F|              4|        Unknown|      Divorced| Less than $40K|         Blue|            45|                       3|                     3|                    3|      1757.0|               1307|          450.0|               0.769|           4750|            72|              0.756|                0.744|           0|\n",
      "|721258158|Existing Customer|          65|     F|              1|     Uneducated|        Single|        Unknown|         Blue|            51|                       3|                     3|                    5|     11037.0|               1491|         9546.0|               0.651|           2429|            63|               0.75|                0.135|           0|\n",
      "|713906508|Existing Customer|          42|     F|              3|        Unknown|        Single| Less than $40K|         Blue|            36|                       6|                     3|                    3|      2837.0|                834|         2003.0|               0.427|           3892|            69|              0.408|                0.294|           0|\n",
      "|740784258|Existing Customer|          47|     F|              2|        College|        Single| Less than $40K|         Blue|            40|                       4|                     3|                    3|      1789.0|               1088|          701.0|               0.683|           4191|            90|              0.525|                0.608|           0|\n",
      "|717504258|Existing Customer|          38|     M|              2|        College|       Married|    $40K - $60K|         Blue|            25|                       4|                     1|                    4|      3735.0|               1199|         2536.0|               0.886|           1573|            35|               0.75|                0.321|           0|\n",
      "|717529233|Attrited Customer|          40|     F|              3|       Graduate|        Single| Less than $40K|         Blue|            30|                       4|                     3|                    2|      3319.0|                  0|         3319.0|               0.262|           4840|            44|              0.375|                  0.0|           0|\n",
      "|716421633|Attrited Customer|          43|     F|              3|       Graduate|       Married| Less than $40K|         Blue|            36|                       4|                     2|                    4|      3523.0|                  0|         3523.0|               0.973|           3174|            44|              0.222|                  0.0|           0|\n",
      "|715593783|Existing Customer|          52|     M|              1|        Unknown|       Married|   $80K - $120K|         Blue|            36|                       4|                     2|                    2|     14858.0|               1594|        13264.0|                0.51|           4286|            72|              0.636|                0.107|           0|\n",
      "|717098208|Existing Customer|          48|     M|              3|        Unknown|       Married|   $80K - $120K|         Blue|            41|                       5|                     2|                    2|      5848.0|                950|         4898.0|               0.726|           1961|            51|              0.545|                0.162|           0|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|           0| 2531|\n",
      "|           1| 2532|\n",
      "|           2| 2532|\n",
      "|           3| 2532|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "input = input.withColumn('partition_id',spark_partition_id())\n",
    "input.show()\n",
    "\n",
    "countByPartition = input.groupBy(input.partition_id).count()\n",
    "countByPartition.show()\n",
    "\n",
    "# Data is evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = bank.repartition(200,\"Card_Category\")\n",
    "input.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|CLIENTNUM|   Attrition_Flag|Customer_Age|Gender|Dependent_count|Education_Level|Marital_Status|Income_Category|Card_Category|Months_on_book|Total_Relationship_Count|Months_Inactive_12_mon|Contacts_Count_12_mon|Credit_Limit|Total_Revolving_Bal|Avg_Open_To_Buy|Total_Amt_Chng_Q4_Q1|Total_Trans_Amt|Total_Trans_Ct|Total_Ct_Chng_Q4_Q1|Avg_Utilization_Ratio|partition_id|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "|788386908|Existing Customer|          48|     M|              3|       Graduate|       Unknown|   $80K - $120K|     Platinum|            41|                       3|                     2|                    2|     34516.0|               1531|        32985.0|               0.862|           1156|            29|              1.071|                0.044|          38|\n",
      "|779519058|Existing Customer|          51|     F|              3|  Post-Graduate|      Divorced|        Unknown|     Platinum|            34|                       3|                     1|                    2|     34516.0|               1578|        32938.0|               0.725|           1929|            40|              0.481|                0.046|          38|\n",
      "|708654933|Attrited Customer|          51|     F|              2|      Doctorate|       Married| Less than $40K|     Platinum|            42|                       3|                     2|                    3|     15987.0|                193|        15794.0|               0.435|           2021|            46|              0.394|                0.012|          38|\n",
      "|714754533|Existing Customer|          51|     M|              0|      Doctorate|        Single|        $120K +|     Platinum|            44|                       3|                     2|                    3|     34516.0|               1925|        32591.0|               0.764|           8012|            87|              0.582|                0.056|          38|\n",
      "|714077583|Attrited Customer|          43|     M|              3|  Post-Graduate|       Married|    $40K - $60K|     Platinum|            31|                       2|                     3|                    4|     23981.0|                593|        23388.0|               0.987|           4758|            65|              0.512|                0.025|          38|\n",
      "|709269708|Existing Customer|          45|     M|              4|       Graduate|        Single|        $120K +|     Platinum|            34|                       2|                     2|                    1|     34516.0|               1488|        33028.0|               0.732|           7281|            95|              0.532|                0.043|          38|\n",
      "|719071158|Attrited Customer|          54|     F|              0|       Graduate|        Single|        Unknown|     Platinum|            38|                       2|                     2|                    2|     34516.0|                  0|        34516.0|               0.695|           3901|            54|              0.421|                  0.0|          38|\n",
      "|711364233|Existing Customer|          41|     M|              4|        Unknown|       Married|    $60K - $80K|     Platinum|            26|                       1|                     1|                    2|     34516.0|               1559|        32957.0|               0.759|           8888|           104|              0.763|                0.045|          38|\n",
      "|823848183|Existing Customer|          39|     F|              1|        Unknown|        Single| Less than $40K|     Platinum|            35|                       1|                     2|                    3|     15987.0|               1494|        14493.0|               0.731|           8438|            92|               0.84|                0.093|          38|\n",
      "|714190983|Attrited Customer|          51|     F|              2|       Graduate|        Single|        Unknown|     Platinum|            32|                       2|                     3|                    3|     34516.0|                531|        33985.0|                0.98|           5418|            65|              0.711|                0.015|          38|\n",
      "|770848308|Existing Customer|          45|     M|              2|       Graduate|       Married|    $60K - $80K|     Platinum|            31|                       2|                     2|                    1|     34516.0|               1308|        33208.0|               0.746|           8773|           105|               0.78|                0.038|          38|\n",
      "|713870958|Existing Customer|          56|     F|              3|     Uneducated|        Single|        Unknown|     Platinum|            46|                       2|                     3|                    2|     34516.0|                  0|        34516.0|               0.887|           8416|            93|              0.632|                  0.0|          38|\n",
      "|709319658|Attrited Customer|          48|     F|              4|       Graduate|        Single| Less than $40K|     Platinum|            37|                       5|                     3|                    4|     15987.0|                  0|        15987.0|               0.827|           7681|            71|               0.69|                  0.0|          38|\n",
      "|715017858|Existing Customer|          44|     M|              2|     Uneducated|       Married|        $120K +|     Platinum|            36|                       2|                     1|                    3|     34516.0|               1421|        33095.0|               0.744|          14465|           114|              0.754|                0.041|          38|\n",
      "|771626283|Existing Customer|          54|     M|              2|  Post-Graduate|       Married|    $60K - $80K|     Platinum|            42|                       1|                     3|                    1|     34516.0|               1996|        32520.0|               0.988|          15033|           114|               0.81|                0.058|          38|\n",
      "|710362233|Existing Customer|          48|     M|              3|        Unknown|       Unknown|   $80K - $120K|     Platinum|            40|                       1|                     2|                    3|     34516.0|               1723|        32793.0|               0.628|          13853|            98|              0.782|                 0.05|          38|\n",
      "|709814883|Existing Customer|          45|     F|              2|    High School|        Single| Less than $40K|     Platinum|            36|                       1|                     3|                    1|     15987.0|               2262|        13725.0|               0.714|          15513|           127|              0.649|                0.141|          38|\n",
      "|794494308|Existing Customer|          42|     M|              3|     Uneducated|       Married|        $120K +|     Platinum|            23|                       3|                     4|                    3|     34516.0|               2070|        32446.0|                0.88|          13781|           102|              0.545|                 0.06|          38|\n",
      "|709969758|Existing Customer|          43|     M|              3|       Graduate|        Single|    $60K - $80K|     Platinum|            35|                       4|                     3|                    3|     34516.0|               1774|        32742.0|               0.667|          13966|           115|              0.667|                0.051|          38|\n",
      "|714949758|Existing Customer|          51|     F|              3|       Graduate|        Single|        Unknown|     Platinum|            42|                       3|                     1|                    4|     34516.0|               1913|        32603.0|               0.851|          16712|           123|              0.708|                0.055|          38|\n",
      "+---------+-----------------+------------+------+---------------+---------------+--------------+---------------+-------------+--------------+------------------------+----------------------+---------------------+------------+-------------------+---------------+--------------------+---------------+--------------+-------------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+-----+\n",
      "|partition_id|count|\n",
      "+------------+-----+\n",
      "|          38|   20|\n",
      "|          94|  555|\n",
      "|         128| 9436|\n",
      "|         146|  116|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = input.withColumn('partition_id',spark_partition_id())\n",
    "input.show()\n",
    "\n",
    "countByPartition = input.groupBy(input.partition_id).count()\n",
    "countByPartition.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column methods\n",
    "\n",
    "\n",
    "Method\tReturns\tUse Case\n",
    "df.columns\tList of strings\tQuick column list\n",
    "df.schema.names\tList of strings\tSame as above\n",
    "df.dtypes\tList of (name, type) tuples\tView names + types\n",
    "df.schema\tStructType\tFull schema object\n",
    "df.printSchema()\tPrinted tree\tVisual inspection\n",
    "[f.name for f in df.schema.fields]\tList of names\tCustom logic over schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate expriy date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+\n",
      "| id|rechargeDate|validity|\n",
      "+---+------------+--------+\n",
      "|  1|    20200511|      20|\n",
      "|  2|    20200119|      13|\n",
      "|  3|    20200405|     120|\n",
      "+---+------------+--------+\n",
      "\n",
      "StructType([StructField('id', IntegerType(), True), StructField('rechargeDate', IntegerType(), True), StructField('validity', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "exp_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/expiry.csv')\n",
    "\n",
    "exp_df.show()\n",
    "print(exp_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+----------+\n",
      "| id|rechargeDate|validity|      date|\n",
      "+---+------------+--------+----------+\n",
      "|  1|    20200511|      20|2020-05-11|\n",
      "|  2|    20200119|      13|2020-01-19|\n",
      "|  3|    20200405|     120|2020-04-05|\n",
      "+---+------------+--------+----------+\n",
      "\n",
      "+---+------------+--------+----------+----------+\n",
      "| id|rechargeDate|validity|      date|expiryDate|\n",
      "+---+------------+--------+----------+----------+\n",
      "|  1|    20200511|      20|2020-05-11|2020-05-31|\n",
      "|  2|    20200119|      13|2020-01-19|2020-02-01|\n",
      "|  3|    20200405|     120|2020-04-05|2020-08-03|\n",
      "+---+------------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,to_date,col,expr\n",
    "\n",
    "exp_df = exp_df.withColumn('date',to_date(col('rechargeDate').cast('string'),\"yyyyMMdd\"))\n",
    "exp_df.show()\n",
    "\n",
    "exp_df = exp_df.withColumn('expiryDate',date_add('date','validity'))\n",
    "exp_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge 2 complex dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = true)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+----------------+----+\n",
      "|       Education|Name|\n",
      "+----------------+----+\n",
      "|{28, BCOM, 2013}|Azar|\n",
      "|  {24, BE, 2021}|Amol|\n",
      "+----------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Education', 'Name']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1 = spark.read.json('sample_data/input1.json',multiLine=True)\n",
    "json2 = spark.read.json('sample_data/input2.json',multiLine=True)\n",
    "\n",
    "\n",
    "json1.printSchema()\n",
    "json1.show()\n",
    "json1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Education', StructType([StructField('Age', LongType(), True), StructField('Qualification', StringType(), True), StructField('Year', LongType(), True)]), True), StructField('Name', StringType(), True)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, lit, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def flatten_struct(schema, prefix=\"\"):\n",
    "    result = []\n",
    "    for elem in schema:\n",
    "        if isinstance(elem.dataType,StructType):\n",
    "            result+=flatten_struct(elem.dataType, prefix+elem.name+\".\")\n",
    "        else:\n",
    "            result.append(col(prefix+elem.name).alias(prefix+elem.name))\n",
    "    return result\n",
    "\n",
    "l1 = flatten_struct(json1.schema)\n",
    "l2 = flatten_struct(json2.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = []\n",
    "col2 = []\n",
    "for i in l1:\n",
    "    col1.append(str(i).replace(\"AS\",\"'\").split(\"'\")[1].strip())\n",
    "for i in l2:\n",
    "    col2.append(str(i).replace(\"AS\",\"'\").split(\"'\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Education.Age', 'Education.Qualification', 'Education.Year', 'Name']\n",
      "['Education.Qualification', 'Education.Year', 'Name']\n",
      "{'Education.Age'}\n",
      "LongType()\n",
      "['Qualification', 'Year']\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "|{BSC, 2012, null}|Benita|\n",
      "|{MSC, 2022, null}| Bavya|\n",
      "+-----------------+------+\n",
      "\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "|{null, BSC, 2012}|Benita|\n",
      "|{null, MSC, 2022}| Bavya|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(col1)\n",
    "print(col2)\n",
    "diff = set(col1)-set(col2)\n",
    "print(diff)\n",
    "\n",
    "for i in diff:\n",
    "    if('.' in i):\n",
    "        c,cn = i.split(\".\")\n",
    "        s_type = json1.schema[c].dataType[cn].dataType\n",
    "        print(s_type)\n",
    "        s_fields = json2.schema[c].dataType.names\n",
    "        print(s_fields)\n",
    "        inDf = json2.withColumn(c,struct(*([col(c)[record].alias(record) for record in s_fields] + [lit(None).cast(s_type).alias(cn)])))\n",
    "        s_fields = sorted(inDf.schema[c].dataType.names)\n",
    "        inDf.show()\n",
    "        inDf = inDf.withColumn(c,struct(*([col(c)[record].alias(record) for record in s_fields])))\n",
    "        inDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = false)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Education: struct (nullable = true)\n",
      " |    |-- Age: long (nullable = true)\n",
      " |    |-- Qualification: string (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n",
      "+-----------------+------+\n",
      "|        Education|  Name|\n",
      "+-----------------+------+\n",
      "| {28, BCOM, 2013}|  Azar|\n",
      "|   {24, BE, 2021}|  Amol|\n",
      "|{null, BSC, 2012}|Benita|\n",
      "|{null, MSC, 2022}| Bavya|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json1.printSchema()\n",
    "outputDf = json1.union(inDf)\n",
    "outputDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speculative Execution in Spark\n",
    "\n",
    "Speculative execution in Apache Spark is a performance optimization technique that aims to handle slow or straggling tasks during a job execution. In distributed systems like Spark, tasks are run in parallel across multiple workers. However, due to various reasons (e.g., resource contention, network latency, node failures), some tasks may take much longer to complete than others. These slow tasks are known as \"stragglers.\"\n",
    "\n",
    "Speculative execution helps mitigate the impact of these straggler tasks by launching duplicate copies of the slow tasks on other nodes. The first copy that finishes is considered the result, and the others are discarded. This helps reduce the overall job completion time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf('spark.speculation',True) # Enables or disables speculative execution (default is false).\n",
    "spark.conf('spark.speculation.interval',200) # The frequency at which Spark checks for straggler tasks.\n",
    "spark.conf('spark.speculation.multiplier',5) # threshold = median_task_duration * spark.speculation.multiplier\n",
    "# Where:\n",
    "\n",
    "# median_task_duration is the time it takes for most tasks to finish in the same stage.\n",
    "\n",
    "# spark.speculation.multiplier is the factor that defines how much longer a task can take before it is considered a straggler.\n",
    "\n",
    "# If you have spark.speculation.multiplier = 1.5 and the median time for tasks in a stage is 10 seconds, the threshold for considering a task as a straggler would be:\n",
    "\n",
    "# threshold = 10 * 1.5 = 15 seconds\n",
    "# In this case, if any task takes longer than 15 seconds to complete, Spark will consider it a candidate for speculative execution and will launch a duplicate copy of the task on another executor\n",
    "\n",
    "spark.conf('spark.speculation.quantile',0.75) # The quantile of tasks that are considered for speculative execution (e.g., if you set this to 0.75, Spark will speculate on the slowest 25% of tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to handle corrupt/bad records\n",
    "\n",
    "Modes in spark.read()\n",
    "1. PERMISSIVE\n",
    "2. FAILFAST\n",
    "3. DROPMALFORMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+\n",
      "| emp_no|emp_name|department|\n",
      "+-------+--------+----------+\n",
      "|      1| Murugan|        IT|\n",
      "|invalid| invalid|      null|\n",
      "|      2|  Kannan|   Finance|\n",
      "|      3|   Mohan|      null|\n",
      "|      4|   Pavan|        HR|\n",
      "+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o659.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 123.0 failed 1 times, most recent failure: Lost task 0.0 in stage 123.0 (TID 432) (localhost executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data/badRecords.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o659.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 123.0 failed 1 times, most recent failure: Lost task 0.0 in stage 123.0 (TID 432) (localhost executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\r\n\t... 20 more\r\nCaused by: java.lang.RuntimeException: Malformed CSV record\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('mode','FAILFAST').csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n",
      "root\n",
      " |-- emp_no: string (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('mode','DROPMALFORMED').csv('sample_data/badRecords.csv',header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     3|   Mohan|      null|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('emp_no',IntegerType()),\n",
    "        StructField('emp_name',StringType(),True),\n",
    "        StructField('department',StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.option('mode','DROPMALFORMED').schema(schema=schema).csv('sample_data/badRecords.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_no|emp_name|department|\n",
      "+------+--------+----------+\n",
      "|     1| Murugan|        IT|\n",
      "|  null| invalid|      null|\n",
      "|     2|  Kannan|   Finance|\n",
      "|     3|   Mohan|      null|\n",
      "|     4|   Pavan|        HR|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('emp_no',IntegerType()),\n",
    "        StructField('emp_name',StringType(),True),\n",
    "        StructField('department',StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df = spark.read.schema(schema=schema).csv('sample_data/badRecords.csv',header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to save bad records from a json to a file\n",
    "\n",
    "Option 1: Use mode(\"PERMISSIVE\") and filter out null records to capture bad records.\n",
    "\n",
    "Option 2: Define a custom schema and use exception handling to capture bad records.\n",
    "\n",
    "Option 3: Use RDD transformations like map and filter to catch bad records and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.emp_no.isNull()).write.save('sample_data/bad_records.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       Name|Age|\n",
      "+-----------+---+\n",
      "|Varshini, S| 25|\n",
      "|   Jothi, S| 52|\n",
      "| Sathish, S| 55|\n",
      "| Neelesh, S| 29|\n",
      "+-----------+---+\n",
      "\n",
      "+-----------+---+-------+\n",
      "|       Name|Age|Initial|\n",
      "+-----------+---+-------+\n",
      "|Varshini, S| 25|      S|\n",
      "|   Jothi, S| 52|      S|\n",
      "| Sathish, S| 55|      S|\n",
      "| Neelesh, S| 29|      S|\n",
      "+-----------+---+-------+\n",
      "\n",
      "+--------+---+-------+\n",
      "|    Name|Age|Initial|\n",
      "+--------+---+-------+\n",
      "|Varshini| 25|      S|\n",
      "|   Jothi| 52|      S|\n",
      "| Sathish| 55|      S|\n",
      "| Neelesh| 29|      S|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = spark.read.option('delimiter','~|').csv('sample_data/multidelimiter.csv',header=True)\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('Initial',split('Name',',')[1])\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn('Name',split('Name',',')[0])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map vs FlatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Skewness\n",
    "\n",
    "In spark-3, AQE handles it by itself\n",
    "\n",
    "In spark-2:\n",
    "\n",
    "1. we can try with repartition - Splits the data equally\n",
    "\n",
    "Repartitioning by a more evenly distributed column\n",
    "df = df.repartition(\"region_id\")\n",
    "\n",
    "Or just increasing partitions to spread out skew\n",
    "df = df.repartition(100)\n",
    "\n",
    "Challenges:\n",
    "\n",
    "df.repartition(\"user_id\")\n",
    "If user_id is skewed (like 90% of rows are for user_1), all that data still ends up on one partition — just a different one. So:\n",
    "\n",
    "You've shuffled the data ✔️\n",
    "\n",
    "But the skew is still there ❌\n",
    "\n",
    "✅ Repartitioning just moves the problem to a different executor.\n",
    "\n",
    "Skew Happens During a Join\n",
    "This is a big one.\n",
    "\n",
    "Even if both DataFrames are repartitioned before the join:\n",
    "\n",
    "df1 = df1.repartition(\"user_id\")\n",
    "df2 = df2.repartition(\"user_id\")\n",
    "df1.join(df2, \"user_id\")\n",
    "If user_id = user_1 exists millions of times in both tables, all matching rows go to one task during the shuffle phase. So again:\n",
    "\n",
    "Partitioning was done ✔️\n",
    "\n",
    "But data volume per key caused skew ❌\n",
    "\n",
    "✅ What helps instead: salting, or broadcast joins if one side is small.\n",
    "\n",
    "Not Enough Partitions to Spread the Load\n",
    "If you do:\n",
    "\n",
    "df.repartition(4)\n",
    "...but your data has millions of rows, and one key dominates, you're not helping much.\n",
    "\n",
    "Even worse — repartitioning with too few partitions can make the skew worse, because you’re squeezing more data into fewer slots.\n",
    "\n",
    "✅ Use more partitions, and sometimes pair with salting.\n",
    "\n",
    "4. You Repartition by a Column That's Also Skewed\n",
    "If you repartition by a column that’s also skewed, it just moves the skew around:\n",
    "\n",
    "\n",
    "df.repartition(\"country\")  # But 90% of users are from \"US\"\n",
    "Now all the \"US\" rows go to one partition. Not helpful.\n",
    "\n",
    "✅ Better to repartition on a less skewed column or combine columns to reduce concentration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Salting\n",
    "\n",
    "purchases DataFrame:\n",
    "\n",
    "user_id\tpurchase_amount\n",
    "user_1\t100\n",
    "user_1\t50\n",
    "user_1\t200\n",
    "user_2\t30\n",
    "user_3\t70\n",
    "Notice how user_1 is skewed — lots of rows for just one key.\n",
    "\n",
    "🔧 Step 1: Add a salt\n",
    "We want to break user_1 into multiple salted keys so Spark can parallelize the work.\n",
    "\n",
    "We'll assign a random salt value (between 0 and 1, for this demo — can be more in real world).\n",
    "\n",
    "Let's randomly assign:\n",
    "\n",
    "user_id\tpurchase_amount\tsalt\tuser_id_salted\n",
    "user_1\t100\t0\tuser_1_0\n",
    "user_1\t50\t1\tuser_1_1\n",
    "user_1\t200\t0\tuser_1_0\n",
    "user_2\t30\t0\tuser_2_0\n",
    "user_3\t70\t1\tuser_3_1\n",
    "This spreads the user_1 records across two partitions (user_1_0 and user_1_1).\n",
    "\n",
    "🧮 Step 2: GroupBy on the salted key\n",
    "Now, do the aggregation:\n",
    "\n",
    "grouped = salted_df.groupBy(\"user_id_salted\").agg(sum(\"purchase_amount\").alias(\"partial_sum\"))\n",
    "Result:\n",
    "\n",
    "user_id_salted\tpartial_sum\n",
    "user_1_0\t100 + 200 = 300\n",
    "user_1_1\t50\n",
    "user_2_0\t30\n",
    "user_3_1\t70\n",
    "Nice! Now the work was split between user_1_0 and user_1_1, avoiding a bottleneck.\n",
    "\n",
    "🧹 Step 3: Extract original key and re-group\n",
    "Now we strip off the salt (e.g., split on _), and group by the original user_id:\n",
    "\n",
    "final_df = grouped \\\n",
    "    .withColumn(\"user_id\", split(col(\"user_id_salted\"), \"_\")[0]) \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(sum(\"partial_sum\").alias(\"total_sum\"))\n",
    "Result:\n",
    "\n",
    "user_id\ttotal_sum\n",
    "user_1\t300 + 50 = 350\n",
    "user_2\t30\n",
    "user_3\t70\n",
    "✅ That's your final answer, and you’ve handled the skewed user_1 cleanly and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning\n",
    "\n",
    "spark.read.parquet()\tBased on file count & block size\n",
    "groupBy, join, distinct, etc.\tspark.sql.shuffle.partitions (default: 200)\n",
    "df.rdd.getNumPartitions()\tReturns current partition count\n",
    "df.write.parquet(\"s3://your-bucket/output/\") Will write 200 or df.rdd.getNumPartitions() files (usually .snappy.parquet files) to that path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small files problem\n",
    "\n",
    "In Spark, every partition = one output file (usually .parquet, .json, .csv, etc.).\n",
    "\n",
    "So if your DataFrame has 10,000 tiny partitions (maybe because each partition only has a few rows), then:\n",
    "\n",
    "df.write.parquet(\"s3://bucket/output/\")\n",
    "🚨 Creates 10,000 small files.\n",
    "\n",
    "1. Too Many Files Overwhelm Metadata Systems\n",
    "Systems like HDFS, S3, Hive Metastore hate dealing with thousands of tiny files.\n",
    "\n",
    "Every file = a metadata entry.\n",
    "\n",
    "This bloats memory usage in NameNode (HDFS) or slows down listing/querying (S3, Glue, Hive).\n",
    "\n",
    "2. Bad Performance When Reading\n",
    "When you query later (e.g., using Spark, Athena, Presto), it opens lots of small files.\n",
    "\n",
    "This means lots of I/O operations, network calls, and latency.\n",
    "\n",
    "You’ll see poor parallelism: thousands of files → tasks that finish in milliseconds but cause tons of overhead.\n",
    "\n",
    "3. Wasted Storage and Throughput\n",
    "Each file has some overhead (compression headers, footers, etc.).\n",
    "\n",
    "Small files don't utilize block storage well (esp. in HDFS), wasting space and reducing write efficiency.\n",
    "\n",
    "Best Practices\n",
    "\n",
    "File Format\tTarget File Size\n",
    "Parquet\t128 MB - 1 GB\n",
    "ORC\t256 MB - 1 GB\n",
    "CSV/JSON\tSmaller, but still 10+ MB recommended\n",
    "\n",
    "💡 Large files = fewer tasks, better compression, better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the json content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------------------------+\n",
      "|date      |status |request                               |\n",
      "+----------+-------+--------------------------------------+\n",
      "|2025-04-10|success|\"{\"response\": {\"message_id\": \"msg_001\"|\n",
      "|2025-04-10|error  |\"{\"response\": {\"message_id\": \"msg_002\"|\n",
      "|2025-04-09|success|\"{\"response\": {\"message_id\": \"msg_003\"|\n",
      "|2025-04-09|error  |\"{\"response\": {\"message_id\": \"msg_004\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_005\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_006\"|\n",
      "|2025-04-07|error  |\"{\"response\": {\"message_id\": \"msg_007\"|\n",
      "|2025-04-07|success|\"{\"response\": {\"message_id\": \"msg_008\"|\n",
      "+----------+-------+--------------------------------------+\n",
      "\n",
      "+----------+-------+--------------------------------------+\n",
      "|date      |status |request                               |\n",
      "+----------+-------+--------------------------------------+\n",
      "|2025-04-10|success|\"{\"response\": {\"message_id\": \"msg_001\"|\n",
      "|2025-04-10|error  |\"{\"response\": {\"message_id\": \"msg_002\"|\n",
      "|2025-04-09|success|\"{\"response\": {\"message_id\": \"msg_003\"|\n",
      "|2025-04-09|error  |\"{\"response\": {\"message_id\": \"msg_004\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_005\"|\n",
      "|2025-04-08|success|\"{\"response\": {\"message_id\": \"msg_006\"|\n",
      "|2025-04-07|error  |\"{\"response\": {\"message_id\": \"msg_007\"|\n",
      "|2025-04-07|success|\"{\"response\": {\"message_id\": \"msg_008\"|\n",
      "+----------+-------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/json.csv',header=True)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Value is truncated\n",
    "\n",
    "# These two options — quote and escape — are 🔑 when dealing with complex strings (like embedded JSON) in CSV files.\n",
    "\n",
    "# option(\"quote\", '\"')\n",
    "# This tells Spark:\n",
    "\n",
    "# \"Text between this character is considered as a single field — even if it has commas.\"\n",
    "\n",
    "# option(\"escape\", '\"')\n",
    "# This tells Spark:\n",
    "\n",
    "# \"If this character appears inside a quoted field, treat it as part of the string — not the end of the quote.\"\n",
    "\n",
    "# quote\tDefines boundaries of a field — usually \".\n",
    "# escape\tTells Spark how to handle quotes inside a quoted field.\n",
    "\n",
    "df = spark.read.option('quote','\"').csv('sample_data/json.csv',header=True)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Even this is not working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON spec requires double quotes for strings and keys\n",
    "Valid JSON:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{\"response\": {\"message_id\": \"msg_001\"}}\n",
    "Invalid JSON:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{'response': {'message_id': 'msg_001'}}\n",
    "Tools like from_json() in Spark (and any proper JSON parser) will reject single quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------------+\n",
      "|date      |status |request                                                                             |\n",
      "+----------+-------+------------------------------------------------------------------------------------+\n",
      "|2025-04-10|success|{\"response\": {\"message_id\": \"msg_001\", \"latitude\": 37.7749, \"longitude\": -122.4194}}|\n",
      "|2025-04-10|error  |{\"response\": {\"message_id\": \"msg_002\", \"latitude\": 40.7128, \"longitude\": -74.0060}} |\n",
      "|2025-04-09|success|{\"response\": {\"message_id\": \"msg_003\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|\n",
      "|2025-04-09|error  |{\"response\": {\"message_id\": \"msg_004\", \"latitude\": 51.5074, \"longitude\": -0.1278}}  |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_005\", \"latitude\": 48.8566, \"longitude\": 2.3522}}   |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_006\", \"latitude\": 52.5200, \"longitude\": 13.4050}}  |\n",
      "|2025-04-07|error  |{\"response\": {\"message_id\": \"msg_007\", \"latitude\": 35.6895, \"longitude\": 139.6917}} |\n",
      "|2025-04-07|success|{\"response\": {\"message_id\": \"msg_008\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|\n",
      "+----------+-------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option('quote','\\'').csv('sample_data/json.csv',header=True)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method-1: Using json_tuple\n",
    "\n",
    "We should know the fields to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+\n",
      "|      date| status|             request|                  c0|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "|2025-04-10|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-10|  error|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-09|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-09|  error|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-07|  error|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "|2025-04-07|success|{\"response\": {\"me...|{\"message_id\":\"ms...|\n",
      "+----------+-------+--------------------+--------------------+\n",
      "\n",
      "+----------+-------+--------------------+\n",
      "|      date| status|                  c0|\n",
      "+----------+-------+--------------------+\n",
      "|2025-04-10|success|{\"message_id\":\"ms...|\n",
      "|2025-04-10|  error|{\"message_id\":\"ms...|\n",
      "|2025-04-09|success|{\"message_id\":\"ms...|\n",
      "|2025-04-09|  error|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|\n",
      "|2025-04-07|  error|{\"message_id\":\"ms...|\n",
      "|2025-04-07|success|{\"message_id\":\"ms...|\n",
      "+----------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "\n",
    "df.select('*',json_tuple('request','response')).show()\n",
    "\n",
    "df1 = df.select('*',json_tuple('request','response')).drop('request')\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+----------+--------+---------+\n",
      "|      date| status|                  c0|message_id|latitude|longitude|\n",
      "+----------+-------+--------------------+----------+--------+---------+\n",
      "|2025-04-10|success|{\"message_id\":\"ms...|   msg_001| 37.7749|-122.4194|\n",
      "|2025-04-10|  error|{\"message_id\":\"ms...|   msg_002| 40.7128|  -74.006|\n",
      "|2025-04-09|success|{\"message_id\":\"ms...|   msg_003| 34.0522|-118.2437|\n",
      "|2025-04-09|  error|{\"message_id\":\"ms...|   msg_004| 51.5074|  -0.1278|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|   msg_005| 48.8566|   2.3522|\n",
      "|2025-04-08|success|{\"message_id\":\"ms...|   msg_006|   52.52|   13.405|\n",
      "|2025-04-07|  error|{\"message_id\":\"ms...|   msg_007| 35.6895| 139.6917|\n",
      "|2025-04-07|success|{\"message_id\":\"ms...|   msg_008| 34.0522|-118.2437|\n",
      "+----------+-------+--------------------+----------+--------+---------+\n",
      "\n",
      "+----------+-------+----------+--------+---------+\n",
      "|      date| status|message_id|latitude|longitude|\n",
      "+----------+-------+----------+--------+---------+\n",
      "|2025-04-10|success|   msg_001| 37.7749|-122.4194|\n",
      "|2025-04-10|  error|   msg_002| 40.7128|  -74.006|\n",
      "|2025-04-09|success|   msg_003| 34.0522|-118.2437|\n",
      "|2025-04-09|  error|   msg_004| 51.5074|  -0.1278|\n",
      "|2025-04-08|success|   msg_005| 48.8566|   2.3522|\n",
      "|2025-04-08|success|   msg_006|   52.52|   13.405|\n",
      "|2025-04-07|  error|   msg_007| 35.6895| 139.6917|\n",
      "|2025-04-07|success|   msg_008| 34.0522|-118.2437|\n",
      "+----------+-------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('*',json_tuple('c0','message_id','latitude','longitude').alias('message_id','latitude','longitude')).show()\n",
    "\n",
    "df2 = df1.select('*',json_tuple('c0','message_id','latitude','longitude').alias('message_id','latitude','longitude')).drop('c0')\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method-2: Using from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|date      |status |request                                                                             |request_parsed                 |\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|2025-04-10|success|{\"response\": {\"message_id\": \"msg_001\", \"latitude\": 37.7749, \"longitude\": -122.4194}}|{{msg_001, 37.7749, -122.4194}}|\n",
      "|2025-04-10|error  |{\"response\": {\"message_id\": \"msg_002\", \"latitude\": 40.7128, \"longitude\": -74.0060}} |{{msg_002, 40.7128, -74.006}}  |\n",
      "|2025-04-09|success|{\"response\": {\"message_id\": \"msg_003\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{msg_003, 34.0522, -118.2437}}|\n",
      "|2025-04-09|error  |{\"response\": {\"message_id\": \"msg_004\", \"latitude\": 51.5074, \"longitude\": -0.1278}}  |{{msg_004, 51.5074, -0.1278}}  |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_005\", \"latitude\": 48.8566, \"longitude\": 2.3522}}   |{{msg_005, 48.8566, 2.3522}}   |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_006\", \"latitude\": 52.5200, \"longitude\": 13.4050}}  |{{msg_006, 52.52, 13.405}}     |\n",
      "|2025-04-07|error  |{\"response\": {\"message_id\": \"msg_007\", \"latitude\": 35.6895, \"longitude\": 139.6917}} |{{msg_007, 35.6895, 139.6917}} |\n",
      "|2025-04-07|success|{\"response\": {\"message_id\": \"msg_008\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{msg_008, 34.0522, -118.2437}}|\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "\n",
      "+----------+-------+----------+--------+---------+\n",
      "|date      |status |message_id|latitude|longitude|\n",
      "+----------+-------+----------+--------+---------+\n",
      "|2025-04-10|success|msg_001   |37.7749 |-122.4194|\n",
      "|2025-04-10|error  |msg_002   |40.7128 |-74.006  |\n",
      "|2025-04-09|success|msg_003   |34.0522 |-118.2437|\n",
      "|2025-04-09|error  |msg_004   |51.5074 |-0.1278  |\n",
      "|2025-04-08|success|msg_005   |48.8566 |2.3522   |\n",
      "|2025-04-08|success|msg_006   |52.52   |13.405   |\n",
      "|2025-04-07|error  |msg_007   |35.6895 |139.6917 |\n",
      "|2025-04-07|success|msg_008   |34.0522 |-118.2437|\n",
      "+----------+-------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json,col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"response\", StructType([\n",
    "        StructField(\"message_id\", StringType()),\n",
    "        StructField(\"latitude\", DoubleType()),\n",
    "        StructField(\"longitude\", DoubleType())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "df_parsed = df.withColumn(\"request_parsed\", from_json(col(\"request\"), json_schema))\n",
    "df_parsed.show(truncate=False)\n",
    "\n",
    "df_flat = df_parsed.select(\n",
    "    \"date\",\n",
    "    \"status\",\n",
    "    col(\"request_parsed.response.message_id\").alias(\"message_id\"),\n",
    "    col(\"request_parsed.response.latitude\").alias(\"latitude\"),\n",
    "    col(\"request_parsed.response.longitude\").alias(\"longitude\")\n",
    ")\n",
    "\n",
    "df_flat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above case we are manually giving the schema, but we can also get it from the data and pass it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('response', StructType([StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True), StructField('message_id', StringType(), True)]), True)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col('request').alias('jsonCol')).rdd.map(lambda x:x.jsonCol).collect()\n",
    "\n",
    "df_schema = spark.read.json(df.select(col('request').alias('jsonCol')).rdd.map(lambda x: x.jsonCol)).schema\n",
    "\n",
    "df_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|date      |status |request                                                                             |request_parsed                 |\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "|2025-04-10|success|{\"response\": {\"message_id\": \"msg_001\", \"latitude\": 37.7749, \"longitude\": -122.4194}}|{{37.7749, -122.4194, msg_001}}|\n",
      "|2025-04-10|error  |{\"response\": {\"message_id\": \"msg_002\", \"latitude\": 40.7128, \"longitude\": -74.0060}} |{{40.7128, -74.006, msg_002}}  |\n",
      "|2025-04-09|success|{\"response\": {\"message_id\": \"msg_003\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{34.0522, -118.2437, msg_003}}|\n",
      "|2025-04-09|error  |{\"response\": {\"message_id\": \"msg_004\", \"latitude\": 51.5074, \"longitude\": -0.1278}}  |{{51.5074, -0.1278, msg_004}}  |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_005\", \"latitude\": 48.8566, \"longitude\": 2.3522}}   |{{48.8566, 2.3522, msg_005}}   |\n",
      "|2025-04-08|success|{\"response\": {\"message_id\": \"msg_006\", \"latitude\": 52.5200, \"longitude\": 13.4050}}  |{{52.52, 13.405, msg_006}}     |\n",
      "|2025-04-07|error  |{\"response\": {\"message_id\": \"msg_007\", \"latitude\": 35.6895, \"longitude\": 139.6917}} |{{35.6895, 139.6917, msg_007}} |\n",
      "|2025-04-07|success|{\"response\": {\"message_id\": \"msg_008\", \"latitude\": 34.0522, \"longitude\": -118.2437}}|{{34.0522, -118.2437, msg_008}}|\n",
      "+----------+-------+------------------------------------------------------------------------------------+-------------------------------+\n",
      "\n",
      "+----------+-------+----------+--------+---------+\n",
      "|date      |status |message_id|latitude|longitude|\n",
      "+----------+-------+----------+--------+---------+\n",
      "|2025-04-10|success|msg_001   |37.7749 |-122.4194|\n",
      "|2025-04-10|error  |msg_002   |40.7128 |-74.006  |\n",
      "|2025-04-09|success|msg_003   |34.0522 |-118.2437|\n",
      "|2025-04-09|error  |msg_004   |51.5074 |-0.1278  |\n",
      "|2025-04-08|success|msg_005   |48.8566 |2.3522   |\n",
      "|2025-04-08|success|msg_006   |52.52   |13.405   |\n",
      "|2025-04-07|error  |msg_007   |35.6895 |139.6917 |\n",
      "|2025-04-07|success|msg_008   |34.0522 |-118.2437|\n",
      "+----------+-------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parsed = df.withColumn(\"request_parsed\", from_json(col(\"request\"), df_schema))\n",
    "df_parsed.show(truncate=False)\n",
    "\n",
    "df_flat = df_parsed.select(\n",
    "    \"date\",\n",
    "    \"status\",\n",
    "    col(\"request_parsed.response.message_id\").alias(\"message_id\"),\n",
    "    col(\"request_parsed.response.latitude\").alias(\"latitude\"),\n",
    "    col(\"request_parsed.response.longitude\").alias(\"longitude\")\n",
    ")\n",
    "\n",
    "df_flat.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      " |-- request_parsed: struct (nullable = true)\n",
      " |    |-- response: struct (nullable = true)\n",
      " |    |    |-- latitude: double (nullable = true)\n",
      " |    |    |-- longitude: double (nullable = true)\n",
      " |    |    |-- message_id: string (nullable = true)\n",
      "\n",
      "+----------+-------+--------------------+--------------------+--------+---------+----------+\n",
      "|      date| status|             request|      request_parsed|latitude|longitude|message_id|\n",
      "+----------+-------+--------------------+--------------------+--------+---------+----------+\n",
      "|2025-04-10|success|{\"response\": {\"me...|{{37.7749, -122.4...| 37.7749|-122.4194|   msg_001|\n",
      "|2025-04-10|  error|{\"response\": {\"me...|{{40.7128, -74.00...| 40.7128|  -74.006|   msg_002|\n",
      "|2025-04-09|success|{\"response\": {\"me...|{{34.0522, -118.2...| 34.0522|-118.2437|   msg_003|\n",
      "|2025-04-09|  error|{\"response\": {\"me...|{{51.5074, -0.127...| 51.5074|  -0.1278|   msg_004|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{{48.8566, 2.3522...| 48.8566|   2.3522|   msg_005|\n",
      "|2025-04-08|success|{\"response\": {\"me...|{{52.52, 13.405, ...|   52.52|   13.405|   msg_006|\n",
      "|2025-04-07|  error|{\"response\": {\"me...|{{35.6895, 139.69...| 35.6895| 139.6917|   msg_007|\n",
      "|2025-04-07|success|{\"response\": {\"me...|{{34.0522, -118.2...| 34.0522|-118.2437|   msg_008|\n",
      "+----------+-------+--------------------+--------------------+--------+---------+----------+\n",
      "\n",
      "+----------+-------+--------+---------+----------+\n",
      "|      date| status|latitude|longitude|message_id|\n",
      "+----------+-------+--------+---------+----------+\n",
      "|2025-04-10|success| 37.7749|-122.4194|   msg_001|\n",
      "|2025-04-10|  error| 40.7128|  -74.006|   msg_002|\n",
      "|2025-04-09|success| 34.0522|-118.2437|   msg_003|\n",
      "|2025-04-09|  error| 51.5074|  -0.1278|   msg_004|\n",
      "|2025-04-08|success| 48.8566|   2.3522|   msg_005|\n",
      "|2025-04-08|success|   52.52|   13.405|   msg_006|\n",
      "|2025-04-07|  error| 35.6895| 139.6917|   msg_007|\n",
      "|2025-04-07|success| 34.0522|-118.2437|   msg_008|\n",
      "+----------+-------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parsed.printSchema()\n",
    "json_col = df_parsed.schema['request_parsed'].dataType.names[0]\n",
    "check = \"request_parsed.\"+json_col+\".*\"\n",
    "check\n",
    "df_parsed.select('*',col(check)).show()\n",
    "df_parsed.select('*',col(check)).drop('request').drop('request_parsed').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common records in file1 and file2\n",
    "\n",
    "We can do this in many ways:\n",
    "- set operations: union, unionAll, distinct, intersect, intersectAll, subtract, exceptAll\n",
    "- join operations: leftouter, rightouter, leftsemi, leftanti, cartesian\n",
    "\n",
    "set operations can be only used if both the tables has same no of columns\n",
    "\n",
    "Both ultimately result in same time and operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|rollno|          name|    mobile|\n",
      "+------+--------------+----------+\n",
      "|   101|      John Doe|9876543210|\n",
      "|   102|    Jane Smith|9123456789|\n",
      "|   103|  Robert Brown|9988776655|\n",
      "|   104|   Alice Green|9012345678|\n",
      "|   105|  Mike Johnson|9870011223|\n",
      "|   106|  Linda Taylor|9345678901|\n",
      "|   107|  David Wilson|9001122334|\n",
      "|   108|    Emma Davis|9212345678|\n",
      "|   109|  Chris Martin|9812345678|\n",
      "|   110|    Sophia Lee|8899001122|\n",
      "|   111|  Daniel Lewis|9700112233|\n",
      "|   112|     Mia Clark|9456789012|\n",
      "|   113|   Noah Walker|9332211455|\n",
      "|   114|   Olivia Hall|9543217890|\n",
      "|   115|    Liam Young|9877003456|\n",
      "|   116|      Ava King|9445566778|\n",
      "|   117|  Ethan Wright|9789012345|\n",
      "|   118|Isabella Scott|9311234567|\n",
      "|   119|   Logan Adams|9654321098|\n",
      "|   120|   Emily Baker|9876001122|\n",
      "+------+--------------+----------+\n",
      "\n",
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   105|Mike Johnson|9870011223|\n",
      "|   106|Linda Taylor|9345678901|\n",
      "|   107|David Wilson|9001122334|\n",
      "|   108|  Emma Davis|9212345678|\n",
      "|   109|Chris Martin|9812345678|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA = spark.read.option('header',True).option('inferSchema',True).csv('sample_data/studentA.csv')\n",
    "studentB = spark.read.option('header',True).option('inferSchema',True).csv('sample_data/studentB.csv')\n",
    "studentA.show()\n",
    "studentB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   105|Mike Johnson|9870011223|\n",
      "|   106|Linda Taylor|9345678901|\n",
      "|   107|David Wilson|9001122334|\n",
      "|   108|  Emma Davis|9212345678|\n",
      "|   109|Chris Martin|9812345678|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.join(studentB,studentA.rollno == studentB.rollno,'inner').select(studentA.rollno,studentA.name,studentA.mobile).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using set operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   109|Chris Martin|9812345678|\n",
      "|   101|    John Doe|9876543210|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   106|Linda Taylor|9345678901|\n",
      "|   107|David Wilson|9001122334|\n",
      "|   105|Mike Johnson|9870011223|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   108|  Emma Davis|9212345678|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.intersectAll(studentB).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records present in one file but not in another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|rollno|          name|    mobile|\n",
      "+------+--------------+----------+\n",
      "|   110|    Sophia Lee|8899001122|\n",
      "|   111|  Daniel Lewis|9700112233|\n",
      "|   112|     Mia Clark|9456789012|\n",
      "|   113|   Noah Walker|9332211455|\n",
      "|   114|   Olivia Hall|9543217890|\n",
      "|   115|    Liam Young|9877003456|\n",
      "|   116|      Ava King|9445566778|\n",
      "|   117|  Ethan Wright|9789012345|\n",
      "|   118|Isabella Scott|9311234567|\n",
      "|   119|   Logan Adams|9654321098|\n",
      "|   120|   Emily Baker|9876001122|\n",
      "+------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.join(studentB,studentA.rollno == studentB.rollno,'leftanti').select(studentA.rollno,studentA.name,studentA.mobile).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+\n",
      "|rollno|          name|    mobile|\n",
      "+------+--------------+----------+\n",
      "|   116|      Ava King|9445566778|\n",
      "|   112|     Mia Clark|9456789012|\n",
      "|   114|   Olivia Hall|9543217890|\n",
      "|   118|Isabella Scott|9311234567|\n",
      "|   111|  Daniel Lewis|9700112233|\n",
      "|   117|  Ethan Wright|9789012345|\n",
      "|   110|    Sophia Lee|8899001122|\n",
      "|   120|   Emily Baker|9876001122|\n",
      "|   113|   Noah Walker|9332211455|\n",
      "|   115|    Liam Young|9877003456|\n",
      "|   119|   Logan Adams|9654321098|\n",
      "+------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studentA.exceptAll(studentB).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot and combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+---------+-------+-------+-----+\n",
      "|rollno|math|physics|chemistry|biology|english|total|\n",
      "+------+----+-------+---------+-------+-------+-----+\n",
      "|   101|  78|     82|       76|     85|     90|  411|\n",
      "|   102|  88|     79|       84|     81|     86|  418|\n",
      "|   103|  65|     70|       72|     68|     74|  349|\n",
      "|   104|  92|     88|       91|     90|     89|  450|\n",
      "|   105|  55|     60|       58|     62|     59|  294|\n",
      "|   106|  73|     75|       78|     74|     76|  376|\n",
      "|   107|  80|     83|       79|     81|     82|  405|\n",
      "|   108|  90|     88|       85|     89|     87|  439|\n",
      "|   109|  66|     68|       64|     70|     72|  340|\n",
      "|   110|  85|     87|       84|     86|     88|  430|\n",
      "|   111|  77|     74|       79|     76|     78|  384|\n",
      "|   112|  69|     71|       68|     70|     72|  350|\n",
      "|   113|  94|     95|       93|     96|     92|  470|\n",
      "|   114|  58|     62|       60|     59|     61|  300|\n",
      "|   115|  87|     89|       85|     88|     90|  439|\n",
      "|   116|  72|     74|       70|     73|     75|  364|\n",
      "|   117|  60|     65|       62|     64|     66|  317|\n",
      "|   118|  83|     81|       80|     82|     85|  411|\n",
      "|   119|  70|     68|       72|     71|     69|  350|\n",
      "|   120|  91|     90|       89|     88|     87|  445|\n",
      "+------+----+-------+---------+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "marks = spark.read.csv('sample_data/marks.csv',header=True,inferSchema=True)\n",
    "marks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:1 - Using create_map and explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'math'>, Column<'math'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>, Column<'biology'>, Column<'biology'>]\n",
      "[Column<'math'>, Column<'math'>, Column<'physics'>, Column<'physics'>, Column<'chemistry'>, Column<'chemistry'>, Column<'biology'>, Column<'biology'>, Column<'english'>, Column<'english'>]\n",
      "+------+---------+-----+\n",
      "|rollno|  subject|marks|\n",
      "+------+---------+-----+\n",
      "|   101|     math|   78|\n",
      "|   101|  physics|   82|\n",
      "|   101|chemistry|   76|\n",
      "|   101|  biology|   85|\n",
      "|   101|  english|   90|\n",
      "|   102|     math|   88|\n",
      "|   102|  physics|   79|\n",
      "|   102|chemistry|   84|\n",
      "|   102|  biology|   81|\n",
      "|   102|  english|   86|\n",
      "|   103|     math|   65|\n",
      "|   103|  physics|   70|\n",
      "|   103|chemistry|   72|\n",
      "|   103|  biology|   68|\n",
      "|   103|  english|   74|\n",
      "|   104|     math|   92|\n",
      "|   104|  physics|   88|\n",
      "|   104|chemistry|   91|\n",
      "|   104|  biology|   90|\n",
      "|   104|  english|   89|\n",
      "+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map,lit,col,explode\n",
    "\n",
    "subject_cols = [\"math\", \"physics\", \"chemistry\", \"biology\", \"english\"]\n",
    "\n",
    "# Build map of subject -> marks\n",
    "pairs = []\n",
    "for sub in subject_cols:\n",
    "    pairs.extend([lit(sub), col(sub)])\n",
    "    print(pairs)\n",
    "\n",
    "df_map = marks.select(\"rollno\", explode(create_map(*pairs)).alias(\"subject\", \"marks\"))\n",
    "df_map.show()\n",
    "\n",
    "# 1. 📦 create_map() – Build a Map from Column Pairs\n",
    "\n",
    "# from pyspark.sql.functions import create_map, lit, col\n",
    "# Let’s say you have:\n",
    "\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     (101, 78, 82, 90)\n",
    "# ], [\"rollno\", \"math\", \"physics\", \"english\"])\n",
    "# To build a map like this:\n",
    "\n",
    "\n",
    "# {\n",
    "#   \"math\": 78,\n",
    "#   \"physics\": 82,\n",
    "#   \"english\": 90\n",
    "# }\n",
    "# You use create_map():\n",
    "\n",
    "\n",
    "# create_map(\n",
    "#     lit(\"math\"), col(\"math\"),\n",
    "#     lit(\"physics\"), col(\"physics\"),\n",
    "#     lit(\"english\"), col(\"english\")\n",
    "# )\n",
    "# The lit() gives you the key, the col() gives you the value.\n",
    "\n",
    "# 2. 💥 explode() – Turn Map Entries into Rows\n",
    "# Now that we have a column that’s a map, explode() will take each key-value pair in that map and create a new row.\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# df.select(\n",
    "#     \"rollno\",\n",
    "#     explode(\n",
    "#         create_map(\n",
    "#             lit(\"math\"), col(\"math\"),\n",
    "#             lit(\"physics\"), col(\"physics\"),\n",
    "#             lit(\"english\"), col(\"english\")\n",
    "#         )\n",
    "#     ).alias(\"subject\", \"marks\")\n",
    "# )\n",
    "# 🧾 Output:\n",
    "# text\n",
    "# Copy\n",
    "# Edit\n",
    "# +-------+--------+-----+\n",
    "# |rollno |subject |marks|\n",
    "# +-------+--------+-----+\n",
    "# |101    |math    |78   |\n",
    "# |101    |physics |82   |\n",
    "# |101    |english |90   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'math', math, 'physics', physics, 'chemistry', chemistry, 'biology', biology, 'english', english\n",
      "+------+---------+-----+\n",
      "|rollno|  subject|marks|\n",
      "+------+---------+-----+\n",
      "|   101|     math|   78|\n",
      "|   101|  physics|   82|\n",
      "|   101|chemistry|   76|\n",
      "|   101|  biology|   85|\n",
      "|   101|  english|   90|\n",
      "|   102|     math|   88|\n",
      "|   102|  physics|   79|\n",
      "|   102|chemistry|   84|\n",
      "|   102|  biology|   81|\n",
      "|   102|  english|   86|\n",
      "|   103|     math|   65|\n",
      "|   103|  physics|   70|\n",
      "|   103|chemistry|   72|\n",
      "|   103|  biology|   68|\n",
      "|   103|  english|   74|\n",
      "|   104|     math|   92|\n",
      "|   104|  physics|   88|\n",
      "|   104|chemistry|   91|\n",
      "|   104|  biology|   90|\n",
      "|   104|  english|   89|\n",
      "+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subject_list = [\"math\", \"physics\", \"chemistry\", \"biology\", \"english\"]\n",
    "num_subjects = len(subject_list)\n",
    "\n",
    "stack_expr = \", \".join([f\"'{sub}', {sub}\" for sub in subject_list])\n",
    "print(stack_expr)\n",
    "\n",
    "df_unpivoted = marks.selectExpr(\"rollno\", f\"stack({num_subjects}, {stack_expr}) as (subject, marks)\")\n",
    "df_unpivoted.show()\n",
    "\n",
    "# The stack(n, col1, col2, ..., coln) function is used to unpivot columns — it turns columns into rows.\n",
    "\n",
    "# Think of it as:\n",
    "\n",
    "# \"For each row in the original DataFrame, stack will output n new rows, using pairs of values I give it.\"\n",
    "\n",
    "# 🎯 Let's say you have this data:\n",
    "# text\n",
    "# Copy\n",
    "# Edit\n",
    "# +-------+-----+--------+----------+\n",
    "# |rollno |math |physics |english   |\n",
    "# +-------+-----+--------+----------+\n",
    "# |101    | 78  | 82     | 90       |\n",
    "# You want to convert it to:\n",
    "\n",
    "# text\n",
    "# Copy\n",
    "# Edit\n",
    "# +-------+--------+-------+\n",
    "# |rollno |subject |marks  |\n",
    "# +-------+--------+-------+\n",
    "# |101    |math    |78     |\n",
    "# |101    |physics |82     |\n",
    "# |101    |english |90     |\n",
    "# 💡 Step-by-step breakdown of stack\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# df.selectExpr(\"rollno\", \"stack(3, 'math', math, 'physics', physics, 'english', english) as (subject, marks)\")\n",
    "# 🧩 Here's what it's doing:\n",
    "# stack(3, ...):\n",
    "# → Create 3 rows per original row (one per subject)\n",
    "\n",
    "# The arguments come in pairs: → First is a literal (the subject name)\n",
    "# → Second is the column (marks value)\n",
    "\n",
    "# So this:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# stack(3,\n",
    "#   'math', math,\n",
    "#   'physics', physics,\n",
    "#   'english', english\n",
    "# )\n",
    "# Is telling Spark:\n",
    "\n",
    "# \"For each row, create 3 new rows like:\n",
    "\n",
    "# ('math', value of math)\n",
    "\n",
    "# ('physics', value of physics)\n",
    "\n",
    "# ('english', value of english)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again pivot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+-------+----+-------+-----+\n",
      "|rollno|biology|chemistry|english|math|physics|total|\n",
      "+------+-------+---------+-------+----+-------+-----+\n",
      "|   108|     89|       85|     87|  90|     88|  439|\n",
      "|   115|     88|       85|     90|  87|     89|  439|\n",
      "|   101|     85|       76|     90|  78|     82|  411|\n",
      "|   103|     68|       72|     74|  65|     70|  349|\n",
      "|   111|     76|       79|     78|  77|     74|  384|\n",
      "|   120|     88|       89|     87|  91|     90|  445|\n",
      "|   117|     64|       62|     66|  60|     65|  317|\n",
      "|   112|     70|       68|     72|  69|     71|  350|\n",
      "|   107|     81|       79|     82|  80|     83|  405|\n",
      "|   114|     59|       60|     61|  58|     62|  300|\n",
      "|   102|     81|       84|     86|  88|     79|  418|\n",
      "|   113|     96|       93|     92|  94|     95|  470|\n",
      "|   109|     70|       64|     72|  66|     68|  340|\n",
      "|   105|     62|       58|     59|  55|     60|  294|\n",
      "|   110|     86|       84|     88|  85|     87|  430|\n",
      "|   106|     74|       78|     76|  73|     75|  376|\n",
      "|   116|     73|       70|     75|  72|     74|  364|\n",
      "|   119|     71|       72|     69|  70|     68|  350|\n",
      "|   118|     82|       80|     85|  83|     81|  411|\n",
      "|   104|     90|       91|     89|  92|     88|  450|\n",
      "+------+-------+---------+-------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "pivotBack = df_unpivoted.groupBy('rollno').pivot('subject').max('marks')\n",
    "\n",
    "df_with_total = pivotBack.withColumn(\n",
    "    \"total\",\n",
    "    coalesce(col(\"math\"), lit(0)) +\n",
    "    coalesce(col(\"physics\"), lit(0)) +\n",
    "    coalesce(col(\"chemistry\"), lit(0)) +\n",
    "    coalesce(col(\"english\"), lit(0)) +\n",
    "    coalesce(col(\"biology\"), lit(0))\n",
    ")\n",
    "\n",
    "df_with_total.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "+------+------------+----------+\n",
      "\n",
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "+------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('sample_data/duplicates.csv',header=True,inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df.groupBy('rollno','name','mobile').count().where(\"count > 1\").drop('count').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+----+\n",
      "|rollno|        name|    mobile|rank|\n",
      "+------+------------+----------+----+\n",
      "|   104| Alice Green|9012345678|   2|\n",
      "|   102|  Jane Smith|9123456789|   2|\n",
      "|   101|    John Doe|9876543210|   2|\n",
      "|   103|Robert Brown|9988776655|   2|\n",
      "+------+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "win = Window.partitionBy('name').orderBy(col('name'))\n",
    "\n",
    "df.withColumn('rank',row_number().over(win)).filter('rank == 2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Array to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------------+\n",
      "| Name|Age|      Education|\n",
      "+-----+---+---------------+\n",
      "| Azar| 24|     MBA,BE,HSC|\n",
      "| Babu| 28|           null|\n",
      "| Mani| 22|MBA,BSE,Diploma|\n",
      "|Mohan| 29|   MBA,BArch,SC|\n",
      "+-----+---+---------------+\n",
      "\n",
      "+-----+---+---------+-----+\n",
      "| Name|Age|Education|Index|\n",
      "+-----+---+---------+-----+\n",
      "| Azar| 24|       BE|    0|\n",
      "| Azar| 24|      HSC|    1|\n",
      "| Azar| 24|      MBA|    2|\n",
      "| Babu| 28|     None|    0|\n",
      "| Mani| 22|      BSE|    0|\n",
      "| Mani| 22|  Diploma|    1|\n",
      "| Mani| 22|      MBA|    2|\n",
      "|Mohan| 29|    BArch|    0|\n",
      "|Mohan| 29|      MBA|    1|\n",
      "|Mohan| 29|       SC|    2|\n",
      "+-----+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = spark.read.option('quote','\\'').csv('sample_data/education.csv',header=True,inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "result = df.withColumn('Education',explode(split(coalesce(col('Education'),lit('None')),',')))\n",
    "\n",
    "wind = Window.partitionBy('Name').orderBy('Education')\n",
    "\n",
    "result.withColumn('Index',row_number().over(wind)-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| Name|Age|Education|\n",
      "+-----+---+---------+\n",
      "| Azar| 24|      MBA|\n",
      "| Azar| 24|       BE|\n",
      "| Azar| 24|      HSC|\n",
      "| Babu| 28|     null|\n",
      "| Mani| 22|      MBA|\n",
      "| Mani| 22|      BSE|\n",
      "| Mani| 22|  Diploma|\n",
      "|Mohan| 29|      MBA|\n",
      "|Mohan| 29|    BArch|\n",
      "|Mohan| 29|       SC|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "df.withColumn('Education',explode_outer(split(col('Education'),','))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambiguous column name during flattening\n",
    "\n",
    "Spark will automatically detect duplicate columns and while reading it throws error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------+\n",
      "|            delivery|    mobile|           name|\n",
      "+--------------------+----------+---------------+\n",
      "|{123 Park Ave, Ne...|9876543210|       John Doe|\n",
      "|{456 Elm St, Los ...|9988776655|    Alice Smith|\n",
      "|{789 Oak Dr, Chic...|9870011223|Michael Johnson|\n",
      "+--------------------+----------+---------------+\n",
      "\n",
      "root\n",
      " |-- delivery: struct (nullable = true)\n",
      " |    |-- address: string (nullable = true)\n",
      " |    |-- mobile: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|    mobile|           name|             address|    mobile|         name|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|9876543210|       John Doe|123 Park Ave, New...|9123456789|     Jane Doe|\n",
      "|9988776655|    Alice Smith|456 Elm St, Los A...|9012345678|    Bob Smith|\n",
      "|9870011223|Michael Johnson|789 Oak Dr, Chica...|9345678901|Sarah Johnson|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('multiline',True).json('sample_data/ambicol.json')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df1 = df.select('*','delivery.*').drop('delivery')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'name' is ambiguous, could be: name, name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \n\u001b[0;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[0;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Reference 'name' is ambiguous, could be: name, name."
     ]
    }
   ],
   "source": [
    "df1.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|    mobile|           name|             address|    mobile|       name_1|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "|9876543210|       John Doe|123 Park Ave, New...|9123456789|     Jane Doe|\n",
      "|9988776655|    Alice Smith|456 Elm St, Los A...|9012345678|    Bob Smith|\n",
      "|9870011223|Michael Johnson|789 Oak Dr, Chica...|9345678901|Sarah Johnson|\n",
      "+----------+---------------+--------------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = df1.columns\n",
    "d = {}\n",
    "for i in range(len(l)):\n",
    "    if(d.get(l[i])):\n",
    "        l[i] = l[i] + '_' + str(d.get(l[i]))\n",
    "    else:\n",
    "        d[l[i]] = i \n",
    "\n",
    "df2 = df1.toDF(*l)\n",
    "df2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD to DF and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "|   c|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|   a|\n",
      "|   b|\n",
      "|   c|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# l = ['a','b','c'] X wrong\n",
    "\n",
    "l = [['a'],['b'],['c']]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "rdd.collect()\n",
    "\n",
    "df = spark.createDataFrame(rdd,['name'])\n",
    "df.show()\n",
    "\n",
    "rdd.toDF(['name']).show()\n",
    "\n",
    "# Your code is almost correct, but there’s a small mismatch in how you're converting the RDD to a DataFrame — specifically, the RDD elements are strings ('a', 'b', 'c'), but Spark expects rows or tuples when creating a DataFrame with a schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='a'), Row(name='b'), Row(name='c')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = df.rdd\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove N lines from a file while reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|# This is a sample CSV file            |\n",
      "+---------------------------------------+\n",
      "|# Generated for testing multi-line skip|\n",
      "|# Author: OpenAI                       |\n",
      "|# Date: 2025-04-10                     |\n",
      "|# Below is the actual data             |\n",
      "|id                                     |\n",
      "|1                                      |\n",
      "|2                                      |\n",
      "|3                                      |\n",
      "|4                                      |\n",
      "|5                                      |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True) \\\n",
    "               .option(\"skipRows\", 5) \\\n",
    "               .csv(\"sample_data/unwanted.csv\")\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile('sample_data/unwanted.csv',1)\n",
    "\n",
    "def skip_first_five_lines(partition_index, lines):\n",
    "    if partition_index == 0:\n",
    "        return iter(list(lines)[6:])  # Skip first 5 lines in first partition\n",
    "    else:\n",
    "        return lines\n",
    "\n",
    "filtered_rdd = rdd.mapPartitionsWithIndex(skip_first_five_lines)\n",
    "filtered_rdd.collect()\n",
    "\n",
    "header = filtered_rdd.first().split(\",\")\n",
    "data_rdd = filtered_rdd.filter(lambda line: line != \",\".join(header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row('1', 'Alice', 'New York')>,\n",
       " <Row('2', 'Bob', 'Los Angeles')>,\n",
       " <Row('3', 'Charlie', 'Chicago')>,\n",
       " <Row('4', 'Diana', 'Houston')>,\n",
       " <Row('5', 'Evan', 'Phoenix')>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rows_rdd = data_rdd.map(lambda line: line.split(\",\")).map(lambda parts: Row(*parts))\n",
    "\n",
    "rows_rdd.collect()\n",
    "\n",
    "# Why are we using Row()?\n",
    "# When creating a DataFrame from an RDD, Spark expects each record (row) to be:\n",
    "\n",
    "# a tuple, or\n",
    "\n",
    "# a Row object\n",
    "\n",
    "# This helps Spark associate values with column names when a schema is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rows_rdd, header)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from multiple directories omitting some \n",
    "\n",
    "method1:\n",
    "list of input paths\n",
    "[\"data1/*.csv\",\"data2/*.csv\"]\n",
    "\n",
    "method2:\n",
    "use regex pattern\n",
    "\"data[1-3]*/*.csv\"\n",
    "\n",
    "method3:\n",
    "use curly braces\n",
    "\"data{1,2,3}*/*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if DF is empty or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "| Monisha| 23|\n",
      "|  Arvind| 24|\n",
      "|Rishitha| 24|\n",
      "|  Anusha| 24|\n",
      "| Gayatri| 25|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = spark.read.csv('sample_data/input1.csv',header=True,inferSchema=True)\n",
    "\n",
    "input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records = input.filter('age > 26')\n",
    "records.show()\n",
    "\n",
    "records.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(not records.count()):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(not records.first()):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(not records.take(0)):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF is empty\n"
     ]
    }
   ],
   "source": [
    "if(records.rdd.isEmpty()):\n",
    "    print('DF is empty')\n",
    "else:\n",
    "    print('DF is not empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rdd.isEmpty()\tChecks if the underlying RDD is empty (efficient)\tLarge DataFrames (best for performance)\n",
    "# df.isEmpty()\tChecks if the DataFrame is empty (available in Spark 3.x and later)\tRecommended for Spark 3.x\n",
    "# df.count()\tReturns row count (slower, scans the entire DataFrame)\tSmall DataFrames, less efficient\n",
    "# df.head() / df.take(1)\tChecks if the first row exists (quick check)\tQuick, small DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators - Alternative for df.count\n",
    "\n",
    "A PySpark accumulator is a shared variable that workers can add to, but only the driver can read from.\n",
    "\n",
    "✅ Use it to:\n",
    "\n",
    "Count rows\n",
    "\n",
    "Count errors, nulls, or bad data\n",
    "\n",
    "Track totals (e.g., bytes processed)\n",
    "\n",
    "Workers can only add\tNo read access inside workers\n",
    "Only reliable in actions\tThey only update when an action is triggered (like .collect(), .count())\n",
    "No guarantees in transformations\tLazy eval means accumulator values aren't reliable in .map() unless an action forces execution\n",
    "\n",
    "\n",
    "🧠 When to Use df.count()\n",
    "✅ Ideal for:\n",
    "Total number of rows\n",
    "\n",
    "Size check after filtering or joins\n",
    "\n",
    "Fast, declarative counting\n",
    "\n",
    "df.filter(\"country = 'India'\").count()\n",
    "\n",
    "🛠 When to Use Accumulators\n",
    "\n",
    "✅ Ideal for:\n",
    "Counting specific conditions (e.g. number of rows with nulls, invalid formats)\n",
    "\n",
    "Tracking custom metrics during transformation\n",
    "\n",
    "Debugging data pipeline quality issues\n",
    "\n",
    "error_count = sc.accumulator(0)\n",
    "\n",
    "def validate(row):\n",
    "    global error_count\n",
    "    if row['email'] is None:\n",
    "        error_count += 1\n",
    "    return row\n",
    "\n",
    "df.rdd.map(validate).count()\n",
    "print(\"Null emails:\", error_count.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10127"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "record_count = spark.sparkContext.accumulator(0)\n",
    "age_count = spark.sparkContext.accumulator(0)\n",
    "\n",
    "bank = spark.read.format('csv').option('header',True).option('inferSchema',True).load('sample_data/BankChurners.csv')\n",
    "\n",
    "bank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10127\n"
     ]
    }
   ],
   "source": [
    "bank.rdd.foreach(lambda x:record_count.add(1))\n",
    "\n",
    "print(record_count.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7730"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.filter('Customer_Age > 40').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7730\n"
     ]
    }
   ],
   "source": [
    "age_count = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def check(row):\n",
    "    if(row['Customer_Age']>40):\n",
    "        age_count.add(1)\n",
    "    return row\n",
    "\n",
    "bank.rdd.map(check).collect()\n",
    "\n",
    "print(age_count.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking Data - Sensitive data\n",
    "\n",
    "Use UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+\n",
      "|rollno|        name|    mobile|\n",
      "+------+------------+----------+\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "|   101|    John Doe|9876543210|\n",
      "|   102|  Jane Smith|9123456789|\n",
      "|   103|Robert Brown|9988776655|\n",
      "|   104| Alice Green|9012345678|\n",
      "+------+------------+----------+\n",
      "\n",
      "root\n",
      " |-- rollno: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- mobile: string (nullable = true)\n",
      "\n",
      "+------+------------+----------+-------------+\n",
      "|rollno|        name|    mobile|mobile_masked|\n",
      "+------+------------+----------+-------------+\n",
      "|   101|    John Doe|9876543210|  98*******10|\n",
      "|   102|  Jane Smith|9123456789|  91*******89|\n",
      "|   103|Robert Brown|9988776655|  99*******55|\n",
      "|   104| Alice Green|9012345678|  90*******78|\n",
      "|   101|    John Doe|9876543210|  98*******10|\n",
      "|   102|  Jane Smith|9123456789|  91*******89|\n",
      "|   103|Robert Brown|9988776655|  99*******55|\n",
      "|   104| Alice Green|9012345678|  90*******78|\n",
      "+------+------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf,col\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "    StructField('rollno',IntegerType()),\n",
    "    StructField('name',StringType()),\n",
    "    StructField('mobile',StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "unmask = spark.read.schema(schema=schema).csv('sample_data/duplicates.csv',header=True)\n",
    "\n",
    "unmask.show()\n",
    "\n",
    "unmask.printSchema()\n",
    "\n",
    "def mobile_mask(col):\n",
    "    new = col[0:2] + ('*' * 7) + col[-2] + col[-1]\n",
    "    return new\n",
    "    \n",
    "# print(mobile_mask('9876543210'))\n",
    "\n",
    "mask_udf = udf(mobile_mask,StringType())\n",
    "\n",
    "mask_df = unmask.withColumn('mobile_masked',mask_udf(col('mobile')))\n",
    "\n",
    "mask_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+\n",
      "|customer_id|transaction_type|amount|\n",
      "+-----------+----------------+------+\n",
      "|        101|          credit| 500.0|\n",
      "|        102|           debit| 200.0|\n",
      "|        101|           debit| 150.0|\n",
      "|        103|          credit|1000.0|\n",
      "|        102|          credit| 300.0|\n",
      "|        104|           debit| 120.0|\n",
      "|        105|          credit| 750.0|\n",
      "|        103|           debit| 400.0|\n",
      "|        104|          credit| 250.0|\n",
      "|        101|          credit| 100.0|\n",
      "+-----------+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (101, 'credit', 500.00),\n",
    "    (102, 'debit', 200.00),\n",
    "    (101, 'debit', 150.00),\n",
    "    (103, 'credit', 1000.00),\n",
    "    (102, 'credit', 300.00),\n",
    "    (104, 'debit', 120.00),\n",
    "    (105, 'credit', 750.00),\n",
    "    (103, 'debit', 400.00),\n",
    "    (104, 'credit', 250.00),\n",
    "    (101, 'credit', 100.00),\n",
    "]\n",
    "\n",
    "columns = ['customer_id', 'transaction_type', 'amount']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------+\n",
      "|customer_id|transaction_type|amount|\n",
      "+-----------+----------------+------+\n",
      "|        101|          credit| 500.0|\n",
      "|        102|           debit| 200.0|\n",
      "|        101|           debit| 150.0|\n",
      "|        103|          credit|1000.0|\n",
      "|        102|          credit| 300.0|\n",
      "|        104|           debit| 120.0|\n",
      "|        105|          credit| 750.0|\n",
      "|        103|           debit| 400.0|\n",
      "|        104|          credit| 250.0|\n",
      "|        101|          credit| 100.0|\n",
      "+-----------+----------------+------+\n",
      "\n",
      "+-----------+---------------+\n",
      "|customer_id|sum(amount_chk)|\n",
      "+-----------+---------------+\n",
      "|        101|          450.0|\n",
      "|        102|          100.0|\n",
      "|        103|          600.0|\n",
      "|        104|          130.0|\n",
      "|        105|          750.0|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df1 = df.withColumn('amount_chk',when(col('transaction_type')=='credit',col('amount')).otherwise(-1*col('amount')))\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1.select('customer_id','amount_chk').groupBy('customer_id').sum('amount_chk').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|customer_id|sum(amount_chk)|\n",
      "+-----------+---------------+\n",
      "|        101|          450.0|\n",
      "|        102|          100.0|\n",
      "|        103|          600.0|\n",
      "|        104|          130.0|\n",
      "|        105|          750.0|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df1.groupBy('customer_id').agg(sum('amount_chk')).alias('total balance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+\n",
      "|customer_id|credit|debit|\n",
      "+-----------+------+-----+\n",
      "|        103|1000.0|400.0|\n",
      "|        104| 250.0|120.0|\n",
      "|        105| 750.0| null|\n",
      "|        101| 600.0|150.0|\n",
      "|        102| 300.0|200.0|\n",
      "+-----------+------+-----+\n",
      "\n",
      "+-----------+------------------------------------------+\n",
      "|customer_id|(coalesce(credit, 0) - coalesce(debit, 0))|\n",
      "+-----------+------------------------------------------+\n",
      "|        103|                                     600.0|\n",
      "|        104|                                     130.0|\n",
      "|        105|                                     750.0|\n",
      "|        101|                                     450.0|\n",
      "|        102|                                     100.0|\n",
      "+-----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy('customer_id').pivot('transaction_type').agg(sum('amount'))\n",
    "\n",
    "df2.show()\n",
    "\n",
    "df2.selectExpr('customer_id','coalesce(credit,0)-coalesce(debit,0)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "union and unionByName\n",
    "\n",
    "union \n",
    "\n",
    "merge the 2 dataframes with respect to the position of the column and datatype of the column \n",
    "\n",
    "requires both the tables to have same no of columns\n",
    "\n",
    "column names should be of same order \n",
    "\n",
    "but unionByName will match based on the column names and even if there are different no of columns, it will take null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition(col) Vs PartitionBy(col)\n",
    "\n",
    "PartitionBy can we used only with write, creates separate folders for each partition. There wont be any shuffle with partitionBy, it will directly push the data to the folders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files recursively\n",
    "\n",
    "use option('recursiveFileLookup',true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date time handling in schema using inferschema\n",
    "\n",
    "use option('timestampFormat','M/d/yyy')\n",
    "use option('dateformat','M/d/yyy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
